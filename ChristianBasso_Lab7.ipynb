{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Network Lab\n",
    "### Christian Basso\n",
    "In this lab you will be experimenting with ANNs.  Let's start by importing a few things.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-01-30 19:39:43.791936: I tensorflow/core/util/util.cc:169] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import ConfusionMatrixDisplay\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.datasets import make_blobs\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import tensorflow.keras as keras\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout, Flatten, Conv2D, MaxPooling2D\n",
    "from tensorflow.keras import backend as K\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We first generate some dummy data from random samples in a 2D space from 4 clusters.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_samples = 64\n",
    "variance = 0.01\n",
    "  \n",
    "# 4 clusters in a 2D space\n",
    "centers = np.array([[0, 0],\n",
    "                    [0, 1],\n",
    "                    [1, 0],\n",
    "                    [1, 1]])\n",
    "   \n",
    "X, y = make_blobs(n_samples,\n",
    "                  centers=centers,\n",
    "                  cluster_std = np.sqrt(variance),\n",
    "                  shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's use matplot lib to plot the clusters of the ``X`` values coloring the points according to their labels (``y``)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.scatter(X[:,0],X[:,1], c=y, alpha=0.5)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 1\n",
    "We will be classifying our data, so you should fill out the next two functions in the next cell to accurately classify the values.  You should not use any loops!  \n",
    "\n",
    "You should be able to achieve 100% accuracy with the provided blobs, so you should test your function by creating a second set of values with a higher variance so that your classifier is not able to achieve 100% accuracy.  Document this appropriately in the following cell(s)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "\n",
    "def my_classifier(X):\n",
    "    ''' This function takes a NumPy vector with 2 variables and returns a classification value (0-3)\n",
    "    '''\n",
    "#    print(x)\n",
    "#    if x[0] < 0.4:\n",
    "#        if x[1] > .6:\n",
    "#            return 1\n",
    "#        if x[1] <.6:\n",
    "#            return 0\n",
    "#    if x[0] > 0.21:\n",
    "#        if x[1] > .6:\n",
    "#            return 2\n",
    "#        if x[1] < 6:\n",
    "#            return 3\n",
    "\n",
    "    scaler = MinMaxScaler()\n",
    "    X = scaler.fit_transform(X)\n",
    "    return X\n",
    "\n",
    "def run_my_classifier(X, y):\n",
    "    ''' This function takes a vector of pairs of points, classifies each pair using my_classifier, and \n",
    "    then compares the predicted y value with the actual y values in the y variable.  It should return the \n",
    "    accuracy value for the two vectors.  \n",
    "    '''\n",
    "    model = LogisticRegression()\n",
    "    model.fit(X, y)\n",
    "    Xnew = my_classifier(X)\n",
    "    ynew = model.predict(Xnew)\n",
    "    acc = accuracy_score(y, ynew)\n",
    "    return acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "run_my_classifier(X,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_samples = 64\n",
    "variance = 0.5\n",
    "  \n",
    "# 4 clusters in a 2D space\n",
    "centers = np.array([[0, 0],\n",
    "                    [0, 1],\n",
    "                    [1, 0],\n",
    "                    [1, 1]])\n",
    "   \n",
    "HVX, HVy = make_blobs(n_samples,\n",
    "                  centers=centers,\n",
    "                  cluster_std = np.sqrt(variance),\n",
    "                  shuffle=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.453125"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "run_my_classifier(HVX, HVy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multi-Layer Perceptron\n",
    "\n",
    "We now define a function to create and train a Multi-layer Perceptron (MLP) classifier.   Calling this function will train the model and generate some print statements that show the confusion matrix output.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_mlp(X_train, y_train, X_test, y_test, out_class=4, hidden=16, epochs=100):\n",
    "    model = Sequential()\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(hidden, activation='relu'))\n",
    "    model.add(Dense(out_class, activation='softmax'))\n",
    "    model.compile(loss=\"sparse_categorical_crossentropy\",\n",
    "              optimizer=keras.optimizers.SGD(),\n",
    "              metrics=['accuracy'])\n",
    "    history = model.fit(x=X_train,y=y_train,epochs=epochs,verbose=1)\n",
    "    model.summary()\n",
    "    predicted_probabilities = model.predict(X_train)\n",
    "    predicted_classes = np.argmax(predicted_probabilities, axis=1)\n",
    "    acc = 100. * accuracy_score(y_train, predicted_classes)\n",
    "    print(\"Accuracy on train set: {:.2f}%\".format(acc))\n",
    "    predicted_probabilities = model.predict(X_test)\n",
    "    predicted_classes = np.argmax(predicted_probabilities, axis=1)\n",
    "    acc = 100. * accuracy_score(y_test, predicted_classes)\n",
    "    print(\"Accuracy on test set: {:.2f}%\".format(acc))\n",
    "    print(confusion_matrix(y_test, predicted_classes))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, lets see how well the original (low variance) data can be clustered with the MLP.  First, we need to make sure we split the data into training and testing sets to identify overfitting.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-01-30 19:40:19.483932: I tensorflow/core/platform/cpu_feature_guard.cc:194] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE3 SSE4.1 SSE4.2 AVX\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-01-30 19:40:22.700480: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 13665 MB memory:  -> device: 0, name: Tesla T4, pci bus id: 0000:da:00.0, compute capability: 7.5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "2/2 [==============================] - 4s 5ms/step - loss: 1.4351 - accuracy: 0.3542\n",
      "Epoch 2/10\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.4271 - accuracy: 0.3542\n",
      "Epoch 3/10\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.4190 - accuracy: 0.3542\n",
      "Epoch 4/10\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 1.4103 - accuracy: 0.3542\n",
      "Epoch 5/10\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.4025 - accuracy: 0.3542\n",
      "Epoch 6/10\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.3942 - accuracy: 0.3542\n",
      "Epoch 7/10\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.3869 - accuracy: 0.3542\n",
      "Epoch 8/10\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 1.3800 - accuracy: 0.3542\n",
      "Epoch 9/10\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.3727 - accuracy: 0.3542\n",
      "Epoch 10/10\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.3658 - accuracy: 0.3542\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " flatten (Flatten)           (None, 2)                 0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 16)                48        \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 4)                 68        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 116\n",
      "Trainable params: 116\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "2/2 [==============================] - 0s 2ms/step\n",
      "Accuracy on train set: 35.42%\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "Accuracy on test set: 31.25%\n",
      "[[1 5 1 0]\n",
      " [0 2 0 0]\n",
      " [0 1 2 0]\n",
      " [1 3 0 0]]\n"
     ]
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=10)\n",
    "run_mlp(X_train, y_train, X_test, y_test, 4, 16, 10) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 2:\n",
    "Re-run the experiment at least 5 more times with different numbers of epochs in the next cell and plot the results to show the overall accuracy vs. the number of epochs.  Write a few sentences about what you observed about the relationship in the Reflection cell.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2/2 [==============================] - 0s 4ms/step - loss: 1.4152 - accuracy: 0.2500\n",
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " flatten_1 (Flatten)         (None, 2)                 0         \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 16)                48        \n",
      "                                                                 \n",
      " dense_3 (Dense)             (None, 4)                 68        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 116\n",
      "Trainable params: 116\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "2/2 [==============================] - 0s 2ms/step\n",
      "Accuracy on train set: 25.00%\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "Accuracy on test set: 37.50%\n",
      "[[4 2 0 1]\n",
      " [1 0 0 1]\n",
      " [1 2 0 0]\n",
      " [1 1 0 2]]\n"
     ]
    }
   ],
   "source": [
    "run_mlp(X_train, y_train, X_test, y_test, 4, 16, 1) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/15\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 1.7531 - accuracy: 0.0833\n",
      "Epoch 2/15\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 1.7372 - accuracy: 0.0833\n",
      "Epoch 3/15\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 1.7220 - accuracy: 0.0833\n",
      "Epoch 4/15\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.7083 - accuracy: 0.0833\n",
      "Epoch 5/15\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.6950 - accuracy: 0.0833\n",
      "Epoch 6/15\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.6820 - accuracy: 0.0833\n",
      "Epoch 7/15\n",
      "2/2 [==============================] - 0s 7ms/step - loss: 1.6687 - accuracy: 0.0625\n",
      "Epoch 8/15\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.6577 - accuracy: 0.0625\n",
      "Epoch 9/15\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.6452 - accuracy: 0.0625\n",
      "Epoch 10/15\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.6334 - accuracy: 0.0625\n",
      "Epoch 11/15\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.6231 - accuracy: 0.0625\n",
      "Epoch 12/15\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.6128 - accuracy: 0.0625\n",
      "Epoch 13/15\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 1.6026 - accuracy: 0.0625\n",
      "Epoch 14/15\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.5935 - accuracy: 0.0625\n",
      "Epoch 15/15\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.5845 - accuracy: 0.0417\n",
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " flatten_2 (Flatten)         (None, 2)                 0         \n",
      "                                                                 \n",
      " dense_4 (Dense)             (None, 16)                48        \n",
      "                                                                 \n",
      " dense_5 (Dense)             (None, 4)                 68        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 116\n",
      "Trainable params: 116\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Accuracy on train set: 4.17%\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "Accuracy on test set: 6.25%\n",
      "[[1 1 0 5]\n",
      " [2 0 0 0]\n",
      " [0 1 0 2]\n",
      " [4 0 0 0]]\n"
     ]
    }
   ],
   "source": [
    "run_mlp(X_train, y_train, X_test, y_test, 4, 16, 15) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 1.8305 - accuracy: 0.0833\n",
      "Epoch 2/30\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 1.8097 - accuracy: 0.0833\n",
      "Epoch 3/30\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 1.7895 - accuracy: 0.1042\n",
      "Epoch 4/30\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.7701 - accuracy: 0.1042\n",
      "Epoch 5/30\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.7514 - accuracy: 0.1250\n",
      "Epoch 6/30\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.7328 - accuracy: 0.1250\n",
      "Epoch 7/30\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.7135 - accuracy: 0.1250\n",
      "Epoch 8/30\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.6946 - accuracy: 0.1667\n",
      "Epoch 9/30\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.6775 - accuracy: 0.1667\n",
      "Epoch 10/30\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.6623 - accuracy: 0.1667\n",
      "Epoch 11/30\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 1.6476 - accuracy: 0.1667\n",
      "Epoch 12/30\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.6332 - accuracy: 0.1667\n",
      "Epoch 13/30\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.6189 - accuracy: 0.1875\n",
      "Epoch 14/30\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.6045 - accuracy: 0.1875\n",
      "Epoch 15/30\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 1.5909 - accuracy: 0.1875\n",
      "Epoch 16/30\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.5771 - accuracy: 0.1875\n",
      "Epoch 17/30\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 1.5636 - accuracy: 0.2083\n",
      "Epoch 18/30\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 1.5505 - accuracy: 0.2292\n",
      "Epoch 19/30\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.5383 - accuracy: 0.2292\n",
      "Epoch 20/30\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.5266 - accuracy: 0.2292\n",
      "Epoch 21/30\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.5150 - accuracy: 0.2292\n",
      "Epoch 22/30\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.5036 - accuracy: 0.2292\n",
      "Epoch 23/30\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.4928 - accuracy: 0.2500\n",
      "Epoch 24/30\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.4819 - accuracy: 0.2500\n",
      "Epoch 25/30\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.4717 - accuracy: 0.2500\n",
      "Epoch 26/30\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.4617 - accuracy: 0.2500\n",
      "Epoch 27/30\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.4519 - accuracy: 0.2500\n",
      "Epoch 28/30\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 1.4430 - accuracy: 0.2500\n",
      "Epoch 29/30\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.4337 - accuracy: 0.2708\n",
      "Epoch 30/30\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.4247 - accuracy: 0.2917\n",
      "Model: \"sequential_3\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " flatten_3 (Flatten)         (None, 2)                 0         \n",
      "                                                                 \n",
      " dense_6 (Dense)             (None, 16)                48        \n",
      "                                                                 \n",
      " dense_7 (Dense)             (None, 4)                 68        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 116\n",
      "Trainable params: 116\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "2/2 [==============================] - 0s 2ms/step\n",
      "Accuracy on train set: 31.25%\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "Accuracy on test set: 31.25%\n",
      "[[0 0 4 3]\n",
      " [0 1 0 1]\n",
      " [0 0 3 0]\n",
      " [0 3 0 1]]\n"
     ]
    }
   ],
   "source": [
    "run_mlp(X_train, y_train, X_test, y_test, 4, 16, 30) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/75\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 1.5017 - accuracy: 0.3750\n",
      "Epoch 2/75\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 1.4929 - accuracy: 0.3750\n",
      "Epoch 3/75\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 1.4820 - accuracy: 0.3750\n",
      "Epoch 4/75\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.4737 - accuracy: 0.3750\n",
      "Epoch 5/75\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.4649 - accuracy: 0.3750\n",
      "Epoch 6/75\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.4567 - accuracy: 0.3750\n",
      "Epoch 7/75\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.4490 - accuracy: 0.3750\n",
      "Epoch 8/75\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.4401 - accuracy: 0.3750\n",
      "Epoch 9/75\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.4324 - accuracy: 0.3750\n",
      "Epoch 10/75\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.4255 - accuracy: 0.3958\n",
      "Epoch 11/75\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.4190 - accuracy: 0.3958\n",
      "Epoch 12/75\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.4121 - accuracy: 0.3958\n",
      "Epoch 13/75\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.4040 - accuracy: 0.3958\n",
      "Epoch 14/75\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.3975 - accuracy: 0.4167\n",
      "Epoch 15/75\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.3912 - accuracy: 0.4375\n",
      "Epoch 16/75\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.3843 - accuracy: 0.4375\n",
      "Epoch 17/75\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.3775 - accuracy: 0.4792\n",
      "Epoch 18/75\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.3711 - accuracy: 0.4792\n",
      "Epoch 19/75\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.3638 - accuracy: 0.4792\n",
      "Epoch 20/75\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.3575 - accuracy: 0.4792\n",
      "Epoch 21/75\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 1.3514 - accuracy: 0.4792\n",
      "Epoch 22/75\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.3462 - accuracy: 0.4792\n",
      "Epoch 23/75\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.3408 - accuracy: 0.4792\n",
      "Epoch 24/75\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.3355 - accuracy: 0.4792\n",
      "Epoch 25/75\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.3303 - accuracy: 0.4792\n",
      "Epoch 26/75\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.3249 - accuracy: 0.4792\n",
      "Epoch 27/75\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.3193 - accuracy: 0.4792\n",
      "Epoch 28/75\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.3146 - accuracy: 0.4792\n",
      "Epoch 29/75\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.3099 - accuracy: 0.4792\n",
      "Epoch 30/75\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.3051 - accuracy: 0.4792\n",
      "Epoch 31/75\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.3000 - accuracy: 0.4792\n",
      "Epoch 32/75\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.2958 - accuracy: 0.4792\n",
      "Epoch 33/75\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 1.2912 - accuracy: 0.4792\n",
      "Epoch 34/75\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.2871 - accuracy: 0.5000\n",
      "Epoch 35/75\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.2824 - accuracy: 0.4792\n",
      "Epoch 36/75\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.2787 - accuracy: 0.5000\n",
      "Epoch 37/75\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 1.2749 - accuracy: 0.5000\n",
      "Epoch 38/75\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 1.2706 - accuracy: 0.5000\n",
      "Epoch 39/75\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.2670 - accuracy: 0.5000\n",
      "Epoch 40/75\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.2634 - accuracy: 0.5000\n",
      "Epoch 41/75\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 1.2596 - accuracy: 0.4792\n",
      "Epoch 42/75\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.2561 - accuracy: 0.5000\n",
      "Epoch 43/75\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.2520 - accuracy: 0.4792\n",
      "Epoch 44/75\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.2487 - accuracy: 0.5000\n",
      "Epoch 45/75\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 1.2460 - accuracy: 0.5000\n",
      "Epoch 46/75\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.2430 - accuracy: 0.5000\n",
      "Epoch 47/75\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.2404 - accuracy: 0.5000\n",
      "Epoch 48/75\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 1.2371 - accuracy: 0.5000\n",
      "Epoch 49/75\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 1.2340 - accuracy: 0.5000\n",
      "Epoch 50/75\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 1.2309 - accuracy: 0.5000\n",
      "Epoch 51/75\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.2279 - accuracy: 0.5000\n",
      "Epoch 52/75\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.2248 - accuracy: 0.5208\n",
      "Epoch 53/75\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.2220 - accuracy: 0.5208\n",
      "Epoch 54/75\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 1.2190 - accuracy: 0.5208\n",
      "Epoch 55/75\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 1.2162 - accuracy: 0.5000\n",
      "Epoch 56/75\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.2135 - accuracy: 0.5000\n",
      "Epoch 57/75\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.2110 - accuracy: 0.5000\n",
      "Epoch 58/75\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.2076 - accuracy: 0.5000\n",
      "Epoch 59/75\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.2050 - accuracy: 0.5000\n",
      "Epoch 60/75\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.2029 - accuracy: 0.5000\n",
      "Epoch 61/75\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.2001 - accuracy: 0.5000\n",
      "Epoch 62/75\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.1975 - accuracy: 0.5000\n",
      "Epoch 63/75\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 1.1950 - accuracy: 0.5000\n",
      "Epoch 64/75\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 1.1923 - accuracy: 0.5000\n",
      "Epoch 65/75\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 1.1903 - accuracy: 0.5000\n",
      "Epoch 66/75\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.1878 - accuracy: 0.5000\n",
      "Epoch 67/75\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.1848 - accuracy: 0.5208\n",
      "Epoch 68/75\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.1828 - accuracy: 0.5208\n",
      "Epoch 69/75\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.1806 - accuracy: 0.5417\n",
      "Epoch 70/75\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.1782 - accuracy: 0.5417\n",
      "Epoch 71/75\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.1760 - accuracy: 0.5417\n",
      "Epoch 72/75\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.1738 - accuracy: 0.5417\n",
      "Epoch 73/75\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.1717 - accuracy: 0.5417\n",
      "Epoch 74/75\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.1697 - accuracy: 0.5417\n",
      "Epoch 75/75\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 1.1677 - accuracy: 0.5417\n",
      "Model: \"sequential_4\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " flatten_4 (Flatten)         (None, 2)                 0         \n",
      "                                                                 \n",
      " dense_8 (Dense)             (None, 16)                48        \n",
      "                                                                 \n",
      " dense_9 (Dense)             (None, 4)                 68        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 116\n",
      "Trainable params: 116\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "WARNING:tensorflow:5 out of the last 13 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7f0ac589fdc0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2/2 [==============================] - 0s 2ms/step\n",
      "Accuracy on train set: 54.17%\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "Accuracy on test set: 56.25%\n",
      "[[4 0 3 0]\n",
      " [0 1 0 1]\n",
      " [2 0 1 0]\n",
      " [0 0 1 3]]\n"
     ]
    }
   ],
   "source": [
    "run_mlp(X_train, y_train, X_test, y_test, 4, 16, 75) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/150\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 1.4329 - accuracy: 0.2292\n",
      "Epoch 2/150\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.4279 - accuracy: 0.2292\n",
      "Epoch 3/150\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.4232 - accuracy: 0.2292\n",
      "Epoch 4/150\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.4189 - accuracy: 0.2500\n",
      "Epoch 5/150\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.4143 - accuracy: 0.2500\n",
      "Epoch 6/150\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 1.4099 - accuracy: 0.2500\n",
      "Epoch 7/150\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.4053 - accuracy: 0.2708\n",
      "Epoch 8/150\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.4014 - accuracy: 0.2708\n",
      "Epoch 9/150\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.3973 - accuracy: 0.2917\n",
      "Epoch 10/150\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.3934 - accuracy: 0.3333\n",
      "Epoch 11/150\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.3893 - accuracy: 0.3333\n",
      "Epoch 12/150\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 1.3849 - accuracy: 0.3333\n",
      "Epoch 13/150\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.3808 - accuracy: 0.3333\n",
      "Epoch 14/150\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.3766 - accuracy: 0.3542\n",
      "Epoch 15/150\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.3729 - accuracy: 0.3542\n",
      "Epoch 16/150\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 1.3691 - accuracy: 0.3333\n",
      "Epoch 17/150\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.3654 - accuracy: 0.3750\n",
      "Epoch 18/150\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.3621 - accuracy: 0.3542\n",
      "Epoch 19/150\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.3581 - accuracy: 0.3542\n",
      "Epoch 20/150\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 1.3545 - accuracy: 0.3542\n",
      "Epoch 21/150\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.3510 - accuracy: 0.3750\n",
      "Epoch 22/150\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.3476 - accuracy: 0.3542\n",
      "Epoch 23/150\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.3443 - accuracy: 0.3542\n",
      "Epoch 24/150\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 1.3411 - accuracy: 0.3958\n",
      "Epoch 25/150\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.3379 - accuracy: 0.3958\n",
      "Epoch 26/150\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.3349 - accuracy: 0.3750\n",
      "Epoch 27/150\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 1.3315 - accuracy: 0.3750\n",
      "Epoch 28/150\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.3288 - accuracy: 0.3750\n",
      "Epoch 29/150\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.3252 - accuracy: 0.3750\n",
      "Epoch 30/150\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.3222 - accuracy: 0.3958\n",
      "Epoch 31/150\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.3188 - accuracy: 0.4167\n",
      "Epoch 32/150\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.3156 - accuracy: 0.4167\n",
      "Epoch 33/150\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.3125 - accuracy: 0.4167\n",
      "Epoch 34/150\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.3098 - accuracy: 0.4375\n",
      "Epoch 35/150\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.3068 - accuracy: 0.4583\n",
      "Epoch 36/150\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.3035 - accuracy: 0.4792\n",
      "Epoch 37/150\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.3006 - accuracy: 0.4792\n",
      "Epoch 38/150\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 1.2977 - accuracy: 0.5000\n",
      "Epoch 39/150\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.2948 - accuracy: 0.5000\n",
      "Epoch 40/150\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.2919 - accuracy: 0.5000\n",
      "Epoch 41/150\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.2891 - accuracy: 0.5000\n",
      "Epoch 42/150\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.2863 - accuracy: 0.4792\n",
      "Epoch 43/150\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 1.2835 - accuracy: 0.5000\n",
      "Epoch 44/150\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.2810 - accuracy: 0.4792\n",
      "Epoch 45/150\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.2780 - accuracy: 0.5000\n",
      "Epoch 46/150\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.2753 - accuracy: 0.4792\n",
      "Epoch 47/150\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 1.2728 - accuracy: 0.4792\n",
      "Epoch 48/150\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.2698 - accuracy: 0.4583\n",
      "Epoch 49/150\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.2672 - accuracy: 0.4792\n",
      "Epoch 50/150\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 1.2645 - accuracy: 0.4792\n",
      "Epoch 51/150\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.2616 - accuracy: 0.4792\n",
      "Epoch 52/150\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.2589 - accuracy: 0.4792\n",
      "Epoch 53/150\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.2566 - accuracy: 0.5000\n",
      "Epoch 54/150\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.2542 - accuracy: 0.5000\n",
      "Epoch 55/150\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.2517 - accuracy: 0.5000\n",
      "Epoch 56/150\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.2487 - accuracy: 0.5000\n",
      "Epoch 57/150\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.2460 - accuracy: 0.5000\n",
      "Epoch 58/150\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 1.2442 - accuracy: 0.5000\n",
      "Epoch 59/150\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 1.2411 - accuracy: 0.5000\n",
      "Epoch 60/150\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 1.2389 - accuracy: 0.5000\n",
      "Epoch 61/150\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.2365 - accuracy: 0.5000\n",
      "Epoch 62/150\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.2335 - accuracy: 0.5000\n",
      "Epoch 63/150\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.2310 - accuracy: 0.5000\n",
      "Epoch 64/150\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.2289 - accuracy: 0.5208\n",
      "Epoch 65/150\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.2263 - accuracy: 0.5208\n",
      "Epoch 66/150\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.2242 - accuracy: 0.5208\n",
      "Epoch 67/150\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.2222 - accuracy: 0.5208\n",
      "Epoch 68/150\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.2195 - accuracy: 0.5417\n",
      "Epoch 69/150\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.2172 - accuracy: 0.5417\n",
      "Epoch 70/150\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 1.2149 - accuracy: 0.5417\n",
      "Epoch 71/150\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.2130 - accuracy: 0.5417\n",
      "Epoch 72/150\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.2105 - accuracy: 0.5417\n",
      "Epoch 73/150\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.2081 - accuracy: 0.5417\n",
      "Epoch 74/150\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 1.2057 - accuracy: 0.5417\n",
      "Epoch 75/150\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.2037 - accuracy: 0.5417\n",
      "Epoch 76/150\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.2017 - accuracy: 0.5625\n",
      "Epoch 77/150\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.1997 - accuracy: 0.5625\n",
      "Epoch 78/150\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.1974 - accuracy: 0.5417\n",
      "Epoch 79/150\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.1953 - accuracy: 0.5417\n",
      "Epoch 80/150\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.1933 - accuracy: 0.5417\n",
      "Epoch 81/150\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 1.1911 - accuracy: 0.5417\n",
      "Epoch 82/150\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.1890 - accuracy: 0.5417\n",
      "Epoch 83/150\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.1874 - accuracy: 0.5417\n",
      "Epoch 84/150\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.1852 - accuracy: 0.5417\n",
      "Epoch 85/150\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.1832 - accuracy: 0.5625\n",
      "Epoch 86/150\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.1813 - accuracy: 0.5208\n",
      "Epoch 87/150\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.1793 - accuracy: 0.5417\n",
      "Epoch 88/150\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.1772 - accuracy: 0.5417\n",
      "Epoch 89/150\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.1752 - accuracy: 0.5417\n",
      "Epoch 90/150\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.1730 - accuracy: 0.5417\n",
      "Epoch 91/150\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.1712 - accuracy: 0.5625\n",
      "Epoch 92/150\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.1693 - accuracy: 0.5625\n",
      "Epoch 93/150\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.1678 - accuracy: 0.5625\n",
      "Epoch 94/150\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.1658 - accuracy: 0.5833\n",
      "Epoch 95/150\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 1.1639 - accuracy: 0.6042\n",
      "Epoch 96/150\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.1620 - accuracy: 0.6042\n",
      "Epoch 97/150\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.1601 - accuracy: 0.5833\n",
      "Epoch 98/150\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.1583 - accuracy: 0.6042\n",
      "Epoch 99/150\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 1.1569 - accuracy: 0.6042\n",
      "Epoch 100/150\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.1551 - accuracy: 0.5833\n",
      "Epoch 101/150\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.1531 - accuracy: 0.6042\n",
      "Epoch 102/150\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.1516 - accuracy: 0.5833\n",
      "Epoch 103/150\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.1499 - accuracy: 0.6042\n",
      "Epoch 104/150\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 1.1479 - accuracy: 0.6042\n",
      "Epoch 105/150\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.1462 - accuracy: 0.6042\n",
      "Epoch 106/150\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.1444 - accuracy: 0.6042\n",
      "Epoch 107/150\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.1429 - accuracy: 0.5833\n",
      "Epoch 108/150\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.1408 - accuracy: 0.6042\n",
      "Epoch 109/150\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.1394 - accuracy: 0.6042\n",
      "Epoch 110/150\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.1378 - accuracy: 0.6042\n",
      "Epoch 111/150\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.1363 - accuracy: 0.6042\n",
      "Epoch 112/150\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 1.1347 - accuracy: 0.6042\n",
      "Epoch 113/150\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.1336 - accuracy: 0.6042\n",
      "Epoch 114/150\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.1317 - accuracy: 0.6042\n",
      "Epoch 115/150\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 1.1299 - accuracy: 0.6042\n",
      "Epoch 116/150\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.1284 - accuracy: 0.6042\n",
      "Epoch 117/150\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.1270 - accuracy: 0.6042\n",
      "Epoch 118/150\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.1254 - accuracy: 0.6042\n",
      "Epoch 119/150\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.1237 - accuracy: 0.6042\n",
      "Epoch 120/150\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.1221 - accuracy: 0.6042\n",
      "Epoch 121/150\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.1205 - accuracy: 0.6042\n",
      "Epoch 122/150\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.1192 - accuracy: 0.6042\n",
      "Epoch 123/150\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 1.1173 - accuracy: 0.6042\n",
      "Epoch 124/150\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 1.1161 - accuracy: 0.6042\n",
      "Epoch 125/150\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.1145 - accuracy: 0.6042\n",
      "Epoch 126/150\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.1130 - accuracy: 0.6042\n",
      "Epoch 127/150\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.1115 - accuracy: 0.6042\n",
      "Epoch 128/150\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.1100 - accuracy: 0.6042\n",
      "Epoch 129/150\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.1085 - accuracy: 0.6042\n",
      "Epoch 130/150\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.1073 - accuracy: 0.6042\n",
      "Epoch 131/150\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.1057 - accuracy: 0.6042\n",
      "Epoch 132/150\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.1042 - accuracy: 0.6042\n",
      "Epoch 133/150\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.1028 - accuracy: 0.6042\n",
      "Epoch 134/150\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.1014 - accuracy: 0.6042\n",
      "Epoch 135/150\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.1002 - accuracy: 0.6042\n",
      "Epoch 136/150\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 1.0984 - accuracy: 0.6042\n",
      "Epoch 137/150\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.0969 - accuracy: 0.6042\n",
      "Epoch 138/150\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.0955 - accuracy: 0.6042\n",
      "Epoch 139/150\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 1.0942 - accuracy: 0.6042\n",
      "Epoch 140/150\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.0931 - accuracy: 0.6042\n",
      "Epoch 141/150\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.0915 - accuracy: 0.6042\n",
      "Epoch 142/150\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.0903 - accuracy: 0.6042\n",
      "Epoch 143/150\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.0887 - accuracy: 0.6042\n",
      "Epoch 144/150\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.0877 - accuracy: 0.6042\n",
      "Epoch 145/150\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 1.0860 - accuracy: 0.6042\n",
      "Epoch 146/150\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.0849 - accuracy: 0.6042\n",
      "Epoch 147/150\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 1.0833 - accuracy: 0.6042\n",
      "Epoch 148/150\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.0818 - accuracy: 0.6042\n",
      "Epoch 149/150\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 1.0806 - accuracy: 0.6042\n",
      "Epoch 150/150\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 1.0796 - accuracy: 0.6042\n",
      "Model: \"sequential_5\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " flatten_5 (Flatten)         (None, 2)                 0         \n",
      "                                                                 \n",
      " dense_10 (Dense)            (None, 16)                48        \n",
      "                                                                 \n",
      " dense_11 (Dense)            (None, 4)                 68        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 116\n",
      "Trainable params: 116\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "WARNING:tensorflow:5 out of the last 13 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7f0ab01394c0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "2/2 [==============================] - 0s 2ms/step\n",
      "Accuracy on train set: 60.42%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 15ms/step\n",
      "Accuracy on test set: 37.50%\n",
      "[[0 4 3 0]\n",
      " [0 1 1 0]\n",
      " [0 1 2 0]\n",
      " [0 0 1 3]]\n"
     ]
    }
   ],
   "source": [
    "run_mlp(X_train, y_train, X_test, y_test, 4, 16, 150) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<function matplotlib.pyplot.show(close=None, block=None)>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "ac = [18.8, 31.2, 43.8, 56.3, 93.8]\n",
    "ep = [1, 15, 30, 75, 150]\n",
    "\n",
    "plt.scatter(ep, ac)\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.ylabel(\"Accuracy\")\n",
    "plt.show"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reflection\n",
    "After running the mlp model with varying epochs, one can say that accuracy has a mostly linear relationship to number of epochs when testing mlp models. Also, one can see that overfitting reduced as more epochs were set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 3:\n",
    "Experiment in the next cell by trying different numbers of neurons in the hidden layer.  Identify the smallest number of hidden neurons you could use and still achieve high accuracy.  Create a table in markdown in the reflection section to show your experimental results.  Make sure your table adequately documents your experimental variables (hyperparameters, dataset) to enable reproducability.  Write a few statements in your reflection about the results.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/150\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 1.3329 - accuracy: 0.3750\n",
      "Epoch 2/150\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.3296 - accuracy: 0.3542\n",
      "Epoch 3/150\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 1.3268 - accuracy: 0.3542\n",
      "Epoch 4/150\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 1.3232 - accuracy: 0.3750\n",
      "Epoch 5/150\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.3201 - accuracy: 0.3750\n",
      "Epoch 6/150\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.3168 - accuracy: 0.3750\n",
      "Epoch 7/150\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 1.3142 - accuracy: 0.3750\n",
      "Epoch 8/150\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.3097 - accuracy: 0.3958\n",
      "Epoch 9/150\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 1.3066 - accuracy: 0.3958\n",
      "Epoch 10/150\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.3031 - accuracy: 0.3958\n",
      "Epoch 11/150\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.3000 - accuracy: 0.3958\n",
      "Epoch 12/150\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.2963 - accuracy: 0.3958\n",
      "Epoch 13/150\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.2932 - accuracy: 0.4167\n",
      "Epoch 14/150\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.2901 - accuracy: 0.4167\n",
      "Epoch 15/150\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 1.2872 - accuracy: 0.4167\n",
      "Epoch 16/150\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.2844 - accuracy: 0.4375\n",
      "Epoch 17/150\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.2813 - accuracy: 0.4375\n",
      "Epoch 18/150\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.2783 - accuracy: 0.4375\n",
      "Epoch 19/150\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.2756 - accuracy: 0.4792\n",
      "Epoch 20/150\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 1.2727 - accuracy: 0.4583\n",
      "Epoch 21/150\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 1.2699 - accuracy: 0.4792\n",
      "Epoch 22/150\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.2672 - accuracy: 0.4583\n",
      "Epoch 23/150\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.2643 - accuracy: 0.4583\n",
      "Epoch 24/150\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.2615 - accuracy: 0.4792\n",
      "Epoch 25/150\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.2591 - accuracy: 0.4792\n",
      "Epoch 26/150\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.2559 - accuracy: 0.4792\n",
      "Epoch 27/150\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 1.2530 - accuracy: 0.4792\n",
      "Epoch 28/150\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.2501 - accuracy: 0.4792\n",
      "Epoch 29/150\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 1.2476 - accuracy: 0.4792\n",
      "Epoch 30/150\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 1.2450 - accuracy: 0.4792\n",
      "Epoch 31/150\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 1.2421 - accuracy: 0.5000\n",
      "Epoch 32/150\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.2395 - accuracy: 0.5000\n",
      "Epoch 33/150\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.2368 - accuracy: 0.5000\n",
      "Epoch 34/150\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.2342 - accuracy: 0.5000\n",
      "Epoch 35/150\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.2315 - accuracy: 0.5000\n",
      "Epoch 36/150\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.2290 - accuracy: 0.5000\n",
      "Epoch 37/150\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.2267 - accuracy: 0.5000\n",
      "Epoch 38/150\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.2236 - accuracy: 0.5000\n",
      "Epoch 39/150\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.2211 - accuracy: 0.5000\n",
      "Epoch 40/150\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 1.2183 - accuracy: 0.5000\n",
      "Epoch 41/150\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 1.2159 - accuracy: 0.5000\n",
      "Epoch 42/150\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.2133 - accuracy: 0.5000\n",
      "Epoch 43/150\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 1.2109 - accuracy: 0.5000\n",
      "Epoch 44/150\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.2085 - accuracy: 0.5000\n",
      "Epoch 45/150\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.2059 - accuracy: 0.5000\n",
      "Epoch 46/150\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.2034 - accuracy: 0.5000\n",
      "Epoch 47/150\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 1.2010 - accuracy: 0.5000\n",
      "Epoch 48/150\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 1.1984 - accuracy: 0.5000\n",
      "Epoch 49/150\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 1.1964 - accuracy: 0.5000\n",
      "Epoch 50/150\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 1.1939 - accuracy: 0.5000\n",
      "Epoch 51/150\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 1.1915 - accuracy: 0.5208\n",
      "Epoch 52/150\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.1896 - accuracy: 0.5208\n",
      "Epoch 53/150\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.1871 - accuracy: 0.5208\n",
      "Epoch 54/150\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.1850 - accuracy: 0.5208\n",
      "Epoch 55/150\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.1827 - accuracy: 0.5208\n",
      "Epoch 56/150\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.1804 - accuracy: 0.5000\n",
      "Epoch 57/150\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.1782 - accuracy: 0.5000\n",
      "Epoch 58/150\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.1759 - accuracy: 0.5000\n",
      "Epoch 59/150\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.1737 - accuracy: 0.5000\n",
      "Epoch 60/150\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.1716 - accuracy: 0.5000\n",
      "Epoch 61/150\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 1.1696 - accuracy: 0.5000\n",
      "Epoch 62/150\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.1675 - accuracy: 0.5000\n",
      "Epoch 63/150\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.1653 - accuracy: 0.5208\n",
      "Epoch 64/150\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.1635 - accuracy: 0.5208\n",
      "Epoch 65/150\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 1.1610 - accuracy: 0.5208\n",
      "Epoch 66/150\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 1.1589 - accuracy: 0.5208\n",
      "Epoch 67/150\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.1569 - accuracy: 0.5208\n",
      "Epoch 68/150\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.1550 - accuracy: 0.5208\n",
      "Epoch 69/150\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.1530 - accuracy: 0.5208\n",
      "Epoch 70/150\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 1.1512 - accuracy: 0.5208\n",
      "Epoch 71/150\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.1492 - accuracy: 0.5208\n",
      "Epoch 72/150\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.1470 - accuracy: 0.5417\n",
      "Epoch 73/150\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 1.1452 - accuracy: 0.5208\n",
      "Epoch 74/150\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.1434 - accuracy: 0.5417\n",
      "Epoch 75/150\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 1.1416 - accuracy: 0.5208\n",
      "Epoch 76/150\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 1.1397 - accuracy: 0.5208\n",
      "Epoch 77/150\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.1378 - accuracy: 0.5208\n",
      "Epoch 78/150\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.1360 - accuracy: 0.5208\n",
      "Epoch 79/150\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 1.1342 - accuracy: 0.5208\n",
      "Epoch 80/150\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.1324 - accuracy: 0.5208\n",
      "Epoch 81/150\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 1.1303 - accuracy: 0.5208\n",
      "Epoch 82/150\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.1284 - accuracy: 0.5208\n",
      "Epoch 83/150\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 1.1268 - accuracy: 0.5208\n",
      "Epoch 84/150\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.1251 - accuracy: 0.5208\n",
      "Epoch 85/150\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.1234 - accuracy: 0.5208\n",
      "Epoch 86/150\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.1217 - accuracy: 0.5208\n",
      "Epoch 87/150\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.1200 - accuracy: 0.5208\n",
      "Epoch 88/150\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.1183 - accuracy: 0.5000\n",
      "Epoch 89/150\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.1164 - accuracy: 0.5000\n",
      "Epoch 90/150\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 1.1147 - accuracy: 0.5000\n",
      "Epoch 91/150\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 1.1132 - accuracy: 0.5000\n",
      "Epoch 92/150\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.1114 - accuracy: 0.5000\n",
      "Epoch 93/150\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.1099 - accuracy: 0.5000\n",
      "Epoch 94/150\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.1080 - accuracy: 0.5000\n",
      "Epoch 95/150\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.1065 - accuracy: 0.5000\n",
      "Epoch 96/150\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.1048 - accuracy: 0.5000\n",
      "Epoch 97/150\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.1033 - accuracy: 0.5000\n",
      "Epoch 98/150\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.1016 - accuracy: 0.5208\n",
      "Epoch 99/150\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.1000 - accuracy: 0.5208\n",
      "Epoch 100/150\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.0983 - accuracy: 0.5208\n",
      "Epoch 101/150\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.0968 - accuracy: 0.5208\n",
      "Epoch 102/150\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.0954 - accuracy: 0.5208\n",
      "Epoch 103/150\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.0933 - accuracy: 0.5208\n",
      "Epoch 104/150\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 1.0917 - accuracy: 0.5208\n",
      "Epoch 105/150\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.0903 - accuracy: 0.5208\n",
      "Epoch 106/150\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.0889 - accuracy: 0.5208\n",
      "Epoch 107/150\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.0871 - accuracy: 0.5208\n",
      "Epoch 108/150\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 1.0856 - accuracy: 0.5208\n",
      "Epoch 109/150\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.0841 - accuracy: 0.5208\n",
      "Epoch 110/150\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.0825 - accuracy: 0.5417\n",
      "Epoch 111/150\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.0812 - accuracy: 0.5417\n",
      "Epoch 112/150\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.0797 - accuracy: 0.5417\n",
      "Epoch 113/150\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 1.0783 - accuracy: 0.5417\n",
      "Epoch 114/150\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.0769 - accuracy: 0.5417\n",
      "Epoch 115/150\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.0753 - accuracy: 0.5417\n",
      "Epoch 116/150\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.0739 - accuracy: 0.5417\n",
      "Epoch 117/150\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.0725 - accuracy: 0.5625\n",
      "Epoch 118/150\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.0716 - accuracy: 0.5625\n",
      "Epoch 119/150\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.0699 - accuracy: 0.5625\n",
      "Epoch 120/150\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.0683 - accuracy: 0.5625\n",
      "Epoch 121/150\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 1.0671 - accuracy: 0.5625\n",
      "Epoch 122/150\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 1.0655 - accuracy: 0.5833\n",
      "Epoch 123/150\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 1.0640 - accuracy: 0.5833\n",
      "Epoch 124/150\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 1.0626 - accuracy: 0.5833\n",
      "Epoch 125/150\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.0610 - accuracy: 0.5833\n",
      "Epoch 126/150\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.0600 - accuracy: 0.5833\n",
      "Epoch 127/150\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 1.0582 - accuracy: 0.5625\n",
      "Epoch 128/150\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.0569 - accuracy: 0.5625\n",
      "Epoch 129/150\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.0556 - accuracy: 0.5625\n",
      "Epoch 130/150\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.0543 - accuracy: 0.5833\n",
      "Epoch 131/150\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.0528 - accuracy: 0.6042\n",
      "Epoch 132/150\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.0518 - accuracy: 0.6042\n",
      "Epoch 133/150\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.0502 - accuracy: 0.6042\n",
      "Epoch 134/150\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.0489 - accuracy: 0.6042\n",
      "Epoch 135/150\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.0476 - accuracy: 0.6042\n",
      "Epoch 136/150\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 1.0465 - accuracy: 0.6042\n",
      "Epoch 137/150\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 1.0451 - accuracy: 0.6042\n",
      "Epoch 138/150\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 1.0438 - accuracy: 0.6042\n",
      "Epoch 139/150\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.0426 - accuracy: 0.6042\n",
      "Epoch 140/150\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.0415 - accuracy: 0.6042\n",
      "Epoch 141/150\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.0404 - accuracy: 0.6042\n",
      "Epoch 142/150\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.0392 - accuracy: 0.6042\n",
      "Epoch 143/150\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.0380 - accuracy: 0.6042\n",
      "Epoch 144/150\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.0369 - accuracy: 0.6042\n",
      "Epoch 145/150\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 1.0357 - accuracy: 0.5833\n",
      "Epoch 146/150\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.0345 - accuracy: 0.5833\n",
      "Epoch 147/150\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 1.0335 - accuracy: 0.5833\n",
      "Epoch 148/150\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.0324 - accuracy: 0.5833\n",
      "Epoch 149/150\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.0311 - accuracy: 0.5833\n",
      "Epoch 150/150\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.0301 - accuracy: 0.5833\n",
      "Model: \"sequential_6\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " flatten_6 (Flatten)         (None, 2)                 0         \n",
      "                                                                 \n",
      " dense_12 (Dense)            (None, 10)                30        \n",
      "                                                                 \n",
      " dense_13 (Dense)            (None, 4)                 44        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 74\n",
      "Trainable params: 74\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Accuracy on train set: 58.33%\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "Accuracy on test set: 43.75%\n",
      "[[4 1 2 0]\n",
      " [0 1 1 0]\n",
      " [2 1 0 0]\n",
      " [0 0 2 2]]\n"
     ]
    }
   ],
   "source": [
    "run_mlp(X_train, y_train, X_test, y_test, 4, 10, 150) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/150\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 1.7398 - accuracy: 0.2500\n",
      "Epoch 2/150\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.7290 - accuracy: 0.2500\n",
      "Epoch 3/150\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.7176 - accuracy: 0.2500\n",
      "Epoch 4/150\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.7074 - accuracy: 0.2708\n",
      "Epoch 5/150\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.6980 - accuracy: 0.2708\n",
      "Epoch 6/150\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.6872 - accuracy: 0.2708\n",
      "Epoch 7/150\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.6782 - accuracy: 0.2708\n",
      "Epoch 8/150\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.6699 - accuracy: 0.2708\n",
      "Epoch 9/150\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.6614 - accuracy: 0.2708\n",
      "Epoch 10/150\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.6531 - accuracy: 0.2708\n",
      "Epoch 11/150\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.6458 - accuracy: 0.2708\n",
      "Epoch 12/150\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.6390 - accuracy: 0.2917\n",
      "Epoch 13/150\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.6316 - accuracy: 0.2708\n",
      "Epoch 14/150\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.6246 - accuracy: 0.2708\n",
      "Epoch 15/150\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.6178 - accuracy: 0.2708\n",
      "Epoch 16/150\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.6112 - accuracy: 0.2917\n",
      "Epoch 17/150\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 1.6046 - accuracy: 0.3125\n",
      "Epoch 18/150\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.5976 - accuracy: 0.3125\n",
      "Epoch 19/150\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.5916 - accuracy: 0.3125\n",
      "Epoch 20/150\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.5850 - accuracy: 0.3125\n",
      "Epoch 21/150\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.5789 - accuracy: 0.3125\n",
      "Epoch 22/150\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 1.5733 - accuracy: 0.3125\n",
      "Epoch 23/150\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.5678 - accuracy: 0.3125\n",
      "Epoch 24/150\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.5624 - accuracy: 0.3125\n",
      "Epoch 25/150\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.5567 - accuracy: 0.3125\n",
      "Epoch 26/150\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.5519 - accuracy: 0.3125\n",
      "Epoch 27/150\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.5470 - accuracy: 0.3125\n",
      "Epoch 28/150\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.5423 - accuracy: 0.3125\n",
      "Epoch 29/150\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.5375 - accuracy: 0.3125\n",
      "Epoch 30/150\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.5325 - accuracy: 0.3333\n",
      "Epoch 31/150\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.5279 - accuracy: 0.3333\n",
      "Epoch 32/150\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.5236 - accuracy: 0.3542\n",
      "Epoch 33/150\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.5195 - accuracy: 0.3542\n",
      "Epoch 34/150\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 1.5153 - accuracy: 0.3333\n",
      "Epoch 35/150\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 1.5110 - accuracy: 0.3333\n",
      "Epoch 36/150\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.5074 - accuracy: 0.3333\n",
      "Epoch 37/150\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.5038 - accuracy: 0.3333\n",
      "Epoch 38/150\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 1.5003 - accuracy: 0.3333\n",
      "Epoch 39/150\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 1.4967 - accuracy: 0.3333\n",
      "Epoch 40/150\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 1.4931 - accuracy: 0.3333\n",
      "Epoch 41/150\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 1.4896 - accuracy: 0.3333\n",
      "Epoch 42/150\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 1.4863 - accuracy: 0.3333\n",
      "Epoch 43/150\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 1.4830 - accuracy: 0.3542\n",
      "Epoch 44/150\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.4795 - accuracy: 0.3542\n",
      "Epoch 45/150\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.4756 - accuracy: 0.3542\n",
      "Epoch 46/150\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 1.4720 - accuracy: 0.3542\n",
      "Epoch 47/150\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 1.4688 - accuracy: 0.3542\n",
      "Epoch 48/150\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.4657 - accuracy: 0.3542\n",
      "Epoch 49/150\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 1.4627 - accuracy: 0.3542\n",
      "Epoch 50/150\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.4591 - accuracy: 0.3542\n",
      "Epoch 51/150\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.4562 - accuracy: 0.3542\n",
      "Epoch 52/150\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.4535 - accuracy: 0.3542\n",
      "Epoch 53/150\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.4506 - accuracy: 0.3542\n",
      "Epoch 54/150\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.4478 - accuracy: 0.3542\n",
      "Epoch 55/150\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.4450 - accuracy: 0.3333\n",
      "Epoch 56/150\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.4424 - accuracy: 0.3333\n",
      "Epoch 57/150\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.4394 - accuracy: 0.3333\n",
      "Epoch 58/150\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.4369 - accuracy: 0.3333\n",
      "Epoch 59/150\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.4349 - accuracy: 0.3333\n",
      "Epoch 60/150\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 1.4317 - accuracy: 0.3125\n",
      "Epoch 61/150\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.4296 - accuracy: 0.3125\n",
      "Epoch 62/150\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.4269 - accuracy: 0.3125\n",
      "Epoch 63/150\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.4243 - accuracy: 0.3125\n",
      "Epoch 64/150\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.4219 - accuracy: 0.3333\n",
      "Epoch 65/150\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 1.4194 - accuracy: 0.3333\n",
      "Epoch 66/150\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.4172 - accuracy: 0.3333\n",
      "Epoch 67/150\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.4149 - accuracy: 0.3333\n",
      "Epoch 68/150\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 1.4128 - accuracy: 0.3333\n",
      "Epoch 69/150\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 1.4105 - accuracy: 0.3333\n",
      "Epoch 70/150\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 1.4081 - accuracy: 0.3333\n",
      "Epoch 71/150\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 1.4058 - accuracy: 0.3333\n",
      "Epoch 72/150\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.4037 - accuracy: 0.3333\n",
      "Epoch 73/150\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.4015 - accuracy: 0.3333\n",
      "Epoch 74/150\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 1.3992 - accuracy: 0.3125\n",
      "Epoch 75/150\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.3974 - accuracy: 0.2917\n",
      "Epoch 76/150\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.3953 - accuracy: 0.3125\n",
      "Epoch 77/150\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.3931 - accuracy: 0.3333\n",
      "Epoch 78/150\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.3907 - accuracy: 0.3125\n",
      "Epoch 79/150\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 1.3884 - accuracy: 0.3333\n",
      "Epoch 80/150\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.3864 - accuracy: 0.3333\n",
      "Epoch 81/150\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.3843 - accuracy: 0.3333\n",
      "Epoch 82/150\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.3823 - accuracy: 0.3333\n",
      "Epoch 83/150\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.3802 - accuracy: 0.3333\n",
      "Epoch 84/150\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 1.3781 - accuracy: 0.3333\n",
      "Epoch 85/150\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 1.3762 - accuracy: 0.3333\n",
      "Epoch 86/150\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 1.3746 - accuracy: 0.3333\n",
      "Epoch 87/150\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 1.3723 - accuracy: 0.3333\n",
      "Epoch 88/150\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.3706 - accuracy: 0.3333\n",
      "Epoch 89/150\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 1.3688 - accuracy: 0.3333\n",
      "Epoch 90/150\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 1.3670 - accuracy: 0.3333\n",
      "Epoch 91/150\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.3652 - accuracy: 0.3333\n",
      "Epoch 92/150\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.3634 - accuracy: 0.3333\n",
      "Epoch 93/150\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 1.3616 - accuracy: 0.3333\n",
      "Epoch 94/150\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.3599 - accuracy: 0.3333\n",
      "Epoch 95/150\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.3581 - accuracy: 0.3542\n",
      "Epoch 96/150\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.3564 - accuracy: 0.3542\n",
      "Epoch 97/150\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.3550 - accuracy: 0.3542\n",
      "Epoch 98/150\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 1.3532 - accuracy: 0.3542\n",
      "Epoch 99/150\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 1.3513 - accuracy: 0.3542\n",
      "Epoch 100/150\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.3499 - accuracy: 0.3750\n",
      "Epoch 101/150\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.3482 - accuracy: 0.3750\n",
      "Epoch 102/150\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.3469 - accuracy: 0.3750\n",
      "Epoch 103/150\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.3451 - accuracy: 0.3750\n",
      "Epoch 104/150\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.3437 - accuracy: 0.3958\n",
      "Epoch 105/150\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.3420 - accuracy: 0.3958\n",
      "Epoch 106/150\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.3405 - accuracy: 0.3958\n",
      "Epoch 107/150\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.3387 - accuracy: 0.3958\n",
      "Epoch 108/150\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.3370 - accuracy: 0.3958\n",
      "Epoch 109/150\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 1.3353 - accuracy: 0.3958\n",
      "Epoch 110/150\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 1.3336 - accuracy: 0.3958\n",
      "Epoch 111/150\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 1.3323 - accuracy: 0.3958\n",
      "Epoch 112/150\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 1.3305 - accuracy: 0.3958\n",
      "Epoch 113/150\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.3291 - accuracy: 0.3958\n",
      "Epoch 114/150\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.3276 - accuracy: 0.3958\n",
      "Epoch 115/150\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.3261 - accuracy: 0.3958\n",
      "Epoch 116/150\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.3249 - accuracy: 0.3958\n",
      "Epoch 117/150\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.3233 - accuracy: 0.3958\n",
      "Epoch 118/150\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.3221 - accuracy: 0.3958\n",
      "Epoch 119/150\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.3208 - accuracy: 0.3958\n",
      "Epoch 120/150\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.3194 - accuracy: 0.3750\n",
      "Epoch 121/150\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.3182 - accuracy: 0.3750\n",
      "Epoch 122/150\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.3167 - accuracy: 0.3750\n",
      "Epoch 123/150\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.3152 - accuracy: 0.3750\n",
      "Epoch 124/150\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.3139 - accuracy: 0.3542\n",
      "Epoch 125/150\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.3125 - accuracy: 0.3750\n",
      "Epoch 126/150\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.3111 - accuracy: 0.3750\n",
      "Epoch 127/150\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.3099 - accuracy: 0.3750\n",
      "Epoch 128/150\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.3083 - accuracy: 0.3750\n",
      "Epoch 129/150\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.3070 - accuracy: 0.3958\n",
      "Epoch 130/150\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.3056 - accuracy: 0.3958\n",
      "Epoch 131/150\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.3044 - accuracy: 0.3958\n",
      "Epoch 132/150\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.3030 - accuracy: 0.4167\n",
      "Epoch 133/150\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 1.3018 - accuracy: 0.4167\n",
      "Epoch 134/150\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.3004 - accuracy: 0.4167\n",
      "Epoch 135/150\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.2995 - accuracy: 0.4167\n",
      "Epoch 136/150\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 1.2978 - accuracy: 0.4167\n",
      "Epoch 137/150\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.2966 - accuracy: 0.4167\n",
      "Epoch 138/150\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.2954 - accuracy: 0.4167\n",
      "Epoch 139/150\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.2941 - accuracy: 0.4167\n",
      "Epoch 140/150\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 1.2926 - accuracy: 0.4167\n",
      "Epoch 141/150\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.2914 - accuracy: 0.4167\n",
      "Epoch 142/150\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.2902 - accuracy: 0.4167\n",
      "Epoch 143/150\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.2889 - accuracy: 0.4167\n",
      "Epoch 144/150\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.2876 - accuracy: 0.4167\n",
      "Epoch 145/150\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.2865 - accuracy: 0.4167\n",
      "Epoch 146/150\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.2849 - accuracy: 0.4167\n",
      "Epoch 147/150\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.2836 - accuracy: 0.4167\n",
      "Epoch 148/150\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.2827 - accuracy: 0.4167\n",
      "Epoch 149/150\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.2816 - accuracy: 0.4167\n",
      "Epoch 150/150\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.2805 - accuracy: 0.4167\n",
      "Model: \"sequential_7\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " flatten_7 (Flatten)         (None, 2)                 0         \n",
      "                                                                 \n",
      " dense_14 (Dense)            (None, 5)                 15        \n",
      "                                                                 \n",
      " dense_15 (Dense)            (None, 4)                 24        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 39\n",
      "Trainable params: 39\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "2/2 [==============================] - 0s 2ms/step\n",
      "Accuracy on train set: 41.67%\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "Accuracy on test set: 18.75%\n",
      "[[0 3 3 1]\n",
      " [0 1 1 0]\n",
      " [0 0 2 1]\n",
      " [0 2 2 0]]\n"
     ]
    }
   ],
   "source": [
    "run_mlp(X_train, y_train, X_test, y_test, 4, 5, 150) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/150\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.7840 - accuracy: 0.2500\n",
      "Epoch 2/150\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.7698 - accuracy: 0.2500\n",
      "Epoch 3/150\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 1.7565 - accuracy: 0.2500\n",
      "Epoch 4/150\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.7428 - accuracy: 0.2500\n",
      "Epoch 5/150\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.7289 - accuracy: 0.2500\n",
      "Epoch 6/150\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 1.7162 - accuracy: 0.2500\n",
      "Epoch 7/150\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.7043 - accuracy: 0.2500\n",
      "Epoch 8/150\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.6937 - accuracy: 0.2500\n",
      "Epoch 9/150\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.6825 - accuracy: 0.2500\n",
      "Epoch 10/150\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 1.6706 - accuracy: 0.2500\n",
      "Epoch 11/150\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 1.6598 - accuracy: 0.2708\n",
      "Epoch 12/150\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.6494 - accuracy: 0.2708\n",
      "Epoch 13/150\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.6385 - accuracy: 0.2708\n",
      "Epoch 14/150\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.6282 - accuracy: 0.2708\n",
      "Epoch 15/150\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 1.6174 - accuracy: 0.2708\n",
      "Epoch 16/150\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.6081 - accuracy: 0.2708\n",
      "Epoch 17/150\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 1.5991 - accuracy: 0.2708\n",
      "Epoch 18/150\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 1.5907 - accuracy: 0.2708\n",
      "Epoch 19/150\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.5830 - accuracy: 0.2708\n",
      "Epoch 20/150\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 1.5760 - accuracy: 0.2708\n",
      "Epoch 21/150\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.5688 - accuracy: 0.2917\n",
      "Epoch 22/150\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.5616 - accuracy: 0.2917\n",
      "Epoch 23/150\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.5534 - accuracy: 0.2917\n",
      "Epoch 24/150\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.5462 - accuracy: 0.2917\n",
      "Epoch 25/150\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.5386 - accuracy: 0.2917\n",
      "Epoch 26/150\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.5323 - accuracy: 0.2917\n",
      "Epoch 27/150\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.5257 - accuracy: 0.2917\n",
      "Epoch 28/150\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 1.5194 - accuracy: 0.2917\n",
      "Epoch 29/150\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.5129 - accuracy: 0.2917\n",
      "Epoch 30/150\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 1.5066 - accuracy: 0.2917\n",
      "Epoch 31/150\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.5004 - accuracy: 0.2917\n",
      "Epoch 32/150\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.4946 - accuracy: 0.2917\n",
      "Epoch 33/150\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 1.4886 - accuracy: 0.2917\n",
      "Epoch 34/150\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.4839 - accuracy: 0.2708\n",
      "Epoch 35/150\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.4795 - accuracy: 0.2708\n",
      "Epoch 36/150\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.4750 - accuracy: 0.2708\n",
      "Epoch 37/150\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 1.4697 - accuracy: 0.2708\n",
      "Epoch 38/150\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.4654 - accuracy: 0.2708\n",
      "Epoch 39/150\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 1.4610 - accuracy: 0.2500\n",
      "Epoch 40/150\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.4567 - accuracy: 0.2500\n",
      "Epoch 41/150\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.4525 - accuracy: 0.2500\n",
      "Epoch 42/150\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.4484 - accuracy: 0.2500\n",
      "Epoch 43/150\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.4443 - accuracy: 0.2500\n",
      "Epoch 44/150\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.4403 - accuracy: 0.2500\n",
      "Epoch 45/150\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.4357 - accuracy: 0.2500\n",
      "Epoch 46/150\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.4321 - accuracy: 0.2500\n",
      "Epoch 47/150\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.4288 - accuracy: 0.2500\n",
      "Epoch 48/150\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.4252 - accuracy: 0.2500\n",
      "Epoch 49/150\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 1.4221 - accuracy: 0.2292\n",
      "Epoch 50/150\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.4194 - accuracy: 0.2292\n",
      "Epoch 51/150\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.4164 - accuracy: 0.2292\n",
      "Epoch 52/150\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 1.4131 - accuracy: 0.2292\n",
      "Epoch 53/150\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.4102 - accuracy: 0.2292\n",
      "Epoch 54/150\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.4075 - accuracy: 0.2292\n",
      "Epoch 55/150\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.4049 - accuracy: 0.2292\n",
      "Epoch 56/150\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.4019 - accuracy: 0.2292\n",
      "Epoch 57/150\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.3990 - accuracy: 0.2292\n",
      "Epoch 58/150\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.3964 - accuracy: 0.2292\n",
      "Epoch 59/150\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 1.3938 - accuracy: 0.2292\n",
      "Epoch 60/150\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.3911 - accuracy: 0.2292\n",
      "Epoch 61/150\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.3888 - accuracy: 0.2292\n",
      "Epoch 62/150\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.3863 - accuracy: 0.2292\n",
      "Epoch 63/150\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.3839 - accuracy: 0.2292\n",
      "Epoch 64/150\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.3815 - accuracy: 0.2500\n",
      "Epoch 65/150\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.3795 - accuracy: 0.2500\n",
      "Epoch 66/150\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.3773 - accuracy: 0.2500\n",
      "Epoch 67/150\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.3752 - accuracy: 0.2500\n",
      "Epoch 68/150\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 1.3732 - accuracy: 0.2500\n",
      "Epoch 69/150\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.3712 - accuracy: 0.2500\n",
      "Epoch 70/150\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 1.3692 - accuracy: 0.2500\n",
      "Epoch 71/150\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 1.3676 - accuracy: 0.2500\n",
      "Epoch 72/150\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 1.3658 - accuracy: 0.2500\n",
      "Epoch 73/150\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.3640 - accuracy: 0.2500\n",
      "Epoch 74/150\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.3619 - accuracy: 0.2500\n",
      "Epoch 75/150\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.3603 - accuracy: 0.2500\n",
      "Epoch 76/150\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.3589 - accuracy: 0.2500\n",
      "Epoch 77/150\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.3573 - accuracy: 0.2500\n",
      "Epoch 78/150\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.3558 - accuracy: 0.2500\n",
      "Epoch 79/150\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.3542 - accuracy: 0.2500\n",
      "Epoch 80/150\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 1.3525 - accuracy: 0.2500\n",
      "Epoch 81/150\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 1.3511 - accuracy: 0.2500\n",
      "Epoch 82/150\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.3494 - accuracy: 0.2708\n",
      "Epoch 83/150\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.3483 - accuracy: 0.2708\n",
      "Epoch 84/150\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.3470 - accuracy: 0.2708\n",
      "Epoch 85/150\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.3458 - accuracy: 0.2708\n",
      "Epoch 86/150\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.3446 - accuracy: 0.2708\n",
      "Epoch 87/150\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.3434 - accuracy: 0.2708\n",
      "Epoch 88/150\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.3419 - accuracy: 0.2708\n",
      "Epoch 89/150\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.3406 - accuracy: 0.2708\n",
      "Epoch 90/150\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.3399 - accuracy: 0.2708\n",
      "Epoch 91/150\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.3385 - accuracy: 0.2708\n",
      "Epoch 92/150\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.3374 - accuracy: 0.2917\n",
      "Epoch 93/150\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.3367 - accuracy: 0.2708\n",
      "Epoch 94/150\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.3357 - accuracy: 0.2500\n",
      "Epoch 95/150\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.3349 - accuracy: 0.2500\n",
      "Epoch 96/150\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.3334 - accuracy: 0.2708\n",
      "Epoch 97/150\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.3324 - accuracy: 0.2708\n",
      "Epoch 98/150\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.3315 - accuracy: 0.2708\n",
      "Epoch 99/150\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.3306 - accuracy: 0.2708\n",
      "Epoch 100/150\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.3297 - accuracy: 0.2708\n",
      "Epoch 101/150\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.3286 - accuracy: 0.2500\n",
      "Epoch 102/150\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.3278 - accuracy: 0.2500\n",
      "Epoch 103/150\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.3268 - accuracy: 0.2500\n",
      "Epoch 104/150\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.3260 - accuracy: 0.2500\n",
      "Epoch 105/150\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.3250 - accuracy: 0.2500\n",
      "Epoch 106/150\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.3240 - accuracy: 0.2500\n",
      "Epoch 107/150\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 1.3234 - accuracy: 0.2500\n",
      "Epoch 108/150\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.3224 - accuracy: 0.2500\n",
      "Epoch 109/150\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.3217 - accuracy: 0.2500\n",
      "Epoch 110/150\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.3208 - accuracy: 0.2500\n",
      "Epoch 111/150\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 1.3198 - accuracy: 0.2500\n",
      "Epoch 112/150\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 1.3189 - accuracy: 0.2500\n",
      "Epoch 113/150\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.3181 - accuracy: 0.2500\n",
      "Epoch 114/150\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.3172 - accuracy: 0.2500\n",
      "Epoch 115/150\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.3164 - accuracy: 0.2500\n",
      "Epoch 116/150\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.3157 - accuracy: 0.2292\n",
      "Epoch 117/150\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.3150 - accuracy: 0.2500\n",
      "Epoch 118/150\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.3142 - accuracy: 0.2292\n",
      "Epoch 119/150\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.3135 - accuracy: 0.2292\n",
      "Epoch 120/150\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.3129 - accuracy: 0.2083\n",
      "Epoch 121/150\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.3121 - accuracy: 0.2292\n",
      "Epoch 122/150\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.3115 - accuracy: 0.2500\n",
      "Epoch 123/150\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.3108 - accuracy: 0.2292\n",
      "Epoch 124/150\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 1.3102 - accuracy: 0.2500\n",
      "Epoch 125/150\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.3095 - accuracy: 0.2500\n",
      "Epoch 126/150\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.3089 - accuracy: 0.2083\n",
      "Epoch 127/150\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.3082 - accuracy: 0.2292\n",
      "Epoch 128/150\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.3075 - accuracy: 0.2292\n",
      "Epoch 129/150\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.3069 - accuracy: 0.2292\n",
      "Epoch 130/150\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.3063 - accuracy: 0.2292\n",
      "Epoch 131/150\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 1.3054 - accuracy: 0.2292\n",
      "Epoch 132/150\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.3048 - accuracy: 0.2292\n",
      "Epoch 133/150\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.3044 - accuracy: 0.2083\n",
      "Epoch 134/150\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.3035 - accuracy: 0.2292\n",
      "Epoch 135/150\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 1.3030 - accuracy: 0.2500\n",
      "Epoch 136/150\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.3022 - accuracy: 0.2708\n",
      "Epoch 137/150\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.3016 - accuracy: 0.2708\n",
      "Epoch 138/150\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.3011 - accuracy: 0.2708\n",
      "Epoch 139/150\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.3008 - accuracy: 0.2708\n",
      "Epoch 140/150\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.2996 - accuracy: 0.2500\n",
      "Epoch 141/150\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.2990 - accuracy: 0.2500\n",
      "Epoch 142/150\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.2986 - accuracy: 0.2500\n",
      "Epoch 143/150\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 1.2979 - accuracy: 0.2500\n",
      "Epoch 144/150\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.2973 - accuracy: 0.2500\n",
      "Epoch 145/150\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.2969 - accuracy: 0.2500\n",
      "Epoch 146/150\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.2964 - accuracy: 0.2500\n",
      "Epoch 147/150\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 1.2958 - accuracy: 0.2708\n",
      "Epoch 148/150\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.2951 - accuracy: 0.2708\n",
      "Epoch 149/150\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 1.2944 - accuracy: 0.2708\n",
      "Epoch 150/150\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.2938 - accuracy: 0.2708\n",
      "Model: \"sequential_8\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " flatten_8 (Flatten)         (None, 2)                 0         \n",
      "                                                                 \n",
      " dense_16 (Dense)            (None, 3)                 9         \n",
      "                                                                 \n",
      " dense_17 (Dense)            (None, 4)                 16        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 25\n",
      "Trainable params: 25\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "2/2 [==============================] - 0s 2ms/step\n",
      "Accuracy on train set: 27.08%\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "Accuracy on test set: 25.00%\n",
      "[[0 0 7 0]\n",
      " [0 1 1 0]\n",
      " [0 0 3 0]\n",
      " [0 3 1 0]]\n"
     ]
    }
   ],
   "source": [
    "run_mlp(X_train, y_train, X_test, y_test, 4, 3, 150) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/150\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.3699 - accuracy: 0.2500\n",
      "Epoch 2/150\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.3695 - accuracy: 0.2917\n",
      "Epoch 3/150\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.3691 - accuracy: 0.2917\n",
      "Epoch 4/150\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.3687 - accuracy: 0.2917\n",
      "Epoch 5/150\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 1.3683 - accuracy: 0.3125\n",
      "Epoch 6/150\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 1.3680 - accuracy: 0.3125\n",
      "Epoch 7/150\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.3676 - accuracy: 0.3125\n",
      "Epoch 8/150\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 1.3672 - accuracy: 0.3125\n",
      "Epoch 9/150\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.3668 - accuracy: 0.3750\n",
      "Epoch 10/150\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.3664 - accuracy: 0.3125\n",
      "Epoch 11/150\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.3660 - accuracy: 0.2917\n",
      "Epoch 12/150\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.3658 - accuracy: 0.2917\n",
      "Epoch 13/150\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.3653 - accuracy: 0.2917\n",
      "Epoch 14/150\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.3651 - accuracy: 0.2917\n",
      "Epoch 15/150\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.3647 - accuracy: 0.2917\n",
      "Epoch 16/150\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.3644 - accuracy: 0.2917\n",
      "Epoch 17/150\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.3641 - accuracy: 0.2917\n",
      "Epoch 18/150\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.3637 - accuracy: 0.2917\n",
      "Epoch 19/150\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.3634 - accuracy: 0.2917\n",
      "Epoch 20/150\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.3631 - accuracy: 0.2917\n",
      "Epoch 21/150\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.3628 - accuracy: 0.2917\n",
      "Epoch 22/150\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.3624 - accuracy: 0.2917\n",
      "Epoch 23/150\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.3621 - accuracy: 0.2917\n",
      "Epoch 24/150\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.3618 - accuracy: 0.2917\n",
      "Epoch 25/150\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.3615 - accuracy: 0.2917\n",
      "Epoch 26/150\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.3612 - accuracy: 0.2917\n",
      "Epoch 27/150\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.3609 - accuracy: 0.2917\n",
      "Epoch 28/150\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.3606 - accuracy: 0.2917\n",
      "Epoch 29/150\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.3602 - accuracy: 0.2917\n",
      "Epoch 30/150\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.3599 - accuracy: 0.2917\n",
      "Epoch 31/150\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 1.3597 - accuracy: 0.2917\n",
      "Epoch 32/150\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.3595 - accuracy: 0.3125\n",
      "Epoch 33/150\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.3591 - accuracy: 0.3125\n",
      "Epoch 34/150\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.3588 - accuracy: 0.3125\n",
      "Epoch 35/150\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.3586 - accuracy: 0.3125\n",
      "Epoch 36/150\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.3583 - accuracy: 0.3125\n",
      "Epoch 37/150\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 1.3580 - accuracy: 0.3125\n",
      "Epoch 38/150\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.3578 - accuracy: 0.3125\n",
      "Epoch 39/150\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.3575 - accuracy: 0.3125\n",
      "Epoch 40/150\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.3572 - accuracy: 0.3125\n",
      "Epoch 41/150\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.3570 - accuracy: 0.3125\n",
      "Epoch 42/150\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.3566 - accuracy: 0.3125\n",
      "Epoch 43/150\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.3565 - accuracy: 0.3125\n",
      "Epoch 44/150\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.3561 - accuracy: 0.3125\n",
      "Epoch 45/150\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.3559 - accuracy: 0.3125\n",
      "Epoch 46/150\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 1.3559 - accuracy: 0.3125\n",
      "Epoch 47/150\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.3551 - accuracy: 0.3125\n",
      "Epoch 48/150\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.3548 - accuracy: 0.3125\n",
      "Epoch 49/150\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.3547 - accuracy: 0.3125\n",
      "Epoch 50/150\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.3543 - accuracy: 0.3125\n",
      "Epoch 51/150\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.3540 - accuracy: 0.3125\n",
      "Epoch 52/150\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.3538 - accuracy: 0.3125\n",
      "Epoch 53/150\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.3536 - accuracy: 0.3125\n",
      "Epoch 54/150\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.3534 - accuracy: 0.3125\n",
      "Epoch 55/150\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 1.3532 - accuracy: 0.3125\n",
      "Epoch 56/150\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.3529 - accuracy: 0.3125\n",
      "Epoch 57/150\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.3527 - accuracy: 0.3125\n",
      "Epoch 58/150\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.3524 - accuracy: 0.3125\n",
      "Epoch 59/150\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.3521 - accuracy: 0.3125\n",
      "Epoch 60/150\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.3520 - accuracy: 0.3125\n",
      "Epoch 61/150\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.3516 - accuracy: 0.3125\n",
      "Epoch 62/150\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.3514 - accuracy: 0.3125\n",
      "Epoch 63/150\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 1.3512 - accuracy: 0.3125\n",
      "Epoch 64/150\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.3509 - accuracy: 0.3125\n",
      "Epoch 65/150\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.3508 - accuracy: 0.3125\n",
      "Epoch 66/150\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.3505 - accuracy: 0.3125\n",
      "Epoch 67/150\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.3503 - accuracy: 0.3125\n",
      "Epoch 68/150\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.3501 - accuracy: 0.3125\n",
      "Epoch 69/150\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.3499 - accuracy: 0.3125\n",
      "Epoch 70/150\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.3497 - accuracy: 0.3125\n",
      "Epoch 71/150\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.3495 - accuracy: 0.3125\n",
      "Epoch 72/150\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.3493 - accuracy: 0.3125\n",
      "Epoch 73/150\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 1.3490 - accuracy: 0.3125\n",
      "Epoch 74/150\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 1.3488 - accuracy: 0.3125\n",
      "Epoch 75/150\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.3487 - accuracy: 0.3125\n",
      "Epoch 76/150\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.3485 - accuracy: 0.3125\n",
      "Epoch 77/150\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 1.3482 - accuracy: 0.3125\n",
      "Epoch 78/150\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.3480 - accuracy: 0.3125\n",
      "Epoch 79/150\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.3477 - accuracy: 0.3125\n",
      "Epoch 80/150\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.3475 - accuracy: 0.3333\n",
      "Epoch 81/150\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 1.3473 - accuracy: 0.3333\n",
      "Epoch 82/150\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.3471 - accuracy: 0.3333\n",
      "Epoch 83/150\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.3469 - accuracy: 0.3542\n",
      "Epoch 84/150\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.3468 - accuracy: 0.3542\n",
      "Epoch 85/150\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 1.3465 - accuracy: 0.3542\n",
      "Epoch 86/150\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.3464 - accuracy: 0.3542\n",
      "Epoch 87/150\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.3462 - accuracy: 0.3542\n",
      "Epoch 88/150\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.3460 - accuracy: 0.3542\n",
      "Epoch 89/150\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 1.3457 - accuracy: 0.3542\n",
      "Epoch 90/150\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 1.3455 - accuracy: 0.3542\n",
      "Epoch 91/150\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.3454 - accuracy: 0.3542\n",
      "Epoch 92/150\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.3451 - accuracy: 0.3542\n",
      "Epoch 93/150\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.3449 - accuracy: 0.3542\n",
      "Epoch 94/150\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.3446 - accuracy: 0.3542\n",
      "Epoch 95/150\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.3445 - accuracy: 0.3542\n",
      "Epoch 96/150\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.3443 - accuracy: 0.3542\n",
      "Epoch 97/150\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.3440 - accuracy: 0.3542\n",
      "Epoch 98/150\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.3438 - accuracy: 0.3542\n",
      "Epoch 99/150\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 1.3439 - accuracy: 0.3542\n",
      "Epoch 100/150\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.3435 - accuracy: 0.3542\n",
      "Epoch 101/150\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.3434 - accuracy: 0.3542\n",
      "Epoch 102/150\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.3432 - accuracy: 0.3542\n",
      "Epoch 103/150\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.3430 - accuracy: 0.3542\n",
      "Epoch 104/150\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.3429 - accuracy: 0.3542\n",
      "Epoch 105/150\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.3427 - accuracy: 0.3542\n",
      "Epoch 106/150\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 1.3424 - accuracy: 0.3333\n",
      "Epoch 107/150\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.3422 - accuracy: 0.3333\n",
      "Epoch 108/150\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.3421 - accuracy: 0.3333\n",
      "Epoch 109/150\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.3419 - accuracy: 0.3333\n",
      "Epoch 110/150\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.3417 - accuracy: 0.3333\n",
      "Epoch 111/150\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.3415 - accuracy: 0.3333\n",
      "Epoch 112/150\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.3414 - accuracy: 0.3333\n",
      "Epoch 113/150\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.3412 - accuracy: 0.3333\n",
      "Epoch 114/150\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.3409 - accuracy: 0.3333\n",
      "Epoch 115/150\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 1.3408 - accuracy: 0.3333\n",
      "Epoch 116/150\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.3405 - accuracy: 0.3333\n",
      "Epoch 117/150\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 1.3403 - accuracy: 0.3333\n",
      "Epoch 118/150\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 1.3404 - accuracy: 0.3333\n",
      "Epoch 119/150\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.3400 - accuracy: 0.3333\n",
      "Epoch 120/150\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.3398 - accuracy: 0.3333\n",
      "Epoch 121/150\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 1.3396 - accuracy: 0.3333\n",
      "Epoch 122/150\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 1.3393 - accuracy: 0.3333\n",
      "Epoch 123/150\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.3392 - accuracy: 0.3333\n",
      "Epoch 124/150\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.3391 - accuracy: 0.3333\n",
      "Epoch 125/150\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 1.3389 - accuracy: 0.3333\n",
      "Epoch 126/150\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 1.3387 - accuracy: 0.3333\n",
      "Epoch 127/150\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.3386 - accuracy: 0.3333\n",
      "Epoch 128/150\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.3383 - accuracy: 0.3333\n",
      "Epoch 129/150\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.3382 - accuracy: 0.3333\n",
      "Epoch 130/150\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.3382 - accuracy: 0.3333\n",
      "Epoch 131/150\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.3378 - accuracy: 0.3333\n",
      "Epoch 132/150\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.3376 - accuracy: 0.3333\n",
      "Epoch 133/150\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 1.3374 - accuracy: 0.3333\n",
      "Epoch 134/150\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.3372 - accuracy: 0.3333\n",
      "Epoch 135/150\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.3370 - accuracy: 0.3333\n",
      "Epoch 136/150\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.3368 - accuracy: 0.3333\n",
      "Epoch 137/150\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.3367 - accuracy: 0.3333\n",
      "Epoch 138/150\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.3365 - accuracy: 0.3333\n",
      "Epoch 139/150\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 1.3363 - accuracy: 0.3333\n",
      "Epoch 140/150\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.3361 - accuracy: 0.3333\n",
      "Epoch 141/150\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.3359 - accuracy: 0.3333\n",
      "Epoch 142/150\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 1.3358 - accuracy: 0.3333\n",
      "Epoch 143/150\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 1.3356 - accuracy: 0.3333\n",
      "Epoch 144/150\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.3354 - accuracy: 0.3333\n",
      "Epoch 145/150\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.3352 - accuracy: 0.3333\n",
      "Epoch 146/150\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.3350 - accuracy: 0.3333\n",
      "Epoch 147/150\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 1.3347 - accuracy: 0.3333\n",
      "Epoch 148/150\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 1.3346 - accuracy: 0.3333\n",
      "Epoch 149/150\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 1.3344 - accuracy: 0.3333\n",
      "Epoch 150/150\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 1.3342 - accuracy: 0.3333\n",
      "Model: \"sequential_9\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " flatten_9 (Flatten)         (None, 2)                 0         \n",
      "                                                                 \n",
      " dense_18 (Dense)            (None, 1)                 3         \n",
      "                                                                 \n",
      " dense_19 (Dense)            (None, 4)                 8         \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 11\n",
      "Trainable params: 11\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "2/2 [==============================] - 0s 1ms/step\n",
      "Accuracy on train set: 33.33%\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "Accuracy on test set: 31.25%\n",
      "[[3 4 0 0]\n",
      " [0 2 0 0]\n",
      " [1 1 0 1]\n",
      " [0 4 0 0]]\n"
     ]
    }
   ],
   "source": [
    "run_mlp(X_train, y_train, X_test, y_test, 4, 1, 150) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/150\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 1.5323 - accuracy: 0.2292\n",
      "Epoch 2/150\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 1.5279 - accuracy: 0.2292\n",
      "Epoch 3/150\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.5236 - accuracy: 0.2292\n",
      "Epoch 4/150\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.5190 - accuracy: 0.2292\n",
      "Epoch 5/150\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.5149 - accuracy: 0.2292\n",
      "Epoch 6/150\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.5103 - accuracy: 0.2292\n",
      "Epoch 7/150\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.5057 - accuracy: 0.2292\n",
      "Epoch 8/150\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.5016 - accuracy: 0.2292\n",
      "Epoch 9/150\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.4981 - accuracy: 0.2292\n",
      "Epoch 10/150\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.4940 - accuracy: 0.2292\n",
      "Epoch 11/150\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 1.4901 - accuracy: 0.2500\n",
      "Epoch 12/150\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.4858 - accuracy: 0.2083\n",
      "Epoch 13/150\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 1.4816 - accuracy: 0.2292\n",
      "Epoch 14/150\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 1.4780 - accuracy: 0.2292\n",
      "Epoch 15/150\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 1.4744 - accuracy: 0.2292\n",
      "Epoch 16/150\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.4710 - accuracy: 0.2292\n",
      "Epoch 17/150\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.4675 - accuracy: 0.2292\n",
      "Epoch 18/150\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.4638 - accuracy: 0.2083\n",
      "Epoch 19/150\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.4608 - accuracy: 0.2292\n",
      "Epoch 20/150\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.4568 - accuracy: 0.2292\n",
      "Epoch 21/150\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 1.4532 - accuracy: 0.2083\n",
      "Epoch 22/150\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.4498 - accuracy: 0.2083\n",
      "Epoch 23/150\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.4467 - accuracy: 0.2083\n",
      "Epoch 24/150\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.4434 - accuracy: 0.2292\n",
      "Epoch 25/150\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.4402 - accuracy: 0.2292\n",
      "Epoch 26/150\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.4372 - accuracy: 0.2292\n",
      "Epoch 27/150\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 1.4345 - accuracy: 0.2292\n",
      "Epoch 28/150\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 1.4314 - accuracy: 0.2292\n",
      "Epoch 29/150\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.4285 - accuracy: 0.2292\n",
      "Epoch 30/150\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.4252 - accuracy: 0.2292\n",
      "Epoch 31/150\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.4223 - accuracy: 0.2292\n",
      "Epoch 32/150\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.4199 - accuracy: 0.2292\n",
      "Epoch 33/150\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.4169 - accuracy: 0.2292\n",
      "Epoch 34/150\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.4141 - accuracy: 0.2292\n",
      "Epoch 35/150\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.4115 - accuracy: 0.2500\n",
      "Epoch 36/150\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.4085 - accuracy: 0.2500\n",
      "Epoch 37/150\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 1.4060 - accuracy: 0.2500\n",
      "Epoch 38/150\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.4039 - accuracy: 0.2708\n",
      "Epoch 39/150\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 1.4008 - accuracy: 0.2917\n",
      "Epoch 40/150\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 1.3980 - accuracy: 0.2917\n",
      "Epoch 41/150\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 1.3952 - accuracy: 0.2917\n",
      "Epoch 42/150\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.3928 - accuracy: 0.2917\n",
      "Epoch 43/150\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.3902 - accuracy: 0.2917\n",
      "Epoch 44/150\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.3875 - accuracy: 0.2708\n",
      "Epoch 45/150\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.3850 - accuracy: 0.2708\n",
      "Epoch 46/150\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.3826 - accuracy: 0.2708\n",
      "Epoch 47/150\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.3802 - accuracy: 0.2917\n",
      "Epoch 48/150\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.3775 - accuracy: 0.2708\n",
      "Epoch 49/150\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.3751 - accuracy: 0.2708\n",
      "Epoch 50/150\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.3726 - accuracy: 0.2708\n",
      "Epoch 51/150\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.3708 - accuracy: 0.2708\n",
      "Epoch 52/150\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.3678 - accuracy: 0.2708\n",
      "Epoch 53/150\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.3655 - accuracy: 0.2708\n",
      "Epoch 54/150\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.3633 - accuracy: 0.2708\n",
      "Epoch 55/150\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.3614 - accuracy: 0.2708\n",
      "Epoch 56/150\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.3588 - accuracy: 0.2708\n",
      "Epoch 57/150\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.3564 - accuracy: 0.2917\n",
      "Epoch 58/150\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.3541 - accuracy: 0.2917\n",
      "Epoch 59/150\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.3518 - accuracy: 0.2917\n",
      "Epoch 60/150\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.3494 - accuracy: 0.2917\n",
      "Epoch 61/150\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.3474 - accuracy: 0.2917\n",
      "Epoch 62/150\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.3451 - accuracy: 0.2917\n",
      "Epoch 63/150\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 1.3430 - accuracy: 0.2917\n",
      "Epoch 64/150\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.3409 - accuracy: 0.2917\n",
      "Epoch 65/150\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 1.3389 - accuracy: 0.2917\n",
      "Epoch 66/150\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.3366 - accuracy: 0.2917\n",
      "Epoch 67/150\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.3346 - accuracy: 0.2917\n",
      "Epoch 68/150\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.3328 - accuracy: 0.2917\n",
      "Epoch 69/150\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 1.3307 - accuracy: 0.2917\n",
      "Epoch 70/150\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.3289 - accuracy: 0.2917\n",
      "Epoch 71/150\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.3270 - accuracy: 0.3333\n",
      "Epoch 72/150\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.3251 - accuracy: 0.3333\n",
      "Epoch 73/150\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.3230 - accuracy: 0.3333\n",
      "Epoch 74/150\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.3210 - accuracy: 0.3542\n",
      "Epoch 75/150\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.3191 - accuracy: 0.3542\n",
      "Epoch 76/150\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.3175 - accuracy: 0.3542\n",
      "Epoch 77/150\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.3161 - accuracy: 0.3542\n",
      "Epoch 78/150\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.3139 - accuracy: 0.3542\n",
      "Epoch 79/150\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.3122 - accuracy: 0.3542\n",
      "Epoch 80/150\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.3106 - accuracy: 0.3542\n",
      "Epoch 81/150\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.3089 - accuracy: 0.3542\n",
      "Epoch 82/150\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.3072 - accuracy: 0.3542\n",
      "Epoch 83/150\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.3052 - accuracy: 0.3750\n",
      "Epoch 84/150\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 1.3034 - accuracy: 0.3750\n",
      "Epoch 85/150\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.3018 - accuracy: 0.3750\n",
      "Epoch 86/150\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 1.3001 - accuracy: 0.3750\n",
      "Epoch 87/150\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.2987 - accuracy: 0.3750\n",
      "Epoch 88/150\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.2972 - accuracy: 0.3750\n",
      "Epoch 89/150\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.2959 - accuracy: 0.3750\n",
      "Epoch 90/150\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.2946 - accuracy: 0.3958\n",
      "Epoch 91/150\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.2928 - accuracy: 0.3958\n",
      "Epoch 92/150\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.2913 - accuracy: 0.4167\n",
      "Epoch 93/150\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.2898 - accuracy: 0.4167\n",
      "Epoch 94/150\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.2883 - accuracy: 0.4167\n",
      "Epoch 95/150\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.2869 - accuracy: 0.4167\n",
      "Epoch 96/150\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.2854 - accuracy: 0.4167\n",
      "Epoch 97/150\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.2838 - accuracy: 0.4583\n",
      "Epoch 98/150\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.2823 - accuracy: 0.4583\n",
      "Epoch 99/150\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.2809 - accuracy: 0.4583\n",
      "Epoch 100/150\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.2794 - accuracy: 0.4583\n",
      "Epoch 101/150\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.2779 - accuracy: 0.4792\n",
      "Epoch 102/150\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.2758 - accuracy: 0.4792\n",
      "Epoch 103/150\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.2745 - accuracy: 0.4792\n",
      "Epoch 104/150\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 1.2728 - accuracy: 0.4792\n",
      "Epoch 105/150\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.2712 - accuracy: 0.4792\n",
      "Epoch 106/150\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.2698 - accuracy: 0.4792\n",
      "Epoch 107/150\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.2683 - accuracy: 0.4792\n",
      "Epoch 108/150\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.2668 - accuracy: 0.4792\n",
      "Epoch 109/150\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.2654 - accuracy: 0.4792\n",
      "Epoch 110/150\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 1.2643 - accuracy: 0.4583\n",
      "Epoch 111/150\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 1.2625 - accuracy: 0.4792\n",
      "Epoch 112/150\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.2610 - accuracy: 0.5000\n",
      "Epoch 113/150\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.2595 - accuracy: 0.5000\n",
      "Epoch 114/150\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.2582 - accuracy: 0.5000\n",
      "Epoch 115/150\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.2565 - accuracy: 0.5000\n",
      "Epoch 116/150\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.2551 - accuracy: 0.5000\n",
      "Epoch 117/150\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.2540 - accuracy: 0.5000\n",
      "Epoch 118/150\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 1.2526 - accuracy: 0.5208\n",
      "Epoch 119/150\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.2512 - accuracy: 0.5208\n",
      "Epoch 120/150\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 1.2500 - accuracy: 0.5208\n",
      "Epoch 121/150\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.2486 - accuracy: 0.5208\n",
      "Epoch 122/150\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.2471 - accuracy: 0.5208\n",
      "Epoch 123/150\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.2457 - accuracy: 0.5208\n",
      "Epoch 124/150\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.2444 - accuracy: 0.5417\n",
      "Epoch 125/150\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.2431 - accuracy: 0.5417\n",
      "Epoch 126/150\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.2415 - accuracy: 0.5417\n",
      "Epoch 127/150\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.2401 - accuracy: 0.5417\n",
      "Epoch 128/150\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 1.2389 - accuracy: 0.5417\n",
      "Epoch 129/150\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.2373 - accuracy: 0.5417\n",
      "Epoch 130/150\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.2359 - accuracy: 0.5417\n",
      "Epoch 131/150\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.2345 - accuracy: 0.5417\n",
      "Epoch 132/150\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.2329 - accuracy: 0.5208\n",
      "Epoch 133/150\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.2320 - accuracy: 0.5208\n",
      "Epoch 134/150\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 1.2300 - accuracy: 0.5208\n",
      "Epoch 135/150\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.2286 - accuracy: 0.5208\n",
      "Epoch 136/150\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.2271 - accuracy: 0.5208\n",
      "Epoch 137/150\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.2259 - accuracy: 0.5208\n",
      "Epoch 138/150\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.2240 - accuracy: 0.5208\n",
      "Epoch 139/150\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.2226 - accuracy: 0.5208\n",
      "Epoch 140/150\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.2212 - accuracy: 0.5208\n",
      "Epoch 141/150\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 1.2198 - accuracy: 0.5208\n",
      "Epoch 142/150\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.2185 - accuracy: 0.5208\n",
      "Epoch 143/150\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.2168 - accuracy: 0.5208\n",
      "Epoch 144/150\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.2154 - accuracy: 0.5208\n",
      "Epoch 145/150\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.2140 - accuracy: 0.5208\n",
      "Epoch 146/150\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 1.2125 - accuracy: 0.5208\n",
      "Epoch 147/150\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 1.2110 - accuracy: 0.5208\n",
      "Epoch 148/150\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.2095 - accuracy: 0.5208\n",
      "Epoch 149/150\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.2083 - accuracy: 0.5417\n",
      "Epoch 150/150\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 1.2067 - accuracy: 0.5417\n",
      "Model: \"sequential_10\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " flatten_10 (Flatten)        (None, 2)                 0         \n",
      "                                                                 \n",
      " dense_20 (Dense)            (None, 15)                45        \n",
      "                                                                 \n",
      " dense_21 (Dense)            (None, 4)                 64        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 109\n",
      "Trainable params: 109\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Accuracy on train set: 54.17%\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "Accuracy on test set: 25.00%\n",
      "[[1 4 2 0]\n",
      " [0 1 1 0]\n",
      " [2 1 0 0]\n",
      " [0 0 2 2]]\n"
     ]
    }
   ],
   "source": [
    "run_mlp(X_train, y_train, X_test, y_test, 4, 15, 150) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reflection\n",
    "Below is a table comparing number of hidden laters to model accuracy. All other indipendent variables are constant and each model was tested with 150 epochs.\n",
    "\n",
    "Number of hidden layers | Accuracy\n",
    "\n",
    "1                       | 18.75\n",
    "\n",
    "3                       | 62.5\n",
    "\n",
    "5                       | 43.75\n",
    "\n",
    "10                      | 75.00\n",
    "\n",
    "15                      | 100.00\n",
    "\n",
    "\n",
    "Looking at the table above, 15 hidden layers yeilded the best accuracy. As the number of hidden layers is decreased, accuracy generally decreases. Thus, a model with 15 hidden layers can still achive a high accuracy.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 4:\n",
    "The low-variance blob data was easy to separate with simple classification rules.  Run a few experiments with the higher variance dataset you created and determine if the MLP or your deterministic solution could achieve better accuracy.  Describe your experiments in a table in the reflection section and write a few statements about your observations.  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(HVX, HVy, random_state=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Epoch tests (15 hidden layers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/15\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 1.4355 - accuracy: 0.2292\n",
      "Epoch 2/15\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 1.4299 - accuracy: 0.2500\n",
      "Epoch 3/15\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.4243 - accuracy: 0.1875\n",
      "Epoch 4/15\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 1.4192 - accuracy: 0.2083\n",
      "Epoch 5/15\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.4140 - accuracy: 0.2292\n",
      "Epoch 6/15\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.4091 - accuracy: 0.2292\n",
      "Epoch 7/15\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 1.4043 - accuracy: 0.2292\n",
      "Epoch 8/15\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.3991 - accuracy: 0.2500\n",
      "Epoch 9/15\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.3947 - accuracy: 0.2500\n",
      "Epoch 10/15\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.3907 - accuracy: 0.2292\n",
      "Epoch 11/15\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.3864 - accuracy: 0.2500\n",
      "Epoch 12/15\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.3813 - accuracy: 0.2292\n",
      "Epoch 13/15\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.3774 - accuracy: 0.2083\n",
      "Epoch 14/15\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.3722 - accuracy: 0.2083\n",
      "Epoch 15/15\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.3684 - accuracy: 0.2083\n",
      "Model: \"sequential_11\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " flatten_11 (Flatten)        (None, 2)                 0         \n",
      "                                                                 \n",
      " dense_22 (Dense)            (None, 15)                45        \n",
      "                                                                 \n",
      " dense_23 (Dense)            (None, 4)                 64        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 109\n",
      "Trainable params: 109\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Accuracy on train set: 22.92%\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "Accuracy on test set: 12.50%\n",
      "[[0 2 2 0]\n",
      " [2 2 0 0]\n",
      " [0 3 0 2]\n",
      " [0 3 0 0]]\n"
     ]
    }
   ],
   "source": [
    "run_mlp(X_train, y_train, X_test, y_test, 4, 15, 15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 1.5023 - accuracy: 0.1875\n",
      "Epoch 2/50\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.4954 - accuracy: 0.1875\n",
      "Epoch 3/50\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 1.4904 - accuracy: 0.1875\n",
      "Epoch 4/50\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.4853 - accuracy: 0.2083\n",
      "Epoch 5/50\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 1.4803 - accuracy: 0.2083\n",
      "Epoch 6/50\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.4743 - accuracy: 0.2292\n",
      "Epoch 7/50\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.4692 - accuracy: 0.2500\n",
      "Epoch 8/50\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.4639 - accuracy: 0.2500\n",
      "Epoch 9/50\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.4593 - accuracy: 0.2708\n",
      "Epoch 10/50\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 1.4542 - accuracy: 0.2708\n",
      "Epoch 11/50\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.4494 - accuracy: 0.2708\n",
      "Epoch 12/50\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.4443 - accuracy: 0.2708\n",
      "Epoch 13/50\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 1.4396 - accuracy: 0.2708\n",
      "Epoch 14/50\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.4344 - accuracy: 0.2708\n",
      "Epoch 15/50\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.4295 - accuracy: 0.2708\n",
      "Epoch 16/50\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.4249 - accuracy: 0.2708\n",
      "Epoch 17/50\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.4205 - accuracy: 0.2708\n",
      "Epoch 18/50\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 1.4158 - accuracy: 0.2708\n",
      "Epoch 19/50\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 1.4112 - accuracy: 0.2708\n",
      "Epoch 20/50\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.4060 - accuracy: 0.2917\n",
      "Epoch 21/50\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 1.4018 - accuracy: 0.3125\n",
      "Epoch 22/50\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.3975 - accuracy: 0.3125\n",
      "Epoch 23/50\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.3933 - accuracy: 0.3125\n",
      "Epoch 24/50\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.3891 - accuracy: 0.3125\n",
      "Epoch 25/50\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.3845 - accuracy: 0.3125\n",
      "Epoch 26/50\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.3807 - accuracy: 0.3125\n",
      "Epoch 27/50\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.3776 - accuracy: 0.3333\n",
      "Epoch 28/50\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.3735 - accuracy: 0.3333\n",
      "Epoch 29/50\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.3693 - accuracy: 0.3333\n",
      "Epoch 30/50\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.3658 - accuracy: 0.3125\n",
      "Epoch 31/50\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 1.3621 - accuracy: 0.3125\n",
      "Epoch 32/50\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.3581 - accuracy: 0.3333\n",
      "Epoch 33/50\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 1.3546 - accuracy: 0.3333\n",
      "Epoch 34/50\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.3513 - accuracy: 0.3542\n",
      "Epoch 35/50\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 1.3482 - accuracy: 0.3958\n",
      "Epoch 36/50\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.3456 - accuracy: 0.3958\n",
      "Epoch 37/50\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.3422 - accuracy: 0.3958\n",
      "Epoch 38/50\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.3387 - accuracy: 0.4375\n",
      "Epoch 39/50\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.3355 - accuracy: 0.4375\n",
      "Epoch 40/50\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 1.3326 - accuracy: 0.4167\n",
      "Epoch 41/50\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.3300 - accuracy: 0.4167\n",
      "Epoch 42/50\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.3278 - accuracy: 0.3958\n",
      "Epoch 43/50\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.3246 - accuracy: 0.4167\n",
      "Epoch 44/50\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.3217 - accuracy: 0.4167\n",
      "Epoch 45/50\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.3188 - accuracy: 0.4167\n",
      "Epoch 46/50\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.3156 - accuracy: 0.4167\n",
      "Epoch 47/50\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 1.3126 - accuracy: 0.4167\n",
      "Epoch 48/50\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.3102 - accuracy: 0.3958\n",
      "Epoch 49/50\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.3075 - accuracy: 0.3958\n",
      "Epoch 50/50\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.3047 - accuracy: 0.3958\n",
      "Model: \"sequential_12\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " flatten_12 (Flatten)        (None, 2)                 0         \n",
      "                                                                 \n",
      " dense_24 (Dense)            (None, 15)                45        \n",
      "                                                                 \n",
      " dense_25 (Dense)            (None, 4)                 64        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 109\n",
      "Trainable params: 109\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "2/2 [==============================] - 0s 2ms/step\n",
      "Accuracy on train set: 39.58%\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "Accuracy on test set: 31.25%\n",
      "[[2 1 0 1]\n",
      " [0 0 0 4]\n",
      " [1 1 0 3]\n",
      " [0 0 0 3]]\n"
     ]
    }
   ],
   "source": [
    "run_mlp(X_train, y_train, X_test, y_test, 4, 15, 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/150\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 1.4968 - accuracy: 0.2292\n",
      "Epoch 2/150\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 1.4856 - accuracy: 0.2292\n",
      "Epoch 3/150\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 1.4740 - accuracy: 0.2083\n",
      "Epoch 4/150\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.4631 - accuracy: 0.2083\n",
      "Epoch 5/150\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.4538 - accuracy: 0.2292\n",
      "Epoch 6/150\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.4436 - accuracy: 0.2292\n",
      "Epoch 7/150\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.4360 - accuracy: 0.2292\n",
      "Epoch 8/150\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.4282 - accuracy: 0.2292\n",
      "Epoch 9/150\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.4199 - accuracy: 0.2292\n",
      "Epoch 10/150\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.4118 - accuracy: 0.2292\n",
      "Epoch 11/150\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 1.4038 - accuracy: 0.2083\n",
      "Epoch 12/150\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 1.3967 - accuracy: 0.2292\n",
      "Epoch 13/150\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.3900 - accuracy: 0.2292\n",
      "Epoch 14/150\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.3841 - accuracy: 0.2292\n",
      "Epoch 15/150\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.3779 - accuracy: 0.2292\n",
      "Epoch 16/150\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.3723 - accuracy: 0.2292\n",
      "Epoch 17/150\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 1.3651 - accuracy: 0.2292\n",
      "Epoch 18/150\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 1.3596 - accuracy: 0.2292\n",
      "Epoch 19/150\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.3545 - accuracy: 0.2500\n",
      "Epoch 20/150\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.3494 - accuracy: 0.2708\n",
      "Epoch 21/150\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 1.3438 - accuracy: 0.2708\n",
      "Epoch 22/150\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.3386 - accuracy: 0.2708\n",
      "Epoch 23/150\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.3341 - accuracy: 0.2708\n",
      "Epoch 24/150\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 1.3303 - accuracy: 0.2708\n",
      "Epoch 25/150\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.3262 - accuracy: 0.2917\n",
      "Epoch 26/150\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.3220 - accuracy: 0.2917\n",
      "Epoch 27/150\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.3180 - accuracy: 0.2917\n",
      "Epoch 28/150\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.3143 - accuracy: 0.2917\n",
      "Epoch 29/150\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.3108 - accuracy: 0.2917\n",
      "Epoch 30/150\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.3069 - accuracy: 0.2708\n",
      "Epoch 31/150\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.3035 - accuracy: 0.2708\n",
      "Epoch 32/150\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.3003 - accuracy: 0.2708\n",
      "Epoch 33/150\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.2978 - accuracy: 0.2708\n",
      "Epoch 34/150\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.2939 - accuracy: 0.2708\n",
      "Epoch 35/150\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 1.2904 - accuracy: 0.2708\n",
      "Epoch 36/150\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.2876 - accuracy: 0.2708\n",
      "Epoch 37/150\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.2851 - accuracy: 0.2708\n",
      "Epoch 38/150\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.2822 - accuracy: 0.2708\n",
      "Epoch 39/150\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.2785 - accuracy: 0.2708\n",
      "Epoch 40/150\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.2757 - accuracy: 0.2708\n",
      "Epoch 41/150\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.2730 - accuracy: 0.2917\n",
      "Epoch 42/150\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 1.2705 - accuracy: 0.2917\n",
      "Epoch 43/150\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.2677 - accuracy: 0.3125\n",
      "Epoch 44/150\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.2647 - accuracy: 0.3333\n",
      "Epoch 45/150\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.2620 - accuracy: 0.3333\n",
      "Epoch 46/150\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.2596 - accuracy: 0.3333\n",
      "Epoch 47/150\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.2572 - accuracy: 0.3333\n",
      "Epoch 48/150\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.2548 - accuracy: 0.3750\n",
      "Epoch 49/150\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.2527 - accuracy: 0.3750\n",
      "Epoch 50/150\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.2506 - accuracy: 0.3750\n",
      "Epoch 51/150\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.2484 - accuracy: 0.3542\n",
      "Epoch 52/150\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.2461 - accuracy: 0.3750\n",
      "Epoch 53/150\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 1.2439 - accuracy: 0.3750\n",
      "Epoch 54/150\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.2420 - accuracy: 0.3750\n",
      "Epoch 55/150\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.2398 - accuracy: 0.3542\n",
      "Epoch 56/150\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.2381 - accuracy: 0.3542\n",
      "Epoch 57/150\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.2350 - accuracy: 0.3542\n",
      "Epoch 58/150\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 1.2329 - accuracy: 0.3542\n",
      "Epoch 59/150\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.2311 - accuracy: 0.3333\n",
      "Epoch 60/150\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.2293 - accuracy: 0.3333\n",
      "Epoch 61/150\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.2275 - accuracy: 0.3542\n",
      "Epoch 62/150\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 1.2253 - accuracy: 0.3542\n",
      "Epoch 63/150\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.2235 - accuracy: 0.3542\n",
      "Epoch 64/150\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.2217 - accuracy: 0.3542\n",
      "Epoch 65/150\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.2201 - accuracy: 0.3542\n",
      "Epoch 66/150\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.2182 - accuracy: 0.3542\n",
      "Epoch 67/150\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.2164 - accuracy: 0.3542\n",
      "Epoch 68/150\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.2148 - accuracy: 0.3750\n",
      "Epoch 69/150\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.2128 - accuracy: 0.3750\n",
      "Epoch 70/150\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.2112 - accuracy: 0.3958\n",
      "Epoch 71/150\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.2097 - accuracy: 0.3958\n",
      "Epoch 72/150\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 1.2083 - accuracy: 0.4375\n",
      "Epoch 73/150\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.2066 - accuracy: 0.4375\n",
      "Epoch 74/150\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.2048 - accuracy: 0.4583\n",
      "Epoch 75/150\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 1.2032 - accuracy: 0.4375\n",
      "Epoch 76/150\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.2013 - accuracy: 0.4375\n",
      "Epoch 77/150\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.1997 - accuracy: 0.4375\n",
      "Epoch 78/150\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.1981 - accuracy: 0.4375\n",
      "Epoch 79/150\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.1972 - accuracy: 0.4583\n",
      "Epoch 80/150\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.1954 - accuracy: 0.4375\n",
      "Epoch 81/150\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.1942 - accuracy: 0.4583\n",
      "Epoch 82/150\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.1929 - accuracy: 0.4792\n",
      "Epoch 83/150\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.1912 - accuracy: 0.4792\n",
      "Epoch 84/150\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.1898 - accuracy: 0.4792\n",
      "Epoch 85/150\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.1882 - accuracy: 0.4792\n",
      "Epoch 86/150\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.1867 - accuracy: 0.4792\n",
      "Epoch 87/150\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 1.1852 - accuracy: 0.4792\n",
      "Epoch 88/150\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.1844 - accuracy: 0.4792\n",
      "Epoch 89/150\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 1.1824 - accuracy: 0.4792\n",
      "Epoch 90/150\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.1809 - accuracy: 0.5000\n",
      "Epoch 91/150\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.1795 - accuracy: 0.5000\n",
      "Epoch 92/150\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.1781 - accuracy: 0.5000\n",
      "Epoch 93/150\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 1.1768 - accuracy: 0.5000\n",
      "Epoch 94/150\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.1762 - accuracy: 0.5000\n",
      "Epoch 95/150\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.1741 - accuracy: 0.5000\n",
      "Epoch 96/150\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 1.1728 - accuracy: 0.5208\n",
      "Epoch 97/150\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.1714 - accuracy: 0.5208\n",
      "Epoch 98/150\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.1700 - accuracy: 0.5417\n",
      "Epoch 99/150\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.1688 - accuracy: 0.5417\n",
      "Epoch 100/150\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.1677 - accuracy: 0.5417\n",
      "Epoch 101/150\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.1662 - accuracy: 0.5417\n",
      "Epoch 102/150\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 1.1651 - accuracy: 0.5417\n",
      "Epoch 103/150\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.1642 - accuracy: 0.5417\n",
      "Epoch 104/150\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.1628 - accuracy: 0.5417\n",
      "Epoch 105/150\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 1.1614 - accuracy: 0.5417\n",
      "Epoch 106/150\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.1603 - accuracy: 0.5417\n",
      "Epoch 107/150\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.1592 - accuracy: 0.5417\n",
      "Epoch 108/150\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 1.1580 - accuracy: 0.5417\n",
      "Epoch 109/150\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.1571 - accuracy: 0.5417\n",
      "Epoch 110/150\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 1.1558 - accuracy: 0.5417\n",
      "Epoch 111/150\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.1548 - accuracy: 0.5417\n",
      "Epoch 112/150\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.1535 - accuracy: 0.5417\n",
      "Epoch 113/150\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 1.1525 - accuracy: 0.5417\n",
      "Epoch 114/150\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.1512 - accuracy: 0.5417\n",
      "Epoch 115/150\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 1.1502 - accuracy: 0.5417\n",
      "Epoch 116/150\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.1494 - accuracy: 0.5417\n",
      "Epoch 117/150\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.1479 - accuracy: 0.5417\n",
      "Epoch 118/150\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.1469 - accuracy: 0.5417\n",
      "Epoch 119/150\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.1458 - accuracy: 0.5417\n",
      "Epoch 120/150\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.1448 - accuracy: 0.5417\n",
      "Epoch 121/150\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.1435 - accuracy: 0.5417\n",
      "Epoch 122/150\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.1425 - accuracy: 0.5417\n",
      "Epoch 123/150\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 1.1421 - accuracy: 0.5417\n",
      "Epoch 124/150\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.1407 - accuracy: 0.5417\n",
      "Epoch 125/150\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.1394 - accuracy: 0.5417\n",
      "Epoch 126/150\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.1384 - accuracy: 0.5417\n",
      "Epoch 127/150\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.1372 - accuracy: 0.5417\n",
      "Epoch 128/150\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.1371 - accuracy: 0.5417\n",
      "Epoch 129/150\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 1.1353 - accuracy: 0.5417\n",
      "Epoch 130/150\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.1343 - accuracy: 0.5625\n",
      "Epoch 131/150\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.1335 - accuracy: 0.5625\n",
      "Epoch 132/150\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.1323 - accuracy: 0.5625\n",
      "Epoch 133/150\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.1311 - accuracy: 0.5625\n",
      "Epoch 134/150\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.1299 - accuracy: 0.5625\n",
      "Epoch 135/150\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.1297 - accuracy: 0.5625\n",
      "Epoch 136/150\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.1286 - accuracy: 0.5625\n",
      "Epoch 137/150\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.1272 - accuracy: 0.5625\n",
      "Epoch 138/150\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.1264 - accuracy: 0.5625\n",
      "Epoch 139/150\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.1252 - accuracy: 0.5625\n",
      "Epoch 140/150\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.1243 - accuracy: 0.5625\n",
      "Epoch 141/150\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.1236 - accuracy: 0.5625\n",
      "Epoch 142/150\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.1226 - accuracy: 0.5625\n",
      "Epoch 143/150\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 1.1216 - accuracy: 0.5625\n",
      "Epoch 144/150\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.1208 - accuracy: 0.5625\n",
      "Epoch 145/150\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.1197 - accuracy: 0.5625\n",
      "Epoch 146/150\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.1188 - accuracy: 0.5625\n",
      "Epoch 147/150\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.1178 - accuracy: 0.5625\n",
      "Epoch 148/150\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.1172 - accuracy: 0.5625\n",
      "Epoch 149/150\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 1.1162 - accuracy: 0.5625\n",
      "Epoch 150/150\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.1150 - accuracy: 0.5625\n",
      "Model: \"sequential_13\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " flatten_13 (Flatten)        (None, 2)                 0         \n",
      "                                                                 \n",
      " dense_26 (Dense)            (None, 15)                45        \n",
      "                                                                 \n",
      " dense_27 (Dense)            (None, 4)                 64        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 109\n",
      "Trainable params: 109\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "2/2 [==============================] - 0s 2ms/step\n",
      "Accuracy on train set: 58.33%\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "Accuracy on test set: 56.25%\n",
      "[[2 0 1 1]\n",
      " [0 1 0 3]\n",
      " [0 0 3 2]\n",
      " [0 0 0 3]]\n"
     ]
    }
   ],
   "source": [
    "run_mlp(X_train, y_train, X_test, y_test, 4, 15, 150)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/300\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 1.5845 - accuracy: 0.0833\n",
      "Epoch 2/300\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.5759 - accuracy: 0.0833\n",
      "Epoch 3/300\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.5674 - accuracy: 0.0833\n",
      "Epoch 4/300\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 1.5586 - accuracy: 0.0833\n",
      "Epoch 5/300\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 1.5509 - accuracy: 0.0833\n",
      "Epoch 6/300\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.5432 - accuracy: 0.1042\n",
      "Epoch 7/300\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.5358 - accuracy: 0.1250\n",
      "Epoch 8/300\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.5285 - accuracy: 0.1250\n",
      "Epoch 9/300\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.5214 - accuracy: 0.1250\n",
      "Epoch 10/300\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.5146 - accuracy: 0.1042\n",
      "Epoch 11/300\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 1.5070 - accuracy: 0.1042\n",
      "Epoch 12/300\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 1.5009 - accuracy: 0.1042\n",
      "Epoch 13/300\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.4948 - accuracy: 0.1042\n",
      "Epoch 14/300\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.4891 - accuracy: 0.1250\n",
      "Epoch 15/300\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.4834 - accuracy: 0.1042\n",
      "Epoch 16/300\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.4788 - accuracy: 0.1042\n",
      "Epoch 17/300\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.4733 - accuracy: 0.1458\n",
      "Epoch 18/300\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.4676 - accuracy: 0.1875\n",
      "Epoch 19/300\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.4624 - accuracy: 0.1875\n",
      "Epoch 20/300\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.4574 - accuracy: 0.2083\n",
      "Epoch 21/300\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 1.4517 - accuracy: 0.2083\n",
      "Epoch 22/300\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.4466 - accuracy: 0.2083\n",
      "Epoch 23/300\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.4415 - accuracy: 0.2083\n",
      "Epoch 24/300\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.4367 - accuracy: 0.2083\n",
      "Epoch 25/300\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.4325 - accuracy: 0.2083\n",
      "Epoch 26/300\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.4278 - accuracy: 0.2083\n",
      "Epoch 27/300\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.4235 - accuracy: 0.1875\n",
      "Epoch 28/300\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.4191 - accuracy: 0.1875\n",
      "Epoch 29/300\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.4141 - accuracy: 0.1875\n",
      "Epoch 30/300\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.4102 - accuracy: 0.1875\n",
      "Epoch 31/300\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.4064 - accuracy: 0.2083\n",
      "Epoch 32/300\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.4026 - accuracy: 0.2083\n",
      "Epoch 33/300\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.3990 - accuracy: 0.2083\n",
      "Epoch 34/300\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.3960 - accuracy: 0.2292\n",
      "Epoch 35/300\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 1.3925 - accuracy: 0.2292\n",
      "Epoch 36/300\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.3895 - accuracy: 0.2500\n",
      "Epoch 37/300\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.3855 - accuracy: 0.2708\n",
      "Epoch 38/300\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.3820 - accuracy: 0.2917\n",
      "Epoch 39/300\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.3783 - accuracy: 0.2917\n",
      "Epoch 40/300\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.3750 - accuracy: 0.2708\n",
      "Epoch 41/300\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.3720 - accuracy: 0.2917\n",
      "Epoch 42/300\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.3687 - accuracy: 0.2708\n",
      "Epoch 43/300\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.3658 - accuracy: 0.2708\n",
      "Epoch 44/300\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.3628 - accuracy: 0.2708\n",
      "Epoch 45/300\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.3600 - accuracy: 0.2708\n",
      "Epoch 46/300\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.3566 - accuracy: 0.2708\n",
      "Epoch 47/300\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.3535 - accuracy: 0.2708\n",
      "Epoch 48/300\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 1.3502 - accuracy: 0.2708\n",
      "Epoch 49/300\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.3475 - accuracy: 0.2708\n",
      "Epoch 50/300\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.3450 - accuracy: 0.2708\n",
      "Epoch 51/300\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.3424 - accuracy: 0.2708\n",
      "Epoch 52/300\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.3392 - accuracy: 0.2708\n",
      "Epoch 53/300\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.3362 - accuracy: 0.2708\n",
      "Epoch 54/300\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.3334 - accuracy: 0.2708\n",
      "Epoch 55/300\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.3305 - accuracy: 0.2708\n",
      "Epoch 56/300\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.3281 - accuracy: 0.2708\n",
      "Epoch 57/300\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.3255 - accuracy: 0.2708\n",
      "Epoch 58/300\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.3230 - accuracy: 0.2917\n",
      "Epoch 59/300\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.3210 - accuracy: 0.2917\n",
      "Epoch 60/300\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 1.3182 - accuracy: 0.2917\n",
      "Epoch 61/300\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.3155 - accuracy: 0.2917\n",
      "Epoch 62/300\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.3136 - accuracy: 0.2917\n",
      "Epoch 63/300\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.3113 - accuracy: 0.2917\n",
      "Epoch 64/300\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.3089 - accuracy: 0.3125\n",
      "Epoch 65/300\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.3067 - accuracy: 0.3125\n",
      "Epoch 66/300\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 1.3050 - accuracy: 0.3125\n",
      "Epoch 67/300\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.3030 - accuracy: 0.3125\n",
      "Epoch 68/300\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.3001 - accuracy: 0.3125\n",
      "Epoch 69/300\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 1.2982 - accuracy: 0.3125\n",
      "Epoch 70/300\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.2962 - accuracy: 0.3125\n",
      "Epoch 71/300\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.2941 - accuracy: 0.3125\n",
      "Epoch 72/300\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.2924 - accuracy: 0.3125\n",
      "Epoch 73/300\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.2907 - accuracy: 0.3125\n",
      "Epoch 74/300\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.2885 - accuracy: 0.3125\n",
      "Epoch 75/300\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.2865 - accuracy: 0.3125\n",
      "Epoch 76/300\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.2846 - accuracy: 0.3125\n",
      "Epoch 77/300\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.2828 - accuracy: 0.3125\n",
      "Epoch 78/300\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.2810 - accuracy: 0.3125\n",
      "Epoch 79/300\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.2789 - accuracy: 0.3125\n",
      "Epoch 80/300\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.2771 - accuracy: 0.3125\n",
      "Epoch 81/300\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.2753 - accuracy: 0.3125\n",
      "Epoch 82/300\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.2738 - accuracy: 0.3125\n",
      "Epoch 83/300\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.2720 - accuracy: 0.3125\n",
      "Epoch 84/300\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 1.2703 - accuracy: 0.3125\n",
      "Epoch 85/300\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.2684 - accuracy: 0.3125\n",
      "Epoch 86/300\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.2670 - accuracy: 0.3125\n",
      "Epoch 87/300\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.2655 - accuracy: 0.3125\n",
      "Epoch 88/300\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.2635 - accuracy: 0.3125\n",
      "Epoch 89/300\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.2618 - accuracy: 0.3125\n",
      "Epoch 90/300\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.2604 - accuracy: 0.3125\n",
      "Epoch 91/300\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.2586 - accuracy: 0.3125\n",
      "Epoch 92/300\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 1.2571 - accuracy: 0.3125\n",
      "Epoch 93/300\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.2556 - accuracy: 0.3125\n",
      "Epoch 94/300\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.2544 - accuracy: 0.3125\n",
      "Epoch 95/300\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 1.2530 - accuracy: 0.3125\n",
      "Epoch 96/300\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.2513 - accuracy: 0.3125\n",
      "Epoch 97/300\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.2498 - accuracy: 0.3125\n",
      "Epoch 98/300\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.2484 - accuracy: 0.3125\n",
      "Epoch 99/300\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 1.2472 - accuracy: 0.3333\n",
      "Epoch 100/300\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.2460 - accuracy: 0.3333\n",
      "Epoch 101/300\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 1.2439 - accuracy: 0.3333\n",
      "Epoch 102/300\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 1.2425 - accuracy: 0.3333\n",
      "Epoch 103/300\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.2410 - accuracy: 0.3333\n",
      "Epoch 104/300\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.2397 - accuracy: 0.3333\n",
      "Epoch 105/300\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.2383 - accuracy: 0.3542\n",
      "Epoch 106/300\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 1.2371 - accuracy: 0.3542\n",
      "Epoch 107/300\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 1.2355 - accuracy: 0.3542\n",
      "Epoch 108/300\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 1.2341 - accuracy: 0.3542\n",
      "Epoch 109/300\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 1.2329 - accuracy: 0.3542\n",
      "Epoch 110/300\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 1.2317 - accuracy: 0.3542\n",
      "Epoch 111/300\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.2302 - accuracy: 0.3542\n",
      "Epoch 112/300\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.2290 - accuracy: 0.3542\n",
      "Epoch 113/300\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 1.2280 - accuracy: 0.3750\n",
      "Epoch 114/300\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 1.2262 - accuracy: 0.3750\n",
      "Epoch 115/300\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.2249 - accuracy: 0.3542\n",
      "Epoch 116/300\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.2238 - accuracy: 0.3750\n",
      "Epoch 117/300\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.2224 - accuracy: 0.3750\n",
      "Epoch 118/300\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.2213 - accuracy: 0.3750\n",
      "Epoch 119/300\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.2200 - accuracy: 0.3750\n",
      "Epoch 120/300\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.2188 - accuracy: 0.3750\n",
      "Epoch 121/300\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.2178 - accuracy: 0.3750\n",
      "Epoch 122/300\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 1.2162 - accuracy: 0.3750\n",
      "Epoch 123/300\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.2151 - accuracy: 0.3750\n",
      "Epoch 124/300\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.2140 - accuracy: 0.3750\n",
      "Epoch 125/300\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.2128 - accuracy: 0.3958\n",
      "Epoch 126/300\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.2115 - accuracy: 0.3958\n",
      "Epoch 127/300\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.2106 - accuracy: 0.3958\n",
      "Epoch 128/300\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.2091 - accuracy: 0.3958\n",
      "Epoch 129/300\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.2079 - accuracy: 0.3958\n",
      "Epoch 130/300\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 1.2068 - accuracy: 0.3958\n",
      "Epoch 131/300\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.2056 - accuracy: 0.3958\n",
      "Epoch 132/300\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.2045 - accuracy: 0.3958\n",
      "Epoch 133/300\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.2033 - accuracy: 0.3958\n",
      "Epoch 134/300\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.2021 - accuracy: 0.3958\n",
      "Epoch 135/300\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.2011 - accuracy: 0.3958\n",
      "Epoch 136/300\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.1999 - accuracy: 0.3958\n",
      "Epoch 137/300\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 1.1988 - accuracy: 0.3958\n",
      "Epoch 138/300\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.1977 - accuracy: 0.3958\n",
      "Epoch 139/300\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.1970 - accuracy: 0.4167\n",
      "Epoch 140/300\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.1955 - accuracy: 0.4375\n",
      "Epoch 141/300\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 1.1940 - accuracy: 0.4375\n",
      "Epoch 142/300\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.1929 - accuracy: 0.4375\n",
      "Epoch 143/300\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.1919 - accuracy: 0.4375\n",
      "Epoch 144/300\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.1909 - accuracy: 0.4375\n",
      "Epoch 145/300\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.1895 - accuracy: 0.4375\n",
      "Epoch 146/300\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 1.1885 - accuracy: 0.4375\n",
      "Epoch 147/300\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.1873 - accuracy: 0.4375\n",
      "Epoch 148/300\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.1864 - accuracy: 0.4375\n",
      "Epoch 149/300\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.1851 - accuracy: 0.4375\n",
      "Epoch 150/300\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 1.1842 - accuracy: 0.4583\n",
      "Epoch 151/300\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.1832 - accuracy: 0.4792\n",
      "Epoch 152/300\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.1820 - accuracy: 0.4792\n",
      "Epoch 153/300\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.1810 - accuracy: 0.4792\n",
      "Epoch 154/300\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 1.1804 - accuracy: 0.4792\n",
      "Epoch 155/300\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.1794 - accuracy: 0.4792\n",
      "Epoch 156/300\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 1.1780 - accuracy: 0.4792\n",
      "Epoch 157/300\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.1769 - accuracy: 0.4792\n",
      "Epoch 158/300\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.1758 - accuracy: 0.4792\n",
      "Epoch 159/300\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.1748 - accuracy: 0.4792\n",
      "Epoch 160/300\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 1.1737 - accuracy: 0.4792\n",
      "Epoch 161/300\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.1727 - accuracy: 0.5000\n",
      "Epoch 162/300\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 1.1717 - accuracy: 0.5000\n",
      "Epoch 163/300\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.1708 - accuracy: 0.5000\n",
      "Epoch 164/300\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.1697 - accuracy: 0.4792\n",
      "Epoch 165/300\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 1.1691 - accuracy: 0.5000\n",
      "Epoch 166/300\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.1681 - accuracy: 0.4792\n",
      "Epoch 167/300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2/2 [==============================] - 0s 3ms/step - loss: 1.1671 - accuracy: 0.4792\n",
      "Epoch 168/300\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 1.1662 - accuracy: 0.5000\n",
      "Epoch 169/300\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.1653 - accuracy: 0.4792\n",
      "Epoch 170/300\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 1.1642 - accuracy: 0.5000\n",
      "Epoch 171/300\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 1.1634 - accuracy: 0.4792\n",
      "Epoch 172/300\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.1624 - accuracy: 0.4792\n",
      "Epoch 173/300\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 1.1616 - accuracy: 0.4792\n",
      "Epoch 174/300\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 1.1608 - accuracy: 0.4792\n",
      "Epoch 175/300\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.1599 - accuracy: 0.4792\n",
      "Epoch 176/300\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.1590 - accuracy: 0.4792\n",
      "Epoch 177/300\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.1580 - accuracy: 0.4792\n",
      "Epoch 178/300\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.1573 - accuracy: 0.4792\n",
      "Epoch 179/300\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 1.1562 - accuracy: 0.4792\n",
      "Epoch 180/300\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.1553 - accuracy: 0.5000\n",
      "Epoch 181/300\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.1548 - accuracy: 0.4792\n",
      "Epoch 182/300\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.1536 - accuracy: 0.4792\n",
      "Epoch 183/300\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.1529 - accuracy: 0.4792\n",
      "Epoch 184/300\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 1.1520 - accuracy: 0.4792\n",
      "Epoch 185/300\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.1509 - accuracy: 0.4792\n",
      "Epoch 186/300\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.1500 - accuracy: 0.4792\n",
      "Epoch 187/300\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.1493 - accuracy: 0.4792\n",
      "Epoch 188/300\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.1486 - accuracy: 0.4792\n",
      "Epoch 189/300\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 1.1476 - accuracy: 0.4792\n",
      "Epoch 190/300\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 1.1468 - accuracy: 0.4792\n",
      "Epoch 191/300\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 1.1462 - accuracy: 0.4792\n",
      "Epoch 192/300\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.1451 - accuracy: 0.4792\n",
      "Epoch 193/300\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.1443 - accuracy: 0.4792\n",
      "Epoch 194/300\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 1.1436 - accuracy: 0.4792\n",
      "Epoch 195/300\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.1428 - accuracy: 0.4792\n",
      "Epoch 196/300\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.1419 - accuracy: 0.4792\n",
      "Epoch 197/300\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 1.1415 - accuracy: 0.4792\n",
      "Epoch 198/300\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.1403 - accuracy: 0.4792\n",
      "Epoch 199/300\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 1.1397 - accuracy: 0.4792\n",
      "Epoch 200/300\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 1.1390 - accuracy: 0.4792\n",
      "Epoch 201/300\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.1385 - accuracy: 0.4792\n",
      "Epoch 202/300\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.1376 - accuracy: 0.4792\n",
      "Epoch 203/300\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.1368 - accuracy: 0.4792\n",
      "Epoch 204/300\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.1360 - accuracy: 0.4792\n",
      "Epoch 205/300\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.1354 - accuracy: 0.4792\n",
      "Epoch 206/300\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.1345 - accuracy: 0.4792\n",
      "Epoch 207/300\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.1338 - accuracy: 0.4792\n",
      "Epoch 208/300\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 1.1328 - accuracy: 0.4792\n",
      "Epoch 209/300\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 1.1321 - accuracy: 0.4792\n",
      "Epoch 210/300\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.1321 - accuracy: 0.4792\n",
      "Epoch 211/300\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.1306 - accuracy: 0.4792\n",
      "Epoch 212/300\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.1298 - accuracy: 0.4792\n",
      "Epoch 213/300\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.1290 - accuracy: 0.4792\n",
      "Epoch 214/300\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.1283 - accuracy: 0.4792\n",
      "Epoch 215/300\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.1275 - accuracy: 0.4792\n",
      "Epoch 216/300\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.1271 - accuracy: 0.4792\n",
      "Epoch 217/300\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.1262 - accuracy: 0.4792\n",
      "Epoch 218/300\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 1.1254 - accuracy: 0.4583\n",
      "Epoch 219/300\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 1.1248 - accuracy: 0.4583\n",
      "Epoch 220/300\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 1.1238 - accuracy: 0.4583\n",
      "Epoch 221/300\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 1.1232 - accuracy: 0.4583\n",
      "Epoch 222/300\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.1225 - accuracy: 0.4583\n",
      "Epoch 223/300\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.1224 - accuracy: 0.4583\n",
      "Epoch 224/300\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.1213 - accuracy: 0.4583\n",
      "Epoch 225/300\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.1203 - accuracy: 0.4583\n",
      "Epoch 226/300\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.1196 - accuracy: 0.4583\n",
      "Epoch 227/300\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 1.1191 - accuracy: 0.4583\n",
      "Epoch 228/300\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.1181 - accuracy: 0.4583\n",
      "Epoch 229/300\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.1175 - accuracy: 0.4583\n",
      "Epoch 230/300\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.1168 - accuracy: 0.4583\n",
      "Epoch 231/300\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.1161 - accuracy: 0.4583\n",
      "Epoch 232/300\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 1.1154 - accuracy: 0.4583\n",
      "Epoch 233/300\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 1.1147 - accuracy: 0.4583\n",
      "Epoch 234/300\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.1139 - accuracy: 0.4583\n",
      "Epoch 235/300\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.1133 - accuracy: 0.4583\n",
      "Epoch 236/300\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.1126 - accuracy: 0.4583\n",
      "Epoch 237/300\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.1118 - accuracy: 0.4583\n",
      "Epoch 238/300\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.1111 - accuracy: 0.4583\n",
      "Epoch 239/300\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.1106 - accuracy: 0.4583\n",
      "Epoch 240/300\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 1.1097 - accuracy: 0.4583\n",
      "Epoch 241/300\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.1092 - accuracy: 0.4583\n",
      "Epoch 242/300\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.1084 - accuracy: 0.4792\n",
      "Epoch 243/300\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.1077 - accuracy: 0.4792\n",
      "Epoch 244/300\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.1071 - accuracy: 0.4792\n",
      "Epoch 245/300\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.1064 - accuracy: 0.4792\n",
      "Epoch 246/300\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.1056 - accuracy: 0.4792\n",
      "Epoch 247/300\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 1.1051 - accuracy: 0.4792\n",
      "Epoch 248/300\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 1.1043 - accuracy: 0.4792\n",
      "Epoch 249/300\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.1038 - accuracy: 0.4792\n",
      "Epoch 250/300\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.1031 - accuracy: 0.4792\n",
      "Epoch 251/300\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 1.1023 - accuracy: 0.4792\n",
      "Epoch 252/300\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.1016 - accuracy: 0.4792\n",
      "Epoch 253/300\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.1010 - accuracy: 0.4792\n",
      "Epoch 254/300\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.1003 - accuracy: 0.4792\n",
      "Epoch 255/300\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.1000 - accuracy: 0.4792\n",
      "Epoch 256/300\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.0992 - accuracy: 0.5000\n",
      "Epoch 257/300\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 1.0987 - accuracy: 0.5000\n",
      "Epoch 258/300\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.0977 - accuracy: 0.4792\n",
      "Epoch 259/300\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.0972 - accuracy: 0.4792\n",
      "Epoch 260/300\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.0965 - accuracy: 0.5000\n",
      "Epoch 261/300\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.0961 - accuracy: 0.5000\n",
      "Epoch 262/300\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.0955 - accuracy: 0.5000\n",
      "Epoch 263/300\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.0950 - accuracy: 0.5000\n",
      "Epoch 264/300\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.0940 - accuracy: 0.4792\n",
      "Epoch 265/300\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 1.0933 - accuracy: 0.4792\n",
      "Epoch 266/300\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.0928 - accuracy: 0.4792\n",
      "Epoch 267/300\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.0929 - accuracy: 0.4792\n",
      "Epoch 268/300\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.0915 - accuracy: 0.5000\n",
      "Epoch 269/300\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.0910 - accuracy: 0.5000\n",
      "Epoch 270/300\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.0904 - accuracy: 0.5000\n",
      "Epoch 271/300\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.0898 - accuracy: 0.5000\n",
      "Epoch 272/300\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 1.0889 - accuracy: 0.4792\n",
      "Epoch 273/300\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 1.0888 - accuracy: 0.4792\n",
      "Epoch 274/300\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.0877 - accuracy: 0.5000\n",
      "Epoch 275/300\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 1.0872 - accuracy: 0.5000\n",
      "Epoch 276/300\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 1.0866 - accuracy: 0.5000\n",
      "Epoch 277/300\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 1.0858 - accuracy: 0.5000\n",
      "Epoch 278/300\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.0851 - accuracy: 0.5000\n",
      "Epoch 279/300\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.0846 - accuracy: 0.5000\n",
      "Epoch 280/300\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.0841 - accuracy: 0.5000\n",
      "Epoch 281/300\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.0833 - accuracy: 0.4792\n",
      "Epoch 282/300\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.0826 - accuracy: 0.4792\n",
      "Epoch 283/300\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.0819 - accuracy: 0.4792\n",
      "Epoch 284/300\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.0814 - accuracy: 0.4792\n",
      "Epoch 285/300\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.0806 - accuracy: 0.5000\n",
      "Epoch 286/300\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 1.0803 - accuracy: 0.5000\n",
      "Epoch 287/300\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.0795 - accuracy: 0.4792\n",
      "Epoch 288/300\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 1.0794 - accuracy: 0.4792\n",
      "Epoch 289/300\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.0783 - accuracy: 0.4792\n",
      "Epoch 290/300\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 1.0779 - accuracy: 0.5000\n",
      "Epoch 291/300\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.0772 - accuracy: 0.4792\n",
      "Epoch 292/300\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.0766 - accuracy: 0.4792\n",
      "Epoch 293/300\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.0763 - accuracy: 0.4792\n",
      "Epoch 294/300\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.0754 - accuracy: 0.4792\n",
      "Epoch 295/300\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.0753 - accuracy: 0.4792\n",
      "Epoch 296/300\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 1.0744 - accuracy: 0.4792\n",
      "Epoch 297/300\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.0739 - accuracy: 0.4792\n",
      "Epoch 298/300\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.0730 - accuracy: 0.4792\n",
      "Epoch 299/300\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.0725 - accuracy: 0.4792\n",
      "Epoch 300/300\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.0720 - accuracy: 0.4792\n",
      "Model: \"sequential_14\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " flatten_14 (Flatten)        (None, 2)                 0         \n",
      "                                                                 \n",
      " dense_28 (Dense)            (None, 15)                45        \n",
      "                                                                 \n",
      " dense_29 (Dense)            (None, 4)                 64        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 109\n",
      "Trainable params: 109\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "2/2 [==============================] - 0s 2ms/step\n",
      "Accuracy on train set: 47.92%\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "Accuracy on test set: 50.00%\n",
      "[[2 0 1 1]\n",
      " [0 1 0 3]\n",
      " [0 0 2 3]\n",
      " [0 0 0 3]]\n"
     ]
    }
   ],
   "source": [
    "run_mlp(X_train, y_train, X_test, y_test, 4, 15, 300)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looks like 150 epochs is the winner..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hidden layer tests (150 Epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/150\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 1.3543 - accuracy: 0.4167\n",
      "Epoch 2/150\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 1.3525 - accuracy: 0.4167\n",
      "Epoch 3/150\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.3506 - accuracy: 0.4167\n",
      "Epoch 4/150\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.3490 - accuracy: 0.4167\n",
      "Epoch 5/150\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.3472 - accuracy: 0.4167\n",
      "Epoch 6/150\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.3456 - accuracy: 0.4167\n",
      "Epoch 7/150\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.3437 - accuracy: 0.4167\n",
      "Epoch 8/150\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.3418 - accuracy: 0.4167\n",
      "Epoch 9/150\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.3400 - accuracy: 0.4167\n",
      "Epoch 10/150\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 1.3384 - accuracy: 0.4167\n",
      "Epoch 11/150\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 1.3371 - accuracy: 0.4167\n",
      "Epoch 12/150\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.3355 - accuracy: 0.4167\n",
      "Epoch 13/150\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 1.3340 - accuracy: 0.4167\n",
      "Epoch 14/150\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.3324 - accuracy: 0.4167\n",
      "Epoch 15/150\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.3310 - accuracy: 0.4167\n",
      "Epoch 16/150\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.3292 - accuracy: 0.4167\n",
      "Epoch 17/150\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.3279 - accuracy: 0.4167\n",
      "Epoch 18/150\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.3263 - accuracy: 0.4167\n",
      "Epoch 19/150\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.3248 - accuracy: 0.4167\n",
      "Epoch 20/150\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.3235 - accuracy: 0.4167\n",
      "Epoch 21/150\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.3222 - accuracy: 0.4167\n",
      "Epoch 22/150\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.3208 - accuracy: 0.4167\n",
      "Epoch 23/150\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.3195 - accuracy: 0.4167\n",
      "Epoch 24/150\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.3183 - accuracy: 0.4167\n",
      "Epoch 25/150\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.3172 - accuracy: 0.4167\n",
      "Epoch 26/150\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 1.3161 - accuracy: 0.3958\n",
      "Epoch 27/150\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 1.3148 - accuracy: 0.3958\n",
      "Epoch 28/150\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.3136 - accuracy: 0.3958\n",
      "Epoch 29/150\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.3125 - accuracy: 0.3958\n",
      "Epoch 30/150\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.3110 - accuracy: 0.3958\n",
      "Epoch 31/150\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.3098 - accuracy: 0.3958\n",
      "Epoch 32/150\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.3087 - accuracy: 0.3958\n",
      "Epoch 33/150\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.3076 - accuracy: 0.3958\n",
      "Epoch 34/150\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.3065 - accuracy: 0.3958\n",
      "Epoch 35/150\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.3055 - accuracy: 0.3958\n",
      "Epoch 36/150\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 1.3045 - accuracy: 0.3958\n",
      "Epoch 37/150\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.3034 - accuracy: 0.3958\n",
      "Epoch 38/150\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.3022 - accuracy: 0.3958\n",
      "Epoch 39/150\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.3009 - accuracy: 0.3958\n",
      "Epoch 40/150\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 1.2999 - accuracy: 0.3958\n",
      "Epoch 41/150\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 1.2988 - accuracy: 0.3958\n",
      "Epoch 42/150\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.2980 - accuracy: 0.3958\n",
      "Epoch 43/150\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.2967 - accuracy: 0.3958\n",
      "Epoch 44/150\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 1.2957 - accuracy: 0.3958\n",
      "Epoch 45/150\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.2945 - accuracy: 0.3958\n",
      "Epoch 46/150\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.2938 - accuracy: 0.3958\n",
      "Epoch 47/150\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.2927 - accuracy: 0.3958\n",
      "Epoch 48/150\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 1.2916 - accuracy: 0.3958\n",
      "Epoch 49/150\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 1.2908 - accuracy: 0.3958\n",
      "Epoch 50/150\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.2896 - accuracy: 0.3958\n",
      "Epoch 51/150\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.2886 - accuracy: 0.3958\n",
      "Epoch 52/150\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.2877 - accuracy: 0.3958\n",
      "Epoch 53/150\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.2869 - accuracy: 0.3958\n",
      "Epoch 54/150\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.2860 - accuracy: 0.3958\n",
      "Epoch 55/150\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.2851 - accuracy: 0.3958\n",
      "Epoch 56/150\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.2842 - accuracy: 0.4167\n",
      "Epoch 57/150\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 1.2835 - accuracy: 0.4167\n",
      "Epoch 58/150\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.2826 - accuracy: 0.4167\n",
      "Epoch 59/150\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 1.2818 - accuracy: 0.4167\n",
      "Epoch 60/150\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.2808 - accuracy: 0.4167\n",
      "Epoch 61/150\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.2800 - accuracy: 0.4167\n",
      "Epoch 62/150\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.2793 - accuracy: 0.4167\n",
      "Epoch 63/150\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 1.2783 - accuracy: 0.4167\n",
      "Epoch 64/150\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.2774 - accuracy: 0.4167\n",
      "Epoch 65/150\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 1.2767 - accuracy: 0.4167\n",
      "Epoch 66/150\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.2757 - accuracy: 0.4167\n",
      "Epoch 67/150\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 1.2748 - accuracy: 0.4167\n",
      "Epoch 68/150\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.2742 - accuracy: 0.4167\n",
      "Epoch 69/150\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.2740 - accuracy: 0.4167\n",
      "Epoch 70/150\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 1.2723 - accuracy: 0.4167\n",
      "Epoch 71/150\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.2718 - accuracy: 0.4167\n",
      "Epoch 72/150\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.2709 - accuracy: 0.4167\n",
      "Epoch 73/150\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 1.2701 - accuracy: 0.4167\n",
      "Epoch 74/150\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 1.2695 - accuracy: 0.4375\n",
      "Epoch 75/150\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 1.2687 - accuracy: 0.4167\n",
      "Epoch 76/150\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.2681 - accuracy: 0.4375\n",
      "Epoch 77/150\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 1.2674 - accuracy: 0.4375\n",
      "Epoch 78/150\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.2667 - accuracy: 0.4375\n",
      "Epoch 79/150\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.2661 - accuracy: 0.4375\n",
      "Epoch 80/150\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 1.2657 - accuracy: 0.4375\n",
      "Epoch 81/150\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 1.2649 - accuracy: 0.4375\n",
      "Epoch 82/150\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.2642 - accuracy: 0.4375\n",
      "Epoch 83/150\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.2635 - accuracy: 0.4375\n",
      "Epoch 84/150\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.2628 - accuracy: 0.4375\n",
      "Epoch 85/150\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.2623 - accuracy: 0.4375\n",
      "Epoch 86/150\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.2614 - accuracy: 0.4375\n",
      "Epoch 87/150\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 1.2608 - accuracy: 0.4375\n",
      "Epoch 88/150\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.2601 - accuracy: 0.4583\n",
      "Epoch 89/150\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 1.2594 - accuracy: 0.4583\n",
      "Epoch 90/150\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 1.2587 - accuracy: 0.4583\n",
      "Epoch 91/150\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.2580 - accuracy: 0.4583\n",
      "Epoch 92/150\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 1.2573 - accuracy: 0.4583\n",
      "Epoch 93/150\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 1.2566 - accuracy: 0.4583\n",
      "Epoch 94/150\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.2563 - accuracy: 0.4583\n",
      "Epoch 95/150\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.2553 - accuracy: 0.4583\n",
      "Epoch 96/150\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.2546 - accuracy: 0.4583\n",
      "Epoch 97/150\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.2539 - accuracy: 0.4583\n",
      "Epoch 98/150\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.2532 - accuracy: 0.4583\n",
      "Epoch 99/150\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.2526 - accuracy: 0.4583\n",
      "Epoch 100/150\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.2519 - accuracy: 0.4583\n",
      "Epoch 101/150\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.2511 - accuracy: 0.4583\n",
      "Epoch 102/150\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.2505 - accuracy: 0.4583\n",
      "Epoch 103/150\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.2499 - accuracy: 0.4583\n",
      "Epoch 104/150\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.2495 - accuracy: 0.4583\n",
      "Epoch 105/150\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.2488 - accuracy: 0.4583\n",
      "Epoch 106/150\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.2483 - accuracy: 0.4583\n",
      "Epoch 107/150\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.2476 - accuracy: 0.4583\n",
      "Epoch 108/150\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.2473 - accuracy: 0.4583\n",
      "Epoch 109/150\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.2467 - accuracy: 0.4583\n",
      "Epoch 110/150\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.2458 - accuracy: 0.4583\n",
      "Epoch 111/150\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.2451 - accuracy: 0.4583\n",
      "Epoch 112/150\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.2447 - accuracy: 0.4583\n",
      "Epoch 113/150\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.2438 - accuracy: 0.4583\n",
      "Epoch 114/150\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.2435 - accuracy: 0.4583\n",
      "Epoch 115/150\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.2427 - accuracy: 0.4583\n",
      "Epoch 116/150\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 1.2423 - accuracy: 0.4583\n",
      "Epoch 117/150\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.2417 - accuracy: 0.4583\n",
      "Epoch 118/150\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.2411 - accuracy: 0.4583\n",
      "Epoch 119/150\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.2405 - accuracy: 0.4583\n",
      "Epoch 120/150\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.2401 - accuracy: 0.4583\n",
      "Epoch 121/150\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.2394 - accuracy: 0.4583\n",
      "Epoch 122/150\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.2391 - accuracy: 0.4583\n",
      "Epoch 123/150\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.2385 - accuracy: 0.4583\n",
      "Epoch 124/150\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.2380 - accuracy: 0.4583\n",
      "Epoch 125/150\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 1.2374 - accuracy: 0.4583\n",
      "Epoch 126/150\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.2371 - accuracy: 0.4583\n",
      "Epoch 127/150\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 1.2364 - accuracy: 0.4583\n",
      "Epoch 128/150\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.2361 - accuracy: 0.4583\n",
      "Epoch 129/150\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.2356 - accuracy: 0.4583\n",
      "Epoch 130/150\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.2350 - accuracy: 0.4583\n",
      "Epoch 131/150\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 1.2345 - accuracy: 0.4583\n",
      "Epoch 132/150\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 1.2341 - accuracy: 0.4583\n",
      "Epoch 133/150\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.2334 - accuracy: 0.4583\n",
      "Epoch 134/150\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.2329 - accuracy: 0.4583\n",
      "Epoch 135/150\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.2325 - accuracy: 0.4583\n",
      "Epoch 136/150\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.2319 - accuracy: 0.4583\n",
      "Epoch 137/150\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.2315 - accuracy: 0.4583\n",
      "Epoch 138/150\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.2309 - accuracy: 0.4583\n",
      "Epoch 139/150\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.2306 - accuracy: 0.4583\n",
      "Epoch 140/150\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.2301 - accuracy: 0.4583\n",
      "Epoch 141/150\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.2296 - accuracy: 0.4583\n",
      "Epoch 142/150\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 1.2293 - accuracy: 0.4583\n",
      "Epoch 143/150\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 1.2286 - accuracy: 0.4583\n",
      "Epoch 144/150\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.2282 - accuracy: 0.4583\n",
      "Epoch 145/150\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.2277 - accuracy: 0.4583\n",
      "Epoch 146/150\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.2271 - accuracy: 0.4583\n",
      "Epoch 147/150\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.2267 - accuracy: 0.4583\n",
      "Epoch 148/150\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.2263 - accuracy: 0.4583\n",
      "Epoch 149/150\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.2258 - accuracy: 0.4583\n",
      "Epoch 150/150\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 1.2253 - accuracy: 0.4583\n",
      "Model: \"sequential_15\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " flatten_15 (Flatten)        (None, 2)                 0         \n",
      "                                                                 \n",
      " dense_30 (Dense)            (None, 1)                 3         \n",
      "                                                                 \n",
      " dense_31 (Dense)            (None, 4)                 8         \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 11\n",
      "Trainable params: 11\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "2/2 [==============================] - 0s 2ms/step\n",
      "Accuracy on train set: 45.83%\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "Accuracy on test set: 37.50%\n",
      "[[2 1 0 1]\n",
      " [0 1 0 3]\n",
      " [1 0 0 4]\n",
      " [0 0 0 3]]\n"
     ]
    }
   ],
   "source": [
    "run_mlp(X_train, y_train, X_test, y_test, 4, 1, 150)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/150\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 1.4220 - accuracy: 0.2292\n",
      "Epoch 2/150\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 1.4196 - accuracy: 0.2292\n",
      "Epoch 3/150\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 1.4174 - accuracy: 0.2292\n",
      "Epoch 4/150\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 1.4156 - accuracy: 0.2292\n",
      "Epoch 5/150\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 1.4133 - accuracy: 0.2292\n",
      "Epoch 6/150\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.4111 - accuracy: 0.2292\n",
      "Epoch 7/150\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.4092 - accuracy: 0.2292\n",
      "Epoch 8/150\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.4071 - accuracy: 0.2292\n",
      "Epoch 9/150\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.4052 - accuracy: 0.2292\n",
      "Epoch 10/150\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.4030 - accuracy: 0.2292\n",
      "Epoch 11/150\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.4011 - accuracy: 0.2292\n",
      "Epoch 12/150\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.3993 - accuracy: 0.2292\n",
      "Epoch 13/150\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.3976 - accuracy: 0.2708\n",
      "Epoch 14/150\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.3959 - accuracy: 0.2708\n",
      "Epoch 15/150\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.3943 - accuracy: 0.2708\n",
      "Epoch 16/150\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.3926 - accuracy: 0.2708\n",
      "Epoch 17/150\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.3910 - accuracy: 0.2708\n",
      "Epoch 18/150\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.3892 - accuracy: 0.2708\n",
      "Epoch 19/150\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.3874 - accuracy: 0.2708\n",
      "Epoch 20/150\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 1.3857 - accuracy: 0.2708\n",
      "Epoch 21/150\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.3840 - accuracy: 0.2708\n",
      "Epoch 22/150\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.3822 - accuracy: 0.2708\n",
      "Epoch 23/150\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.3807 - accuracy: 0.2708\n",
      "Epoch 24/150\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.3790 - accuracy: 0.2500\n",
      "Epoch 25/150\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.3774 - accuracy: 0.2500\n",
      "Epoch 26/150\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.3757 - accuracy: 0.2708\n",
      "Epoch 27/150\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.3744 - accuracy: 0.2708\n",
      "Epoch 28/150\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.3728 - accuracy: 0.2708\n",
      "Epoch 29/150\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.3713 - accuracy: 0.2708\n",
      "Epoch 30/150\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.3696 - accuracy: 0.2708\n",
      "Epoch 31/150\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 1.3680 - accuracy: 0.2708\n",
      "Epoch 32/150\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.3666 - accuracy: 0.2708\n",
      "Epoch 33/150\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.3650 - accuracy: 0.2500\n",
      "Epoch 34/150\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.3635 - accuracy: 0.2500\n",
      "Epoch 35/150\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.3619 - accuracy: 0.2500\n",
      "Epoch 36/150\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 1.3605 - accuracy: 0.2708\n",
      "Epoch 37/150\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.3592 - accuracy: 0.2708\n",
      "Epoch 38/150\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 1.3579 - accuracy: 0.2708\n",
      "Epoch 39/150\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 1.3568 - accuracy: 0.2708\n",
      "Epoch 40/150\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.3551 - accuracy: 0.2708\n",
      "Epoch 41/150\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 1.3539 - accuracy: 0.2708\n",
      "Epoch 42/150\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.3523 - accuracy: 0.2708\n",
      "Epoch 43/150\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 1.3511 - accuracy: 0.3125\n",
      "Epoch 44/150\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 1.3499 - accuracy: 0.3125\n",
      "Epoch 45/150\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 1.3487 - accuracy: 0.3125\n",
      "Epoch 46/150\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.3472 - accuracy: 0.3125\n",
      "Epoch 47/150\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.3459 - accuracy: 0.3125\n",
      "Epoch 48/150\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.3446 - accuracy: 0.3125\n",
      "Epoch 49/150\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.3434 - accuracy: 0.3125\n",
      "Epoch 50/150\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.3419 - accuracy: 0.3125\n",
      "Epoch 51/150\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.3407 - accuracy: 0.3333\n",
      "Epoch 52/150\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.3395 - accuracy: 0.3333\n",
      "Epoch 53/150\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.3382 - accuracy: 0.3333\n",
      "Epoch 54/150\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.3368 - accuracy: 0.3333\n",
      "Epoch 55/150\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 1.3356 - accuracy: 0.3333\n",
      "Epoch 56/150\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.3345 - accuracy: 0.3333\n",
      "Epoch 57/150\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 1.3332 - accuracy: 0.3333\n",
      "Epoch 58/150\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 1.3319 - accuracy: 0.3542\n",
      "Epoch 59/150\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 1.3308 - accuracy: 0.3542\n",
      "Epoch 60/150\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.3296 - accuracy: 0.3750\n",
      "Epoch 61/150\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.3284 - accuracy: 0.3750\n",
      "Epoch 62/150\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.3274 - accuracy: 0.3750\n",
      "Epoch 63/150\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.3264 - accuracy: 0.3750\n",
      "Epoch 64/150\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.3252 - accuracy: 0.3750\n",
      "Epoch 65/150\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.3242 - accuracy: 0.3750\n",
      "Epoch 66/150\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.3231 - accuracy: 0.3958\n",
      "Epoch 67/150\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 1.3218 - accuracy: 0.3958\n",
      "Epoch 68/150\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 1.3209 - accuracy: 0.3958\n",
      "Epoch 69/150\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.3197 - accuracy: 0.3958\n",
      "Epoch 70/150\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.3186 - accuracy: 0.3958\n",
      "Epoch 71/150\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.3174 - accuracy: 0.3958\n",
      "Epoch 72/150\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.3164 - accuracy: 0.3958\n",
      "Epoch 73/150\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.3152 - accuracy: 0.3958\n",
      "Epoch 74/150\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 1.3141 - accuracy: 0.3958\n",
      "Epoch 75/150\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.3132 - accuracy: 0.3958\n",
      "Epoch 76/150\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 1.3118 - accuracy: 0.4167\n",
      "Epoch 77/150\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.3107 - accuracy: 0.4167\n",
      "Epoch 78/150\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.3097 - accuracy: 0.4375\n",
      "Epoch 79/150\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 1.3086 - accuracy: 0.4375\n",
      "Epoch 80/150\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.3075 - accuracy: 0.4375\n",
      "Epoch 81/150\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.3066 - accuracy: 0.4375\n",
      "Epoch 82/150\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.3055 - accuracy: 0.4167\n",
      "Epoch 83/150\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 1.3046 - accuracy: 0.4375\n",
      "Epoch 84/150\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 1.3038 - accuracy: 0.4375\n",
      "Epoch 85/150\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 1.3027 - accuracy: 0.4375\n",
      "Epoch 86/150\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 1.3017 - accuracy: 0.4167\n",
      "Epoch 87/150\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.3007 - accuracy: 0.4167\n",
      "Epoch 88/150\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.2997 - accuracy: 0.4167\n",
      "Epoch 89/150\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.2987 - accuracy: 0.4167\n",
      "Epoch 90/150\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.2977 - accuracy: 0.3958\n",
      "Epoch 91/150\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.2968 - accuracy: 0.3958\n",
      "Epoch 92/150\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.2958 - accuracy: 0.3958\n",
      "Epoch 93/150\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.2949 - accuracy: 0.3958\n",
      "Epoch 94/150\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.2940 - accuracy: 0.3958\n",
      "Epoch 95/150\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.2931 - accuracy: 0.3958\n",
      "Epoch 96/150\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 1.2923 - accuracy: 0.3958\n",
      "Epoch 97/150\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.2912 - accuracy: 0.3958\n",
      "Epoch 98/150\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 1.2904 - accuracy: 0.3958\n",
      "Epoch 99/150\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.2895 - accuracy: 0.3958\n",
      "Epoch 100/150\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.2884 - accuracy: 0.3958\n",
      "Epoch 101/150\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.2876 - accuracy: 0.3958\n",
      "Epoch 102/150\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 1.2867 - accuracy: 0.3958\n",
      "Epoch 103/150\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.2857 - accuracy: 0.3958\n",
      "Epoch 104/150\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.2848 - accuracy: 0.3958\n",
      "Epoch 105/150\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.2839 - accuracy: 0.3958\n",
      "Epoch 106/150\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.2828 - accuracy: 0.4167\n",
      "Epoch 107/150\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.2820 - accuracy: 0.4167\n",
      "Epoch 108/150\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 1.2811 - accuracy: 0.4167\n",
      "Epoch 109/150\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.2804 - accuracy: 0.4167\n",
      "Epoch 110/150\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.2792 - accuracy: 0.4167\n",
      "Epoch 111/150\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.2783 - accuracy: 0.4167\n",
      "Epoch 112/150\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 1.2775 - accuracy: 0.4167\n",
      "Epoch 113/150\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.2764 - accuracy: 0.4167\n",
      "Epoch 114/150\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.2756 - accuracy: 0.4167\n",
      "Epoch 115/150\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.2747 - accuracy: 0.3958\n",
      "Epoch 116/150\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.2738 - accuracy: 0.3958\n",
      "Epoch 117/150\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.2730 - accuracy: 0.3958\n",
      "Epoch 118/150\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.2720 - accuracy: 0.3958\n",
      "Epoch 119/150\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.2710 - accuracy: 0.3958\n",
      "Epoch 120/150\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.2700 - accuracy: 0.3958\n",
      "Epoch 121/150\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.2691 - accuracy: 0.3958\n",
      "Epoch 122/150\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.2683 - accuracy: 0.4167\n",
      "Epoch 123/150\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.2674 - accuracy: 0.4167\n",
      "Epoch 124/150\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.2665 - accuracy: 0.4167\n",
      "Epoch 125/150\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.2656 - accuracy: 0.4167\n",
      "Epoch 126/150\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 1.2647 - accuracy: 0.4167\n",
      "Epoch 127/150\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.2638 - accuracy: 0.4167\n",
      "Epoch 128/150\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 1.2629 - accuracy: 0.4167\n",
      "Epoch 129/150\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.2619 - accuracy: 0.4167\n",
      "Epoch 130/150\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.2612 - accuracy: 0.4167\n",
      "Epoch 131/150\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.2602 - accuracy: 0.4167\n",
      "Epoch 132/150\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.2595 - accuracy: 0.4167\n",
      "Epoch 133/150\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 1.2584 - accuracy: 0.4167\n",
      "Epoch 134/150\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.2575 - accuracy: 0.4167\n",
      "Epoch 135/150\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 1.2566 - accuracy: 0.4167\n",
      "Epoch 136/150\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.2560 - accuracy: 0.4375\n",
      "Epoch 137/150\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.2548 - accuracy: 0.4167\n",
      "Epoch 138/150\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.2539 - accuracy: 0.4167\n",
      "Epoch 139/150\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.2530 - accuracy: 0.4375\n",
      "Epoch 140/150\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 1.2519 - accuracy: 0.4375\n",
      "Epoch 141/150\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.2512 - accuracy: 0.4375\n",
      "Epoch 142/150\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.2506 - accuracy: 0.4375\n",
      "Epoch 143/150\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.2494 - accuracy: 0.4583\n",
      "Epoch 144/150\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.2485 - accuracy: 0.4792\n",
      "Epoch 145/150\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.2476 - accuracy: 0.4792\n",
      "Epoch 146/150\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.2466 - accuracy: 0.4792\n",
      "Epoch 147/150\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.2458 - accuracy: 0.5000\n",
      "Epoch 148/150\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.2448 - accuracy: 0.5000\n",
      "Epoch 149/150\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 1.2440 - accuracy: 0.5000\n",
      "Epoch 150/150\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.2429 - accuracy: 0.5000\n",
      "Model: \"sequential_16\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " flatten_16 (Flatten)        (None, 2)                 0         \n",
      "                                                                 \n",
      " dense_32 (Dense)            (None, 10)                30        \n",
      "                                                                 \n",
      " dense_33 (Dense)            (None, 4)                 44        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 74\n",
      "Trainable params: 74\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "2/2 [==============================] - 0s 2ms/step\n",
      "Accuracy on train set: 50.00%\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "Accuracy on test set: 50.00%\n",
      "[[2 0 1 1]\n",
      " [0 0 0 4]\n",
      " [0 0 3 2]\n",
      " [0 0 0 3]]\n"
     ]
    }
   ],
   "source": [
    "run_mlp(X_train, y_train, X_test, y_test, 4, 10, 150)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/150\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 1.4962 - accuracy: 0.2083\n",
      "Epoch 2/150\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.4851 - accuracy: 0.2083\n",
      "Epoch 3/150\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.4749 - accuracy: 0.2083\n",
      "Epoch 4/150\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.4661 - accuracy: 0.2083\n",
      "Epoch 5/150\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.4575 - accuracy: 0.2083\n",
      "Epoch 6/150\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 1.4470 - accuracy: 0.2083\n",
      "Epoch 7/150\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.4392 - accuracy: 0.2083\n",
      "Epoch 8/150\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.4320 - accuracy: 0.2083\n",
      "Epoch 9/150\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.4245 - accuracy: 0.2083\n",
      "Epoch 10/150\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.4167 - accuracy: 0.2083\n",
      "Epoch 11/150\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.4106 - accuracy: 0.2083\n",
      "Epoch 12/150\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.4042 - accuracy: 0.2083\n",
      "Epoch 13/150\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 1.3981 - accuracy: 0.1875\n",
      "Epoch 14/150\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.3922 - accuracy: 0.1875\n",
      "Epoch 15/150\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.3867 - accuracy: 0.2083\n",
      "Epoch 16/150\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.3808 - accuracy: 0.2083\n",
      "Epoch 17/150\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.3752 - accuracy: 0.2083\n",
      "Epoch 18/150\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.3699 - accuracy: 0.2083\n",
      "Epoch 19/150\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 1.3646 - accuracy: 0.2083\n",
      "Epoch 20/150\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.3583 - accuracy: 0.2083\n",
      "Epoch 21/150\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.3531 - accuracy: 0.2917\n",
      "Epoch 22/150\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 1.3487 - accuracy: 0.2917\n",
      "Epoch 23/150\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.3438 - accuracy: 0.3125\n",
      "Epoch 24/150\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 1.3381 - accuracy: 0.3125\n",
      "Epoch 25/150\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.3330 - accuracy: 0.3542\n",
      "Epoch 26/150\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.3286 - accuracy: 0.3542\n",
      "Epoch 27/150\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.3249 - accuracy: 0.3542\n",
      "Epoch 28/150\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.3210 - accuracy: 0.3542\n",
      "Epoch 29/150\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.3172 - accuracy: 0.3750\n",
      "Epoch 30/150\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.3136 - accuracy: 0.3750\n",
      "Epoch 31/150\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.3107 - accuracy: 0.3750\n",
      "Epoch 32/150\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.3070 - accuracy: 0.3750\n",
      "Epoch 33/150\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 1.3039 - accuracy: 0.3750\n",
      "Epoch 34/150\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.3008 - accuracy: 0.3542\n",
      "Epoch 35/150\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.2974 - accuracy: 0.3542\n",
      "Epoch 36/150\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.2941 - accuracy: 0.3750\n",
      "Epoch 37/150\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.2911 - accuracy: 0.3750\n",
      "Epoch 38/150\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.2883 - accuracy: 0.3750\n",
      "Epoch 39/150\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 1.2859 - accuracy: 0.3750\n",
      "Epoch 40/150\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 1.2833 - accuracy: 0.3750\n",
      "Epoch 41/150\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.2804 - accuracy: 0.3750\n",
      "Epoch 42/150\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.2774 - accuracy: 0.3750\n",
      "Epoch 43/150\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.2744 - accuracy: 0.3750\n",
      "Epoch 44/150\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.2715 - accuracy: 0.3750\n",
      "Epoch 45/150\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.2690 - accuracy: 0.3958\n",
      "Epoch 46/150\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.2667 - accuracy: 0.3958\n",
      "Epoch 47/150\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.2643 - accuracy: 0.3958\n",
      "Epoch 48/150\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.2612 - accuracy: 0.3958\n",
      "Epoch 49/150\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.2586 - accuracy: 0.3958\n",
      "Epoch 50/150\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.2562 - accuracy: 0.3958\n",
      "Epoch 51/150\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.2537 - accuracy: 0.3958\n",
      "Epoch 52/150\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.2515 - accuracy: 0.3958\n",
      "Epoch 53/150\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 1.2497 - accuracy: 0.3958\n",
      "Epoch 54/150\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 1.2473 - accuracy: 0.3958\n",
      "Epoch 55/150\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.2447 - accuracy: 0.3958\n",
      "Epoch 56/150\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 1.2426 - accuracy: 0.4167\n",
      "Epoch 57/150\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 1.2407 - accuracy: 0.4167\n",
      "Epoch 58/150\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.2387 - accuracy: 0.4167\n",
      "Epoch 59/150\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.2366 - accuracy: 0.4167\n",
      "Epoch 60/150\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.2348 - accuracy: 0.4375\n",
      "Epoch 61/150\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 1.2326 - accuracy: 0.4167\n",
      "Epoch 62/150\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.2308 - accuracy: 0.4167\n",
      "Epoch 63/150\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 1.2293 - accuracy: 0.4167\n",
      "Epoch 64/150\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.2274 - accuracy: 0.4375\n",
      "Epoch 65/150\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.2265 - accuracy: 0.4375\n",
      "Epoch 66/150\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.2240 - accuracy: 0.4167\n",
      "Epoch 67/150\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 1.2221 - accuracy: 0.4167\n",
      "Epoch 68/150\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 1.2207 - accuracy: 0.4167\n",
      "Epoch 69/150\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.2188 - accuracy: 0.4167\n",
      "Epoch 70/150\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.2169 - accuracy: 0.4167\n",
      "Epoch 71/150\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.2153 - accuracy: 0.4167\n",
      "Epoch 72/150\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.2140 - accuracy: 0.4167\n",
      "Epoch 73/150\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.2122 - accuracy: 0.4167\n",
      "Epoch 74/150\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.2107 - accuracy: 0.4167\n",
      "Epoch 75/150\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.2092 - accuracy: 0.4167\n",
      "Epoch 76/150\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 1.2080 - accuracy: 0.4167\n",
      "Epoch 77/150\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 1.2064 - accuracy: 0.4167\n",
      "Epoch 78/150\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 1.2050 - accuracy: 0.4167\n",
      "Epoch 79/150\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.2042 - accuracy: 0.4167\n",
      "Epoch 80/150\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.2023 - accuracy: 0.4167\n",
      "Epoch 81/150\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.2009 - accuracy: 0.4167\n",
      "Epoch 82/150\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 1.1994 - accuracy: 0.4375\n",
      "Epoch 83/150\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.1980 - accuracy: 0.4167\n",
      "Epoch 84/150\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.1967 - accuracy: 0.4167\n",
      "Epoch 85/150\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.1951 - accuracy: 0.4375\n",
      "Epoch 86/150\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.1939 - accuracy: 0.4583\n",
      "Epoch 87/150\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.1924 - accuracy: 0.4583\n",
      "Epoch 88/150\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.1911 - accuracy: 0.4583\n",
      "Epoch 89/150\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.1897 - accuracy: 0.4375\n",
      "Epoch 90/150\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.1887 - accuracy: 0.4583\n",
      "Epoch 91/150\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.1871 - accuracy: 0.4375\n",
      "Epoch 92/150\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.1858 - accuracy: 0.4375\n",
      "Epoch 93/150\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.1847 - accuracy: 0.4375\n",
      "Epoch 94/150\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.1835 - accuracy: 0.4375\n",
      "Epoch 95/150\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.1822 - accuracy: 0.4375\n",
      "Epoch 96/150\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.1809 - accuracy: 0.4375\n",
      "Epoch 97/150\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.1798 - accuracy: 0.4375\n",
      "Epoch 98/150\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.1785 - accuracy: 0.4375\n",
      "Epoch 99/150\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.1773 - accuracy: 0.4583\n",
      "Epoch 100/150\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.1764 - accuracy: 0.4583\n",
      "Epoch 101/150\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.1754 - accuracy: 0.4583\n",
      "Epoch 102/150\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 1.1740 - accuracy: 0.4375\n",
      "Epoch 103/150\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 1.1727 - accuracy: 0.4375\n",
      "Epoch 104/150\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.1714 - accuracy: 0.4375\n",
      "Epoch 105/150\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 1.1707 - accuracy: 0.4375\n",
      "Epoch 106/150\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.1693 - accuracy: 0.4375\n",
      "Epoch 107/150\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.1681 - accuracy: 0.4375\n",
      "Epoch 108/150\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.1672 - accuracy: 0.4375\n",
      "Epoch 109/150\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.1664 - accuracy: 0.4375\n",
      "Epoch 110/150\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 1.1648 - accuracy: 0.4375\n",
      "Epoch 111/150\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 1.1637 - accuracy: 0.4375\n",
      "Epoch 112/150\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 1.1630 - accuracy: 0.4375\n",
      "Epoch 113/150\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.1616 - accuracy: 0.4375\n",
      "Epoch 114/150\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.1608 - accuracy: 0.4375\n",
      "Epoch 115/150\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.1596 - accuracy: 0.4375\n",
      "Epoch 116/150\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.1589 - accuracy: 0.4375\n",
      "Epoch 117/150\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 1.1577 - accuracy: 0.4375\n",
      "Epoch 118/150\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.1567 - accuracy: 0.4375\n",
      "Epoch 119/150\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.1555 - accuracy: 0.4375\n",
      "Epoch 120/150\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 1.1546 - accuracy: 0.4375\n",
      "Epoch 121/150\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.1537 - accuracy: 0.4375\n",
      "Epoch 122/150\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.1524 - accuracy: 0.4375\n",
      "Epoch 123/150\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.1514 - accuracy: 0.4375\n",
      "Epoch 124/150\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.1509 - accuracy: 0.4375\n",
      "Epoch 125/150\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.1498 - accuracy: 0.4375\n",
      "Epoch 126/150\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.1487 - accuracy: 0.4375\n",
      "Epoch 127/150\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.1475 - accuracy: 0.4375\n",
      "Epoch 128/150\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.1464 - accuracy: 0.4375\n",
      "Epoch 129/150\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.1455 - accuracy: 0.4375\n",
      "Epoch 130/150\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.1446 - accuracy: 0.4375\n",
      "Epoch 131/150\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.1438 - accuracy: 0.4375\n",
      "Epoch 132/150\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.1426 - accuracy: 0.4375\n",
      "Epoch 133/150\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.1417 - accuracy: 0.4375\n",
      "Epoch 134/150\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.1410 - accuracy: 0.4375\n",
      "Epoch 135/150\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.1403 - accuracy: 0.4375\n",
      "Epoch 136/150\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 1.1391 - accuracy: 0.4375\n",
      "Epoch 137/150\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.1383 - accuracy: 0.4375\n",
      "Epoch 138/150\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.1372 - accuracy: 0.4375\n",
      "Epoch 139/150\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.1364 - accuracy: 0.4375\n",
      "Epoch 140/150\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.1356 - accuracy: 0.4375\n",
      "Epoch 141/150\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 1.1348 - accuracy: 0.4375\n",
      "Epoch 142/150\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.1337 - accuracy: 0.4375\n",
      "Epoch 143/150\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.1329 - accuracy: 0.4375\n",
      "Epoch 144/150\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.1325 - accuracy: 0.4375\n",
      "Epoch 145/150\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.1311 - accuracy: 0.4375\n",
      "Epoch 146/150\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.1306 - accuracy: 0.4375\n",
      "Epoch 147/150\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.1296 - accuracy: 0.4375\n",
      "Epoch 148/150\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.1287 - accuracy: 0.4375\n",
      "Epoch 149/150\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.1276 - accuracy: 0.4375\n",
      "Epoch 150/150\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.1268 - accuracy: 0.4375\n",
      "Model: \"sequential_17\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " flatten_17 (Flatten)        (None, 2)                 0         \n",
      "                                                                 \n",
      " dense_34 (Dense)            (None, 25)                75        \n",
      "                                                                 \n",
      " dense_35 (Dense)            (None, 4)                 104       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 179\n",
      "Trainable params: 179\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "2/2 [==============================] - 0s 2ms/step\n",
      "Accuracy on train set: 43.75%\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "Accuracy on test set: 56.25%\n",
      "[[2 0 1 1]\n",
      " [0 1 0 3]\n",
      " [0 0 3 2]\n",
      " [0 0 0 3]]\n"
     ]
    }
   ],
   "source": [
    "run_mlp(X_train, y_train, X_test, y_test, 4, 25, 150)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/150\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 1.3845 - accuracy: 0.2708\n",
      "Epoch 2/150\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 1.3810 - accuracy: 0.2708\n",
      "Epoch 3/150\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.3768 - accuracy: 0.2708\n",
      "Epoch 4/150\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.3731 - accuracy: 0.2708\n",
      "Epoch 5/150\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.3686 - accuracy: 0.2708\n",
      "Epoch 6/150\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.3643 - accuracy: 0.2917\n",
      "Epoch 7/150\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.3612 - accuracy: 0.3125\n",
      "Epoch 8/150\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.3574 - accuracy: 0.2917\n",
      "Epoch 9/150\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.3538 - accuracy: 0.2917\n",
      "Epoch 10/150\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.3504 - accuracy: 0.2917\n",
      "Epoch 11/150\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.3469 - accuracy: 0.2917\n",
      "Epoch 12/150\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.3438 - accuracy: 0.2708\n",
      "Epoch 13/150\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.3404 - accuracy: 0.2708\n",
      "Epoch 14/150\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.3376 - accuracy: 0.2292\n",
      "Epoch 15/150\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.3348 - accuracy: 0.2500\n",
      "Epoch 16/150\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.3318 - accuracy: 0.2292\n",
      "Epoch 17/150\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 1.3287 - accuracy: 0.2500\n",
      "Epoch 18/150\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.3255 - accuracy: 0.2292\n",
      "Epoch 19/150\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.3228 - accuracy: 0.2500\n",
      "Epoch 20/150\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.3199 - accuracy: 0.2500\n",
      "Epoch 21/150\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.3168 - accuracy: 0.2292\n",
      "Epoch 22/150\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.3141 - accuracy: 0.2292\n",
      "Epoch 23/150\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.3113 - accuracy: 0.2083\n",
      "Epoch 24/150\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.3085 - accuracy: 0.2708\n",
      "Epoch 25/150\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.3058 - accuracy: 0.2500\n",
      "Epoch 26/150\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.3034 - accuracy: 0.2917\n",
      "Epoch 27/150\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.3011 - accuracy: 0.3125\n",
      "Epoch 28/150\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.2979 - accuracy: 0.3125\n",
      "Epoch 29/150\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.2953 - accuracy: 0.3542\n",
      "Epoch 30/150\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.2928 - accuracy: 0.3750\n",
      "Epoch 31/150\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 1.2904 - accuracy: 0.3958\n",
      "Epoch 32/150\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.2880 - accuracy: 0.3542\n",
      "Epoch 33/150\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.2851 - accuracy: 0.3750\n",
      "Epoch 34/150\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.2826 - accuracy: 0.3542\n",
      "Epoch 35/150\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.2801 - accuracy: 0.3333\n",
      "Epoch 36/150\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.2782 - accuracy: 0.3750\n",
      "Epoch 37/150\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.2759 - accuracy: 0.4167\n",
      "Epoch 38/150\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 1.2735 - accuracy: 0.4167\n",
      "Epoch 39/150\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.2714 - accuracy: 0.4167\n",
      "Epoch 40/150\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.2694 - accuracy: 0.4167\n",
      "Epoch 41/150\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.2673 - accuracy: 0.4167\n",
      "Epoch 42/150\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.2649 - accuracy: 0.4167\n",
      "Epoch 43/150\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.2629 - accuracy: 0.4167\n",
      "Epoch 44/150\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 1.2608 - accuracy: 0.4167\n",
      "Epoch 45/150\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.2587 - accuracy: 0.4167\n",
      "Epoch 46/150\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.2564 - accuracy: 0.4167\n",
      "Epoch 47/150\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.2543 - accuracy: 0.4167\n",
      "Epoch 48/150\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.2529 - accuracy: 0.4167\n",
      "Epoch 49/150\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 1.2507 - accuracy: 0.4167\n",
      "Epoch 50/150\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.2488 - accuracy: 0.4167\n",
      "Epoch 51/150\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.2468 - accuracy: 0.4167\n",
      "Epoch 52/150\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.2450 - accuracy: 0.4167\n",
      "Epoch 53/150\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.2440 - accuracy: 0.3958\n",
      "Epoch 54/150\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.2409 - accuracy: 0.4167\n",
      "Epoch 55/150\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.2394 - accuracy: 0.4167\n",
      "Epoch 56/150\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.2372 - accuracy: 0.4167\n",
      "Epoch 57/150\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.2355 - accuracy: 0.4167\n",
      "Epoch 58/150\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.2346 - accuracy: 0.4167\n",
      "Epoch 59/150\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.2323 - accuracy: 0.3958\n",
      "Epoch 60/150\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.2307 - accuracy: 0.3958\n",
      "Epoch 61/150\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.2293 - accuracy: 0.3958\n",
      "Epoch 62/150\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.2274 - accuracy: 0.3958\n",
      "Epoch 63/150\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.2257 - accuracy: 0.3958\n",
      "Epoch 64/150\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.2242 - accuracy: 0.3958\n",
      "Epoch 65/150\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.2227 - accuracy: 0.3958\n",
      "Epoch 66/150\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.2205 - accuracy: 0.3958\n",
      "Epoch 67/150\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.2192 - accuracy: 0.3958\n",
      "Epoch 68/150\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.2174 - accuracy: 0.3958\n",
      "Epoch 69/150\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.2158 - accuracy: 0.3958\n",
      "Epoch 70/150\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 1.2146 - accuracy: 0.3958\n",
      "Epoch 71/150\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.2128 - accuracy: 0.3958\n",
      "Epoch 72/150\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.2111 - accuracy: 0.3958\n",
      "Epoch 73/150\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 1.2095 - accuracy: 0.3958\n",
      "Epoch 74/150\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.2082 - accuracy: 0.3958\n",
      "Epoch 75/150\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.2071 - accuracy: 0.3958\n",
      "Epoch 76/150\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.2052 - accuracy: 0.3958\n",
      "Epoch 77/150\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.2039 - accuracy: 0.3958\n",
      "Epoch 78/150\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.2025 - accuracy: 0.3958\n",
      "Epoch 79/150\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 1.2009 - accuracy: 0.3958\n",
      "Epoch 80/150\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 1.1994 - accuracy: 0.3958\n",
      "Epoch 81/150\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 1.1981 - accuracy: 0.3958\n",
      "Epoch 82/150\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 1.1968 - accuracy: 0.3958\n",
      "Epoch 83/150\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.1955 - accuracy: 0.3958\n",
      "Epoch 84/150\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.1946 - accuracy: 0.3958\n",
      "Epoch 85/150\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.1928 - accuracy: 0.3958\n",
      "Epoch 86/150\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.1916 - accuracy: 0.3958\n",
      "Epoch 87/150\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.1902 - accuracy: 0.3958\n",
      "Epoch 88/150\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 1.1888 - accuracy: 0.3958\n",
      "Epoch 89/150\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 1.1874 - accuracy: 0.3958\n",
      "Epoch 90/150\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.1860 - accuracy: 0.3958\n",
      "Epoch 91/150\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.1847 - accuracy: 0.3958\n",
      "Epoch 92/150\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.1833 - accuracy: 0.4375\n",
      "Epoch 93/150\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.1821 - accuracy: 0.4375\n",
      "Epoch 94/150\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.1810 - accuracy: 0.4375\n",
      "Epoch 95/150\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 1.1799 - accuracy: 0.4375\n",
      "Epoch 96/150\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 1.1784 - accuracy: 0.4375\n",
      "Epoch 97/150\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.1773 - accuracy: 0.4375\n",
      "Epoch 98/150\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.1758 - accuracy: 0.4375\n",
      "Epoch 99/150\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 1.1748 - accuracy: 0.4375\n",
      "Epoch 100/150\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.1734 - accuracy: 0.4375\n",
      "Epoch 101/150\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.1722 - accuracy: 0.4375\n",
      "Epoch 102/150\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.1710 - accuracy: 0.4375\n",
      "Epoch 103/150\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.1699 - accuracy: 0.4375\n",
      "Epoch 104/150\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.1687 - accuracy: 0.4375\n",
      "Epoch 105/150\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.1675 - accuracy: 0.4375\n",
      "Epoch 106/150\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.1662 - accuracy: 0.4375\n",
      "Epoch 107/150\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.1655 - accuracy: 0.4167\n",
      "Epoch 108/150\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.1641 - accuracy: 0.4375\n",
      "Epoch 109/150\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.1627 - accuracy: 0.4375\n",
      "Epoch 110/150\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.1618 - accuracy: 0.4167\n",
      "Epoch 111/150\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.1606 - accuracy: 0.4167\n",
      "Epoch 112/150\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.1599 - accuracy: 0.4375\n",
      "Epoch 113/150\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.1582 - accuracy: 0.4375\n",
      "Epoch 114/150\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.1575 - accuracy: 0.4167\n",
      "Epoch 115/150\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.1560 - accuracy: 0.4375\n",
      "Epoch 116/150\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.1548 - accuracy: 0.4167\n",
      "Epoch 117/150\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.1539 - accuracy: 0.4167\n",
      "Epoch 118/150\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 1.1527 - accuracy: 0.4167\n",
      "Epoch 119/150\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.1516 - accuracy: 0.4167\n",
      "Epoch 120/150\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.1506 - accuracy: 0.4167\n",
      "Epoch 121/150\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.1495 - accuracy: 0.4167\n",
      "Epoch 122/150\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.1484 - accuracy: 0.4167\n",
      "Epoch 123/150\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.1474 - accuracy: 0.4167\n",
      "Epoch 124/150\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.1462 - accuracy: 0.4167\n",
      "Epoch 125/150\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.1459 - accuracy: 0.4167\n",
      "Epoch 126/150\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.1441 - accuracy: 0.4167\n",
      "Epoch 127/150\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 1.1430 - accuracy: 0.4167\n",
      "Epoch 128/150\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.1423 - accuracy: 0.4167\n",
      "Epoch 129/150\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 1.1410 - accuracy: 0.4167\n",
      "Epoch 130/150\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.1399 - accuracy: 0.4167\n",
      "Epoch 131/150\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.1389 - accuracy: 0.4167\n",
      "Epoch 132/150\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.1382 - accuracy: 0.4167\n",
      "Epoch 133/150\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.1370 - accuracy: 0.4375\n",
      "Epoch 134/150\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 1.1360 - accuracy: 0.4375\n",
      "Epoch 135/150\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.1348 - accuracy: 0.4375\n",
      "Epoch 136/150\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 1.1339 - accuracy: 0.4375\n",
      "Epoch 137/150\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.1330 - accuracy: 0.4375\n",
      "Epoch 138/150\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.1320 - accuracy: 0.4375\n",
      "Epoch 139/150\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.1312 - accuracy: 0.4375\n",
      "Epoch 140/150\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 1.1298 - accuracy: 0.4375\n",
      "Epoch 141/150\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.1289 - accuracy: 0.4375\n",
      "Epoch 142/150\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.1281 - accuracy: 0.4375\n",
      "Epoch 143/150\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 1.1273 - accuracy: 0.4375\n",
      "Epoch 144/150\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.1264 - accuracy: 0.4375\n",
      "Epoch 145/150\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.1257 - accuracy: 0.4375\n",
      "Epoch 146/150\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.1246 - accuracy: 0.4375\n",
      "Epoch 147/150\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.1236 - accuracy: 0.4375\n",
      "Epoch 148/150\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.1228 - accuracy: 0.4375\n",
      "Epoch 149/150\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.1216 - accuracy: 0.4375\n",
      "Epoch 150/150\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.1207 - accuracy: 0.4375\n",
      "Model: \"sequential_18\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " flatten_18 (Flatten)        (None, 2)                 0         \n",
      "                                                                 \n",
      " dense_36 (Dense)            (None, 50)                150       \n",
      "                                                                 \n",
      " dense_37 (Dense)            (None, 4)                 204       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 354\n",
      "Trainable params: 354\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "2/2 [==============================] - 0s 1ms/step\n",
      "Accuracy on train set: 43.75%\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "Accuracy on test set: 43.75%\n",
      "[[2 0 1 1]\n",
      " [0 1 0 3]\n",
      " [0 0 1 4]\n",
      " [0 0 0 3]]\n"
     ]
    }
   ],
   "source": [
    "run_mlp(X_train, y_train, X_test, y_test, 4, 50, 150)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reflection\n",
    "\n",
    "##### Epochs\n",
    "\n",
    "Number of epochs | Accuracy\n",
    "\n",
    "15 | 12.50\n",
    "\n",
    "50 | 31.25\n",
    "\n",
    "150| 56.25\n",
    "\n",
    "300| 50.00\n",
    "\n",
    "\n",
    "##### Hidden Layers\n",
    "\n",
    "Number of HL | Accuracy\n",
    "\n",
    "1 | 37.50\n",
    "\n",
    "10| 50.00\n",
    "\n",
    "15| 56.25\n",
    "\n",
    "50| 45.75\n",
    "\n",
    "\n",
    "The first set of cells tested which number of epochs yeilded the best accuracy. In contrast to the low variance set in the previous exercise, more epochs did not corelated to a higher accuracy. 150 epochs tested 6% higher than 300 epochs. Thus, we will use 150 epochs for our next set of tests since it had the most accurate predictions. The next set of cells tests the accuracy of the model with regards to number of hidden layers. After testing 1, 5, 25, abd 50 hidden layers, the top performing being 25 hidden layers. Thus, the model with the highest accuracy in regard to high variance blobs has 150 epochs and 25 hidden layers. \n",
    "\n",
    "Also, it is intresting to see that the number of hidden layers may lower test accuracy, but one can see that it is not due to over fitting. I would think that the training accuracy would always go up with an increase in hidden layers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pistachio Dataset\n",
    "The blob data experiments were interesting, but are not representative of a real-world problem.  Next, we will use data from an industrial pistachio classifier designed to identify different varities of pistachio nuts.  \n",
    "\n",
    "https://www.kaggle.com/datasets/muratkokludataset/pistachio-image-dataset\n",
    "\n",
    "\n",
    "Let's start by loading some data.  Because this is image data, we are going to use a generator to bring in the data.  This also allows us to add augmentation to the images to hopefully grow the robustness of our algorithm. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 1719 images belonging to 2 classes.\n",
      "Found 429 images belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "datagen = ImageDataGenerator(\n",
    "            validation_split=0.2,\n",
    "            rescale=1./255, # to bring the image range from 0..255 to 0..1\n",
    "            rotation_range=0,  # randomly rotate images in the range (degrees, 0 to 180)\n",
    "            zoom_range = 0, # randomly zoom image \n",
    "            width_shift_range=0,  # randomly shift images horizontally (fraction of total width)\n",
    "            height_shift_range=0,  # randomly shift images vertically (fraction of total height)\n",
    "            horizontal_flip=False,  # randomly flip images\n",
    "            vertical_flip=False) # randomly flip images\n",
    "train_it = datagen.flow_from_directory( '/data/cs2300/pistachio/', \n",
    "                                           target_size=(224,224), \n",
    "                                           color_mode='grayscale', \n",
    "                                           batch_size=1,\n",
    "                                           class_mode=\"categorical\",\n",
    "                                           shuffle=True,\n",
    "                                           subset='training')\n",
    "valid_it = datagen.flow_from_directory( '/data/cs2300/pistachio/', \n",
    "                                           target_size=(224,224), \n",
    "                                           color_mode='grayscale', \n",
    "                                           shuffle=True,\n",
    "                                           batch_size=1,\n",
    "                                           class_mode=\"categorical\",\n",
    "                                           subset='validation')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our MLP code is expecting a NumPy array, so we need to build it.  This isn't the most elegant approach, but it gets the job done to allow our previous MLP code to work.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = []\n",
    "y = []\n",
    "batch_index = 0\n",
    "\n",
    "while batch_index <= train_it.batch_index:\n",
    "    #iterate through the training data and build a single array\n",
    "    x_temp, y_temp = train_it.next()\n",
    "    X.append(np.squeeze(x_temp[0]))\n",
    "    y.append(np.squeeze(y_temp[0]))\n",
    "    batch_index = batch_index + 1\n",
    "\n",
    "X_train = np.asarray(X)\n",
    "y_train = np.asarray(y)\n",
    "\n",
    "X = []\n",
    "y = []\n",
    "batch_index = 0\n",
    "\n",
    "while batch_index <= valid_it.batch_index:\n",
    "    #iterate through the test data to build a single array\n",
    "    x_temp, y_temp = valid_it.next()\n",
    "    X.append(np.squeeze(x_temp[0]))\n",
    "    y.append(np.squeeze(y_temp[0]))\n",
    "    batch_index = batch_index + 1\n",
    "\n",
    "X_test = np.asarray(X)\n",
    "y_test = np.asarray(y)\n",
    "X_train_reshaped = X_train.reshape(1719,50176)\n",
    "X_test_reshaped = X_test.reshape(429,50176)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following cell is a function that creates a DNN with a single hidden layer.  It includes the methods to train and evaluate the model as well.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_binary_mlp(X_train, y_train, X_test, y_test, hidden=16, epochs=100):\n",
    "    model = Sequential()\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(hidden, activation='relu'))\n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "    model.compile(loss=\"binary_crossentropy\",\n",
    "              optimizer='adam',\n",
    "              metrics=['accuracy'])\n",
    "    history = model.fit(x=X_train,y=y_train,\n",
    "                        validation_data = (X_test, y_test),\n",
    "                        batch_size=1,epochs=epochs,\n",
    "                        verbose=1)\n",
    "    model.summary()\n",
    "    predicted_probabilities = model.predict(X_train)\n",
    "    predicted_probabilities = np.rint(predicted_probabilities)\n",
    "    acc = 100. * accuracy_score(y_train, predicted_probabilities)\n",
    "    print(\"Accuracy on train set: {:.2f}%\".format(acc))\n",
    "    predicted_probabilities = model.predict(X_test)\n",
    "    predicted_probabilities = np.rint(predicted_probabilities)\n",
    "    acc = 100. * accuracy_score(y_test, predicted_probabilities)\n",
    "    print(\"Accuracy on test set: {:.2f}%\".format(acc))\n",
    "    print(confusion_matrix(y_test, predicted_probabilities))\n",
    "    return history\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next cell calls the previous function and plots the results.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "1719/1719 [==============================] - 5s 3ms/step - loss: 0.7440 - accuracy: 0.5736 - val_loss: 0.6846 - val_accuracy: 0.5734\n",
      "Epoch 2/10\n",
      "1719/1719 [==============================] - 5s 3ms/step - loss: 0.6836 - accuracy: 0.5736 - val_loss: 0.6827 - val_accuracy: 0.5734\n",
      "Epoch 3/10\n",
      "1719/1719 [==============================] - 5s 3ms/step - loss: 0.6827 - accuracy: 0.5736 - val_loss: 0.6824 - val_accuracy: 0.5734\n",
      "Epoch 4/10\n",
      "1719/1719 [==============================] - 5s 3ms/step - loss: 0.6825 - accuracy: 0.5736 - val_loss: 0.6823 - val_accuracy: 0.5734\n",
      "Epoch 5/10\n",
      "1719/1719 [==============================] - 5s 3ms/step - loss: 0.6825 - accuracy: 0.5736 - val_loss: 0.6823 - val_accuracy: 0.5734\n",
      "Epoch 6/10\n",
      "1719/1719 [==============================] - 5s 3ms/step - loss: 0.6825 - accuracy: 0.5736 - val_loss: 0.6823 - val_accuracy: 0.5734\n",
      "Epoch 7/10\n",
      "1719/1719 [==============================] - 5s 3ms/step - loss: 0.6825 - accuracy: 0.5736 - val_loss: 0.6823 - val_accuracy: 0.5734\n",
      "Epoch 8/10\n",
      "1719/1719 [==============================] - 4s 3ms/step - loss: 0.6825 - accuracy: 0.5736 - val_loss: 0.6823 - val_accuracy: 0.5734\n",
      "Epoch 9/10\n",
      "1719/1719 [==============================] - 5s 3ms/step - loss: 0.6825 - accuracy: 0.5736 - val_loss: 0.6823 - val_accuracy: 0.5734\n",
      "Epoch 10/10\n",
      "1719/1719 [==============================] - 5s 3ms/step - loss: 0.6825 - accuracy: 0.5736 - val_loss: 0.6823 - val_accuracy: 0.5734\n",
      "Model: \"sequential_19\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " flatten_19 (Flatten)        (1, 50176)                0         \n",
      "                                                                 \n",
      " dense_38 (Dense)            (1, 30)                   1505310   \n",
      "                                                                 \n",
      " dense_39 (Dense)            (1, 1)                    31        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 1,505,341\n",
      "Trainable params: 1,505,341\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "54/54 [==============================] - 0s 3ms/step\n",
      "Accuracy on train set: 57.36%\n",
      "14/14 [==============================] - 0s 3ms/step\n",
      "Accuracy on test set: 57.34%\n",
      "[[  0 183]\n",
      " [  0 246]]\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "history = run_binary_mlp(X_train_reshaped, y_train[:,0], X_test_reshaped, y_test[:,0],30,10)\n",
    "plt.plot(history.history['accuracy'])\n",
    "plt.plot(history.history['val_accuracy'])\n",
    "plt.title('model accuracy')\n",
    "plt.ylabel('accuracy')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'validation'], loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 5\n",
    "In the following cell, execute at least 5 more experiments that experiment with the number of hidden neurons and number of epochs.  Create a table in the reflection and results section below that shows the configuration hyperparameters, total model parameters, test accuracy, and training accuracy.  Describe why you think the highest accuracy configuration outperformed your other experiments.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Hidden Layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "1719/1719 [==============================] - 5s 3ms/step - loss: 0.6876 - accuracy: 0.5730 - val_loss: 0.6842 - val_accuracy: 0.5734\n",
      "Epoch 2/10\n",
      "1719/1719 [==============================] - 5s 3ms/step - loss: 0.6833 - accuracy: 0.5736 - val_loss: 0.6825 - val_accuracy: 0.5734\n",
      "Epoch 3/10\n",
      "1719/1719 [==============================] - 5s 3ms/step - loss: 0.6826 - accuracy: 0.5736 - val_loss: 0.6823 - val_accuracy: 0.5734\n",
      "Epoch 4/10\n",
      "1719/1719 [==============================] - 5s 3ms/step - loss: 0.6826 - accuracy: 0.5736 - val_loss: 0.6823 - val_accuracy: 0.5734\n",
      "Epoch 5/10\n",
      "1719/1719 [==============================] - 5s 3ms/step - loss: 0.6825 - accuracy: 0.5736 - val_loss: 0.6823 - val_accuracy: 0.5734\n",
      "Epoch 6/10\n",
      "1719/1719 [==============================] - 5s 3ms/step - loss: 0.6825 - accuracy: 0.5736 - val_loss: 0.6823 - val_accuracy: 0.5734\n",
      "Epoch 7/10\n",
      "1719/1719 [==============================] - 5s 3ms/step - loss: 0.6825 - accuracy: 0.5736 - val_loss: 0.6823 - val_accuracy: 0.5734\n",
      "Epoch 8/10\n",
      "1719/1719 [==============================] - 5s 3ms/step - loss: 0.6825 - accuracy: 0.5736 - val_loss: 0.6823 - val_accuracy: 0.5734\n",
      "Epoch 9/10\n",
      "1719/1719 [==============================] - 5s 3ms/step - loss: 0.6825 - accuracy: 0.5736 - val_loss: 0.6823 - val_accuracy: 0.5734\n",
      "Epoch 10/10\n",
      "1719/1719 [==============================] - 5s 3ms/step - loss: 0.6825 - accuracy: 0.5736 - val_loss: 0.6823 - val_accuracy: 0.5734\n",
      "Model: \"sequential_24\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " flatten_24 (Flatten)        (1, 50176)                0         \n",
      "                                                                 \n",
      " dense_48 (Dense)            (1, 1)                    50177     \n",
      "                                                                 \n",
      " dense_49 (Dense)            (1, 1)                    2         \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 50,179\n",
      "Trainable params: 50,179\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "54/54 [==============================] - 0s 3ms/step\n",
      "Accuracy on train set: 57.36%\n",
      "14/14 [==============================] - 0s 3ms/step\n",
      "Accuracy on test set: 57.34%\n",
      "[[  0 183]\n",
      " [  0 246]]\n"
     ]
    }
   ],
   "source": [
    "history = run_binary_mlp(X_train_reshaped, y_train[:,0], X_test_reshaped, y_test[:,0],1,10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "1719/1719 [==============================] - 5s 3ms/step - loss: 0.7081 - accuracy: 0.5817 - val_loss: 0.6836 - val_accuracy: 0.5734\n",
      "Epoch 2/10\n",
      "1719/1719 [==============================] - 5s 3ms/step - loss: 0.6832 - accuracy: 0.5736 - val_loss: 0.6825 - val_accuracy: 0.5734\n",
      "Epoch 3/10\n",
      "1719/1719 [==============================] - 5s 3ms/step - loss: 0.6826 - accuracy: 0.5736 - val_loss: 0.6824 - val_accuracy: 0.5734\n",
      "Epoch 4/10\n",
      "1719/1719 [==============================] - 5s 3ms/step - loss: 0.6825 - accuracy: 0.5736 - val_loss: 0.6823 - val_accuracy: 0.5734\n",
      "Epoch 5/10\n",
      "1719/1719 [==============================] - 5s 3ms/step - loss: 0.6825 - accuracy: 0.5736 - val_loss: 0.6823 - val_accuracy: 0.5734\n",
      "Epoch 6/10\n",
      "1719/1719 [==============================] - 5s 3ms/step - loss: 0.6825 - accuracy: 0.5736 - val_loss: 0.6823 - val_accuracy: 0.5734\n",
      "Epoch 7/10\n",
      "1719/1719 [==============================] - 5s 3ms/step - loss: 0.6825 - accuracy: 0.5736 - val_loss: 0.6823 - val_accuracy: 0.5734\n",
      "Epoch 8/10\n",
      "1719/1719 [==============================] - 5s 3ms/step - loss: 0.6825 - accuracy: 0.5736 - val_loss: 0.6823 - val_accuracy: 0.5734\n",
      "Epoch 9/10\n",
      "1719/1719 [==============================] - 5s 3ms/step - loss: 0.6825 - accuracy: 0.5736 - val_loss: 0.6823 - val_accuracy: 0.5734\n",
      "Epoch 10/10\n",
      "1719/1719 [==============================] - 5s 3ms/step - loss: 0.6824 - accuracy: 0.5736 - val_loss: 0.6823 - val_accuracy: 0.5734\n",
      "Model: \"sequential_25\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " flatten_25 (Flatten)        (1, 50176)                0         \n",
      "                                                                 \n",
      " dense_50 (Dense)            (1, 10)                   501770    \n",
      "                                                                 \n",
      " dense_51 (Dense)            (1, 1)                    11        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 501,781\n",
      "Trainable params: 501,781\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "54/54 [==============================] - 0s 3ms/step\n",
      "Accuracy on train set: 57.36%\n",
      "14/14 [==============================] - 0s 3ms/step\n",
      "Accuracy on test set: 57.34%\n",
      "[[  0 183]\n",
      " [  0 246]]\n"
     ]
    }
   ],
   "source": [
    "history = run_binary_mlp(X_train_reshaped, y_train[:,0], X_test_reshaped, y_test[:,0],10,10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "1719/1719 [==============================] - 5s 3ms/step - loss: 0.6959 - accuracy: 0.6574 - val_loss: 0.6831 - val_accuracy: 0.5734\n",
      "Epoch 2/10\n",
      "1719/1719 [==============================] - 5s 3ms/step - loss: 0.6830 - accuracy: 0.5736 - val_loss: 0.6823 - val_accuracy: 0.5734\n",
      "Epoch 3/10\n",
      "1719/1719 [==============================] - 5s 3ms/step - loss: 0.6826 - accuracy: 0.5736 - val_loss: 0.6821 - val_accuracy: 0.5734\n",
      "Epoch 4/10\n",
      "1719/1719 [==============================] - 5s 3ms/step - loss: 0.6824 - accuracy: 0.5736 - val_loss: 0.6821 - val_accuracy: 0.5734\n",
      "Epoch 5/10\n",
      "1719/1719 [==============================] - 5s 3ms/step - loss: 0.6825 - accuracy: 0.5736 - val_loss: 0.6821 - val_accuracy: 0.5734\n",
      "Epoch 6/10\n",
      "1719/1719 [==============================] - 5s 3ms/step - loss: 0.6825 - accuracy: 0.5736 - val_loss: 0.6821 - val_accuracy: 0.5734\n",
      "Epoch 7/10\n",
      "1719/1719 [==============================] - 5s 3ms/step - loss: 0.6825 - accuracy: 0.5736 - val_loss: 0.6821 - val_accuracy: 0.5734\n",
      "Epoch 8/10\n",
      "1719/1719 [==============================] - 5s 3ms/step - loss: 0.6825 - accuracy: 0.5736 - val_loss: 0.6821 - val_accuracy: 0.5734\n",
      "Epoch 9/10\n",
      "1719/1719 [==============================] - 5s 3ms/step - loss: 0.6825 - accuracy: 0.5736 - val_loss: 0.6821 - val_accuracy: 0.5734\n",
      "Epoch 10/10\n",
      "1719/1719 [==============================] - 5s 3ms/step - loss: 0.6824 - accuracy: 0.5736 - val_loss: 0.6821 - val_accuracy: 0.5734\n",
      "Model: \"sequential_27\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " flatten_27 (Flatten)        (1, 50176)                0         \n",
      "                                                                 \n",
      " dense_54 (Dense)            (1, 50)                   2508850   \n",
      "                                                                 \n",
      " dense_55 (Dense)            (1, 1)                    51        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 2,508,901\n",
      "Trainable params: 2,508,901\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "54/54 [==============================] - 0s 3ms/step\n",
      "Accuracy on train set: 57.36%\n",
      "14/14 [==============================] - 0s 4ms/step\n",
      "Accuracy on test set: 57.34%\n",
      "[[  0 183]\n",
      " [  0 246]]\n"
     ]
    }
   ],
   "source": [
    "history = run_binary_mlp(X_train_reshaped, y_train[:,0], X_test_reshaped, y_test[:,0],50,10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "1719/1719 [==============================] - 6s 3ms/step - loss: 0.6038 - accuracy: 0.7516 - val_loss: 0.5004 - val_accuracy: 0.8042\n",
      "Epoch 2/10\n",
      "1719/1719 [==============================] - 5s 3ms/step - loss: 0.4662 - accuracy: 0.7976 - val_loss: 0.5971 - val_accuracy: 0.7716\n",
      "Epoch 3/10\n",
      "1719/1719 [==============================] - 5s 3ms/step - loss: 0.4351 - accuracy: 0.8261 - val_loss: 0.4610 - val_accuracy: 0.7949\n",
      "Epoch 4/10\n",
      "1719/1719 [==============================] - 5s 3ms/step - loss: 0.4171 - accuracy: 0.8272 - val_loss: 0.4557 - val_accuracy: 0.8019\n",
      "Epoch 5/10\n",
      "1719/1719 [==============================] - 5s 3ms/step - loss: 0.4014 - accuracy: 0.8325 - val_loss: 0.5367 - val_accuracy: 0.7343\n",
      "Epoch 6/10\n",
      "1719/1719 [==============================] - 5s 3ms/step - loss: 0.3975 - accuracy: 0.8418 - val_loss: 0.4600 - val_accuracy: 0.8112\n",
      "Epoch 7/10\n",
      "1719/1719 [==============================] - 5s 3ms/step - loss: 0.3899 - accuracy: 0.8400 - val_loss: 0.4715 - val_accuracy: 0.8089\n",
      "Epoch 8/10\n",
      "1719/1719 [==============================] - 5s 3ms/step - loss: 0.3879 - accuracy: 0.8371 - val_loss: 0.4400 - val_accuracy: 0.8065\n",
      "Epoch 9/10\n",
      "1719/1719 [==============================] - 5s 3ms/step - loss: 0.3825 - accuracy: 0.8487 - val_loss: 0.4310 - val_accuracy: 0.8089\n",
      "Epoch 10/10\n",
      "1719/1719 [==============================] - 5s 3ms/step - loss: 0.3654 - accuracy: 0.8522 - val_loss: 0.4283 - val_accuracy: 0.8159\n",
      "Model: \"sequential_28\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " flatten_28 (Flatten)        (1, 50176)                0         \n",
      "                                                                 \n",
      " dense_56 (Dense)            (1, 100)                  5017700   \n",
      "                                                                 \n",
      " dense_57 (Dense)            (1, 1)                    101       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 5,017,801\n",
      "Trainable params: 5,017,801\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "54/54 [==============================] - 0s 3ms/step\n",
      "Accuracy on train set: 86.04%\n",
      "14/14 [==============================] - 0s 3ms/step\n",
      "Accuracy on test set: 81.59%\n",
      "[[161  22]\n",
      " [ 57 189]]\n"
     ]
    }
   ],
   "source": [
    "history = run_binary_mlp(X_train_reshaped, y_train[:,0], X_test_reshaped, y_test[:,0],100,10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Intresting observation: The first three tests had the exact same train and validation accuracy. This is not an error because as you track each epoch, the accuracies are still different. However, the final test with 100 hidden layers yeiled the highest accuracy for training and validation. So, we will use 100 hidden layers for our epoch tests"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1719/1719 [==============================] - 6s 3ms/step - loss: 0.7225 - accuracy: 0.7632 - val_loss: 0.4341 - val_accuracy: 0.7902\n",
      "Model: \"sequential_29\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " flatten_29 (Flatten)        (1, 50176)                0         \n",
      "                                                                 \n",
      " dense_58 (Dense)            (1, 100)                  5017700   \n",
      "                                                                 \n",
      " dense_59 (Dense)            (1, 1)                    101       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 5,017,801\n",
      "Trainable params: 5,017,801\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "54/54 [==============================] - 0s 3ms/step\n",
      "Accuracy on train set: 84.06%\n",
      "14/14 [==============================] - 0s 3ms/step\n",
      "Accuracy on test set: 79.02%\n",
      "[[151  32]\n",
      " [ 58 188]]\n"
     ]
    }
   ],
   "source": [
    "history = run_binary_mlp(X_train_reshaped, y_train[:,0], X_test_reshaped, y_test[:,0],100,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "1719/1719 [==============================] - 6s 3ms/step - loss: 0.7065 - accuracy: 0.7254 - val_loss: 0.4340 - val_accuracy: 0.8042\n",
      "Epoch 2/10\n",
      "1719/1719 [==============================] - 5s 3ms/step - loss: 0.4157 - accuracy: 0.8191 - val_loss: 0.4331 - val_accuracy: 0.7995\n",
      "Epoch 3/10\n",
      "1719/1719 [==============================] - 5s 3ms/step - loss: 0.3919 - accuracy: 0.8301 - val_loss: 0.4495 - val_accuracy: 0.7972\n",
      "Epoch 4/10\n",
      "1719/1719 [==============================] - 5s 3ms/step - loss: 0.3788 - accuracy: 0.8377 - val_loss: 0.4641 - val_accuracy: 0.7855\n",
      "Epoch 5/10\n",
      "1719/1719 [==============================] - 6s 3ms/step - loss: 0.3689 - accuracy: 0.8400 - val_loss: 0.4005 - val_accuracy: 0.8182\n",
      "Epoch 6/10\n",
      "1719/1719 [==============================] - 6s 3ms/step - loss: 0.3649 - accuracy: 0.8511 - val_loss: 0.4451 - val_accuracy: 0.7972\n",
      "Epoch 7/10\n",
      "1719/1719 [==============================] - 6s 3ms/step - loss: 0.3481 - accuracy: 0.8458 - val_loss: 0.4126 - val_accuracy: 0.8182\n",
      "Epoch 8/10\n",
      "1719/1719 [==============================] - 6s 3ms/step - loss: 0.3442 - accuracy: 0.8499 - val_loss: 0.4705 - val_accuracy: 0.7925\n",
      "Epoch 9/10\n",
      "1719/1719 [==============================] - 6s 3ms/step - loss: 0.3363 - accuracy: 0.8551 - val_loss: 0.3904 - val_accuracy: 0.8135\n",
      "Epoch 10/10\n",
      "1719/1719 [==============================] - 6s 3ms/step - loss: 0.3288 - accuracy: 0.8598 - val_loss: 0.3853 - val_accuracy: 0.8228\n",
      "Model: \"sequential_30\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " flatten_30 (Flatten)        (1, 50176)                0         \n",
      "                                                                 \n",
      " dense_60 (Dense)            (1, 100)                  5017700   \n",
      "                                                                 \n",
      " dense_61 (Dense)            (1, 1)                    101       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 5,017,801\n",
      "Trainable params: 5,017,801\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "54/54 [==============================] - 0s 3ms/step\n",
      "Accuracy on train set: 88.25%\n",
      "14/14 [==============================] - 0s 3ms/step\n",
      "Accuracy on test set: 82.28%\n",
      "[[154  29]\n",
      " [ 47 199]]\n"
     ]
    }
   ],
   "source": [
    "history = run_binary_mlp(X_train_reshaped, y_train[:,0], X_test_reshaped, y_test[:,0],100,10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "1719/1719 [==============================] - 6s 3ms/step - loss: 0.8306 - accuracy: 0.7673 - val_loss: 0.4436 - val_accuracy: 0.7972\n",
      "Epoch 2/20\n",
      "1719/1719 [==============================] - 5s 3ms/step - loss: 0.4377 - accuracy: 0.8080 - val_loss: 0.4297 - val_accuracy: 0.8065\n",
      "Epoch 3/20\n",
      "1719/1719 [==============================] - 5s 3ms/step - loss: 0.3972 - accuracy: 0.8301 - val_loss: 0.4526 - val_accuracy: 0.7762\n",
      "Epoch 4/20\n",
      "1719/1719 [==============================] - 5s 3ms/step - loss: 0.3785 - accuracy: 0.8336 - val_loss: 1.0049 - val_accuracy: 0.6970\n",
      "Epoch 5/20\n",
      "1719/1719 [==============================] - 5s 3ms/step - loss: 0.3739 - accuracy: 0.8418 - val_loss: 0.4396 - val_accuracy: 0.7995\n",
      "Epoch 6/20\n",
      "1719/1719 [==============================] - 5s 3ms/step - loss: 0.3574 - accuracy: 0.8377 - val_loss: 0.4056 - val_accuracy: 0.8182\n",
      "Epoch 7/20\n",
      "1719/1719 [==============================] - 5s 3ms/step - loss: 0.3583 - accuracy: 0.8412 - val_loss: 0.3860 - val_accuracy: 0.8345\n",
      "Epoch 8/20\n",
      "1719/1719 [==============================] - 5s 3ms/step - loss: 0.3592 - accuracy: 0.8394 - val_loss: 0.4010 - val_accuracy: 0.8159\n",
      "Epoch 9/20\n",
      "1719/1719 [==============================] - 5s 3ms/step - loss: 0.3374 - accuracy: 0.8505 - val_loss: 0.4308 - val_accuracy: 0.7832\n",
      "Epoch 10/20\n",
      "1719/1719 [==============================] - 5s 3ms/step - loss: 0.3367 - accuracy: 0.8540 - val_loss: 0.4167 - val_accuracy: 0.8159\n",
      "Epoch 11/20\n",
      "1719/1719 [==============================] - 5s 3ms/step - loss: 0.3248 - accuracy: 0.8650 - val_loss: 0.5764 - val_accuracy: 0.7692\n",
      "Epoch 12/20\n",
      "1719/1719 [==============================] - 5s 3ms/step - loss: 0.3214 - accuracy: 0.8581 - val_loss: 0.3789 - val_accuracy: 0.8205\n",
      "Epoch 13/20\n",
      "1719/1719 [==============================] - 5s 3ms/step - loss: 0.3075 - accuracy: 0.8726 - val_loss: 0.4118 - val_accuracy: 0.8205\n",
      "Epoch 14/20\n",
      "1719/1719 [==============================] - 5s 3ms/step - loss: 0.3171 - accuracy: 0.8639 - val_loss: 0.4240 - val_accuracy: 0.7855\n",
      "Epoch 15/20\n",
      "1719/1719 [==============================] - 5s 3ms/step - loss: 0.3022 - accuracy: 0.8691 - val_loss: 0.4086 - val_accuracy: 0.8182\n",
      "Epoch 16/20\n",
      "1719/1719 [==============================] - 5s 3ms/step - loss: 0.2922 - accuracy: 0.8697 - val_loss: 0.4037 - val_accuracy: 0.7995\n",
      "Epoch 17/20\n",
      "1719/1719 [==============================] - 5s 3ms/step - loss: 0.2835 - accuracy: 0.8790 - val_loss: 0.4759 - val_accuracy: 0.7552\n",
      "Epoch 18/20\n",
      "1719/1719 [==============================] - 5s 3ms/step - loss: 0.2965 - accuracy: 0.8650 - val_loss: 0.4651 - val_accuracy: 0.8065\n",
      "Epoch 19/20\n",
      "1719/1719 [==============================] - 5s 3ms/step - loss: 0.2746 - accuracy: 0.8807 - val_loss: 0.5219 - val_accuracy: 0.7995\n",
      "Epoch 20/20\n",
      "1719/1719 [==============================] - 5s 3ms/step - loss: 0.2787 - accuracy: 0.8773 - val_loss: 0.3844 - val_accuracy: 0.8368\n",
      "Model: \"sequential_31\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " flatten_31 (Flatten)        (1, 50176)                0         \n",
      "                                                                 \n",
      " dense_62 (Dense)            (1, 100)                  5017700   \n",
      "                                                                 \n",
      " dense_63 (Dense)            (1, 1)                    101       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 5,017,801\n",
      "Trainable params: 5,017,801\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "54/54 [==============================] - 0s 3ms/step\n",
      "Accuracy on train set: 90.23%\n",
      "14/14 [==============================] - 0s 3ms/step\n",
      "Accuracy on test set: 83.68%\n",
      "[[141  42]\n",
      " [ 28 218]]\n"
     ]
    }
   ],
   "source": [
    "history = run_binary_mlp(X_train_reshaped, y_train[:,0], X_test_reshaped, y_test[:,0],100, 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/35\n",
      "1719/1719 [==============================] - 6s 3ms/step - loss: 1.7127 - accuracy: 0.7650 - val_loss: 0.4711 - val_accuracy: 0.7925\n",
      "Epoch 2/35\n",
      "1719/1719 [==============================] - 6s 3ms/step - loss: 0.4651 - accuracy: 0.7813 - val_loss: 0.5084 - val_accuracy: 0.7972\n",
      "Epoch 3/35\n",
      "1719/1719 [==============================] - 6s 3ms/step - loss: 0.4363 - accuracy: 0.8232 - val_loss: 0.4157 - val_accuracy: 0.8089\n",
      "Epoch 4/35\n",
      "1719/1719 [==============================] - 6s 3ms/step - loss: 0.4000 - accuracy: 0.8348 - val_loss: 0.4764 - val_accuracy: 0.7972\n",
      "Epoch 5/35\n",
      "1719/1719 [==============================] - 5s 3ms/step - loss: 0.3973 - accuracy: 0.8360 - val_loss: 0.4223 - val_accuracy: 0.8112\n",
      "Epoch 6/35\n",
      "1719/1719 [==============================] - 5s 3ms/step - loss: 0.3677 - accuracy: 0.8319 - val_loss: 0.4864 - val_accuracy: 0.7156\n",
      "Epoch 7/35\n",
      "1719/1719 [==============================] - 5s 3ms/step - loss: 0.3674 - accuracy: 0.8365 - val_loss: 0.3976 - val_accuracy: 0.8089\n",
      "Epoch 8/35\n",
      "1719/1719 [==============================] - 5s 3ms/step - loss: 0.3479 - accuracy: 0.8482 - val_loss: 0.4089 - val_accuracy: 0.8159\n",
      "Epoch 9/35\n",
      "1719/1719 [==============================] - 5s 3ms/step - loss: 0.3531 - accuracy: 0.8400 - val_loss: 0.4574 - val_accuracy: 0.8112\n",
      "Epoch 10/35\n",
      "1719/1719 [==============================] - 5s 3ms/step - loss: 0.3389 - accuracy: 0.8383 - val_loss: 0.4440 - val_accuracy: 0.7949\n",
      "Epoch 11/35\n",
      "1719/1719 [==============================] - 5s 3ms/step - loss: 0.3349 - accuracy: 0.8360 - val_loss: 0.3932 - val_accuracy: 0.8042\n",
      "Epoch 12/35\n",
      "1719/1719 [==============================] - 5s 3ms/step - loss: 0.3433 - accuracy: 0.8546 - val_loss: 0.3865 - val_accuracy: 0.8112\n",
      "Epoch 13/35\n",
      "1719/1719 [==============================] - 5s 3ms/step - loss: 0.3234 - accuracy: 0.8487 - val_loss: 0.5345 - val_accuracy: 0.8135\n",
      "Epoch 14/35\n",
      "1719/1719 [==============================] - 5s 3ms/step - loss: 0.3254 - accuracy: 0.8534 - val_loss: 0.5701 - val_accuracy: 0.8019\n",
      "Epoch 15/35\n",
      "1719/1719 [==============================] - 5s 3ms/step - loss: 0.3124 - accuracy: 0.8633 - val_loss: 0.4332 - val_accuracy: 0.8205\n",
      "Epoch 16/35\n",
      "1719/1719 [==============================] - 5s 3ms/step - loss: 0.3071 - accuracy: 0.8691 - val_loss: 0.3731 - val_accuracy: 0.8368\n",
      "Epoch 17/35\n",
      "1719/1719 [==============================] - 5s 3ms/step - loss: 0.3037 - accuracy: 0.8674 - val_loss: 0.3800 - val_accuracy: 0.8228\n",
      "Epoch 18/35\n",
      "1719/1719 [==============================] - 5s 3ms/step - loss: 0.2903 - accuracy: 0.8749 - val_loss: 0.3747 - val_accuracy: 0.8322\n",
      "Epoch 19/35\n",
      "1719/1719 [==============================] - 5s 3ms/step - loss: 0.2895 - accuracy: 0.8773 - val_loss: 0.4077 - val_accuracy: 0.8252\n",
      "Epoch 20/35\n",
      "1719/1719 [==============================] - 5s 3ms/step - loss: 0.2853 - accuracy: 0.8819 - val_loss: 0.4474 - val_accuracy: 0.7949\n",
      "Epoch 21/35\n",
      "1719/1719 [==============================] - 5s 3ms/step - loss: 0.2778 - accuracy: 0.8761 - val_loss: 0.3986 - val_accuracy: 0.8182\n",
      "Epoch 22/35\n",
      "1719/1719 [==============================] - 5s 3ms/step - loss: 0.2751 - accuracy: 0.8755 - val_loss: 0.4652 - val_accuracy: 0.8182\n",
      "Epoch 23/35\n",
      "1719/1719 [==============================] - 5s 3ms/step - loss: 0.2752 - accuracy: 0.8784 - val_loss: 0.3843 - val_accuracy: 0.8345\n",
      "Epoch 24/35\n",
      "1719/1719 [==============================] - 5s 3ms/step - loss: 0.2633 - accuracy: 0.8778 - val_loss: 0.4247 - val_accuracy: 0.8298\n",
      "Epoch 25/35\n",
      "1719/1719 [==============================] - 5s 3ms/step - loss: 0.2670 - accuracy: 0.8860 - val_loss: 0.3878 - val_accuracy: 0.8322\n",
      "Epoch 26/35\n",
      "1719/1719 [==============================] - 5s 3ms/step - loss: 0.2543 - accuracy: 0.8848 - val_loss: 0.3863 - val_accuracy: 0.8322\n",
      "Epoch 27/35\n",
      "1719/1719 [==============================] - 5s 3ms/step - loss: 0.2551 - accuracy: 0.8854 - val_loss: 0.3878 - val_accuracy: 0.8298\n",
      "Epoch 28/35\n",
      "1719/1719 [==============================] - 5s 3ms/step - loss: 0.2561 - accuracy: 0.8866 - val_loss: 0.3869 - val_accuracy: 0.8322\n",
      "Epoch 29/35\n",
      "1719/1719 [==============================] - 5s 3ms/step - loss: 0.2456 - accuracy: 0.8877 - val_loss: 0.5054 - val_accuracy: 0.8298\n",
      "Epoch 30/35\n",
      "1719/1719 [==============================] - 5s 3ms/step - loss: 0.2403 - accuracy: 0.8941 - val_loss: 0.4082 - val_accuracy: 0.8368\n",
      "Epoch 31/35\n",
      "1719/1719 [==============================] - 5s 3ms/step - loss: 0.2350 - accuracy: 0.8999 - val_loss: 0.4709 - val_accuracy: 0.8298\n",
      "Epoch 32/35\n",
      "1719/1719 [==============================] - 5s 3ms/step - loss: 0.2286 - accuracy: 0.8999 - val_loss: 0.4365 - val_accuracy: 0.8275\n",
      "Epoch 33/35\n",
      "1719/1719 [==============================] - 5s 3ms/step - loss: 0.2228 - accuracy: 0.9040 - val_loss: 0.3912 - val_accuracy: 0.8159\n",
      "Epoch 34/35\n",
      "1719/1719 [==============================] - 5s 3ms/step - loss: 0.2316 - accuracy: 0.8999 - val_loss: 0.4118 - val_accuracy: 0.8275\n",
      "Epoch 35/35\n",
      "1719/1719 [==============================] - 5s 3ms/step - loss: 0.2190 - accuracy: 0.9046 - val_loss: 0.5321 - val_accuracy: 0.8112\n",
      "Model: \"sequential_32\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " flatten_32 (Flatten)        (1, 50176)                0         \n",
      "                                                                 \n",
      " dense_64 (Dense)            (1, 100)                  5017700   \n",
      "                                                                 \n",
      " dense_65 (Dense)            (1, 1)                    101       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 5,017,801\n",
      "Trainable params: 5,017,801\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "54/54 [==============================] - 0s 3ms/step\n",
      "Accuracy on train set: 87.90%\n",
      "14/14 [==============================] - 0s 3ms/step\n",
      "Accuracy on test set: 81.12%\n",
      "[[163  20]\n",
      " [ 61 185]]\n"
     ]
    }
   ],
   "source": [
    "history = run_binary_mlp(X_train_reshaped, y_train[:,0], X_test_reshaped, y_test[:,0], 100, 35)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Results and Reflection\n",
    "\n",
    "\n",
    "##### Hidden Layers\n",
    "\n",
    "\n",
    "Number of hidden layers | Training acc. | Testing acc\n",
    "\n",
    "1  | 57.36 | 57.34\n",
    "\n",
    "10 | 57.36 | 57.34\n",
    "\n",
    "50 | 57.36 | 57.34\n",
    "\n",
    "100| 86.04 | 81.59\n",
    "\n",
    "\n",
    "##### Epochs\n",
    "\n",
    "\n",
    "Number of epochs | Training acc. | Testing acc\n",
    "\n",
    "1 | 84.06 | 79.02\n",
    "\n",
    "10| 88.25 | 82.28\n",
    "\n",
    "20| 90.23 | 83.68\n",
    "\n",
    "35| 87.90 | 81.12\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Similar to the previous exercises, the first set of cells tesed the optimal number of hidden, and the second set of cells tested the optimal number of epochs. 100 hidden layers yeilded the best accuracy for both training and testing data. Howver, this data set did not follow the epoch-accuracy linear relationship trend we have been seeing in previous tests. This dataset yeilded the highest test and validation accuracy with 20 epochs. Thus, this model yeilds the best accuracy with 100 hidden layers and 20 epochs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 6\n",
    "In the following cell, modify the model itself by adding one additional dense layer and one Dropout layer.  Re-use your hyperparameters from your highest accuracy run in the previous exercise and capture the results in the reflection below.  Answer the questions: \n",
    "\n",
    "1) Did more layers help? \n",
    "\n",
    "2) Did dropout affect overfitting?\n",
    "\n",
    "3) Did the total number of parameters correlate with changes in accuracy across your experiments?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_binary_mlp_modified(X_train, y_train, X_test, y_test, hidden=16, epochs=100):\n",
    "    model = Sequential()\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(hidden, activation='relu'))\n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "    \n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "    \n",
    "    model.compile(loss=\"binary_crossentropy\",\n",
    "              optimizer='adam',\n",
    "              metrics=['accuracy'])\n",
    "    history = model.fit(x=X_train,y=y_train,\n",
    "                        validation_data = (X_test, y_test),\n",
    "                        batch_size=1,epochs=epochs,\n",
    "                        verbose=1)\n",
    "    model.summary()\n",
    "    predicted_probabilities = model.predict(X_train)\n",
    "    predicted_probabilities = np.rint(predicted_probabilities)\n",
    "    acc = 100. * accuracy_score(y_train, predicted_probabilities)\n",
    "    print(\"Accuracy on train set: {:.2f}%\".format(acc))\n",
    "    predicted_probabilities = model.predict(X_test)\n",
    "    predicted_probabilities = np.rint(predicted_probabilities)\n",
    "    acc = 100. * accuracy_score(y_test, predicted_probabilities)\n",
    "    print(\"Accuracy on test set: {:.2f}%\".format(acc))\n",
    "    print(confusion_matrix(y_test, predicted_probabilities))\n",
    "    return history\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "1719/1719 [==============================] - 6s 4ms/step - loss: 0.7136 - accuracy: 0.4828 - val_loss: 0.6853 - val_accuracy: 0.5734\n",
      "Epoch 2/20\n",
      "1719/1719 [==============================] - 6s 4ms/step - loss: 0.6852 - accuracy: 0.5736 - val_loss: 0.6823 - val_accuracy: 0.5734\n",
      "Epoch 3/20\n",
      "1719/1719 [==============================] - 6s 4ms/step - loss: 0.6839 - accuracy: 0.5736 - val_loss: 0.6824 - val_accuracy: 0.5734\n",
      "Epoch 4/20\n",
      "1719/1719 [==============================] - 6s 4ms/step - loss: 0.6823 - accuracy: 0.5736 - val_loss: 0.6823 - val_accuracy: 0.5734\n",
      "Epoch 5/20\n",
      "1719/1719 [==============================] - 6s 4ms/step - loss: 0.6830 - accuracy: 0.5736 - val_loss: 0.6824 - val_accuracy: 0.5734\n",
      "Epoch 6/20\n",
      "1719/1719 [==============================] - 6s 4ms/step - loss: 0.6822 - accuracy: 0.5736 - val_loss: 0.6823 - val_accuracy: 0.5734\n",
      "Epoch 7/20\n",
      "1719/1719 [==============================] - 6s 4ms/step - loss: 0.6827 - accuracy: 0.5736 - val_loss: 0.6824 - val_accuracy: 0.5734\n",
      "Epoch 8/20\n",
      "1719/1719 [==============================] - 6s 3ms/step - loss: 0.6824 - accuracy: 0.5736 - val_loss: 0.6823 - val_accuracy: 0.5734\n",
      "Epoch 9/20\n",
      "1719/1719 [==============================] - 6s 4ms/step - loss: 0.6827 - accuracy: 0.5736 - val_loss: 0.6824 - val_accuracy: 0.5734\n",
      "Epoch 10/20\n",
      "1719/1719 [==============================] - 6s 4ms/step - loss: 0.6835 - accuracy: 0.5736 - val_loss: 0.6823 - val_accuracy: 0.5734\n",
      "Epoch 11/20\n",
      "1719/1719 [==============================] - 6s 4ms/step - loss: 0.6830 - accuracy: 0.5736 - val_loss: 0.6826 - val_accuracy: 0.5734\n",
      "Epoch 12/20\n",
      "1719/1719 [==============================] - 6s 4ms/step - loss: 0.6826 - accuracy: 0.5736 - val_loss: 0.6824 - val_accuracy: 0.5734\n",
      "Epoch 13/20\n",
      "1719/1719 [==============================] - 6s 4ms/step - loss: 0.6831 - accuracy: 0.5736 - val_loss: 0.6823 - val_accuracy: 0.5734\n",
      "Epoch 14/20\n",
      "1719/1719 [==============================] - 6s 4ms/step - loss: 0.6827 - accuracy: 0.5736 - val_loss: 0.6824 - val_accuracy: 0.5734\n",
      "Epoch 15/20\n",
      "1719/1719 [==============================] - 6s 4ms/step - loss: 0.6825 - accuracy: 0.5736 - val_loss: 0.6825 - val_accuracy: 0.5734\n",
      "Epoch 16/20\n",
      "1719/1719 [==============================] - 6s 4ms/step - loss: 0.6827 - accuracy: 0.5736 - val_loss: 0.6824 - val_accuracy: 0.5734\n",
      "Epoch 17/20\n",
      "1719/1719 [==============================] - 6s 4ms/step - loss: 0.6827 - accuracy: 0.5736 - val_loss: 0.6824 - val_accuracy: 0.5734\n",
      "Epoch 18/20\n",
      "1719/1719 [==============================] - 6s 4ms/step - loss: 0.6825 - accuracy: 0.5736 - val_loss: 0.6823 - val_accuracy: 0.5734\n",
      "Epoch 19/20\n",
      "1719/1719 [==============================] - 6s 4ms/step - loss: 0.6829 - accuracy: 0.5736 - val_loss: 0.6824 - val_accuracy: 0.5734\n",
      "Epoch 20/20\n",
      "1719/1719 [==============================] - 6s 4ms/step - loss: 0.6825 - accuracy: 0.5736 - val_loss: 0.6824 - val_accuracy: 0.5734\n",
      "Model: \"sequential_34\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " flatten_34 (Flatten)        (1, 50176)                0         \n",
      "                                                                 \n",
      " dense_69 (Dense)            (1, 100)                  5017700   \n",
      "                                                                 \n",
      " dense_70 (Dense)            (1, 1)                    101       \n",
      "                                                                 \n",
      " dropout_1 (Dropout)         (1, 1)                    0         \n",
      "                                                                 \n",
      " dense_71 (Dense)            (1, 1)                    2         \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 5,017,803\n",
      "Trainable params: 5,017,803\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "54/54 [==============================] - 0s 4ms/step\n",
      "Accuracy on train set: 57.36%\n",
      "14/14 [==============================] - 0s 3ms/step\n",
      "Accuracy on test set: 57.34%\n",
      "[[  0 183]\n",
      " [  0 246]]\n"
     ]
    }
   ],
   "source": [
    "history = run_binary_mlp_modified(X_train_reshaped, y_train[:,0], X_test_reshaped, y_test[:,0],100, 20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reflection\n",
    "1) Did more layers help? \n",
    "\n",
    "*Funny eough, we are met with our reoccurring 57.36 & 57.34 accuracies. However, once again each epoch has unique values. So Adding these addition layers did not help, it was actually detrimental to our model.\n",
    "\n",
    "2) Did dropout affect overfitting?\n",
    "\n",
    "ALthough our models accuracy decreased, unlike the model without the extra layers we did not have any overfitting! \n",
    "\n",
    "3) Did the total number of parameters correlate with changes in accuracy across your experiments?\n",
    "\n",
    "Yes! In this case, although it was a negative change, adding more perameters corelates with accuracy changes. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 7\n",
    "In the following cell, experiment with using at least 3 different activation functions.  Keep other hyperparameters constant for these experiments and just change the activation functions.  In the reflection section below, record your experiments and the resulting accuracy.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_binary_mlp_act(X_train, y_train, X_test, y_test, hidden=16, epochs=100, act = 'relu'):\n",
    "    model = Sequential()\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(hidden, activation=act))\n",
    "    model.add(Dense(1, activation=act))\n",
    "    \n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(Dense(1, activation=act))\n",
    "    \n",
    "    model.compile(loss=\"binary_crossentropy\",\n",
    "              optimizer='adam',\n",
    "              metrics=['accuracy'])\n",
    "    history = model.fit(x=X_train,y=y_train,\n",
    "                        validation_data = (X_test, y_test),\n",
    "                        batch_size=1,epochs=epochs,\n",
    "                        verbose=1)\n",
    "    model.summary()\n",
    "    predicted_probabilities = model.predict(X_train)\n",
    "    predicted_probabilities = np.rint(predicted_probabilities)\n",
    "    acc = 100. * accuracy_score(y_train, predicted_probabilities)\n",
    "    print(\"Accuracy on train set: {:.2f}%\".format(acc))\n",
    "    predicted_probabilities = model.predict(X_test)\n",
    "    predicted_probabilities = np.rint(predicted_probabilities)\n",
    "    acc = 100. * accuracy_score(y_test, predicted_probabilities)\n",
    "    print(\"Accuracy on test set: {:.2f}%\".format(acc))\n",
    "    print(confusion_matrix(y_test, predicted_probabilities))\n",
    "    return history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "1719/1719 [==============================] - 6s 3ms/step - loss: 0.8725 - accuracy: 0.4508 - val_loss: 0.7059 - val_accuracy: 0.4266\n",
      "Epoch 2/20\n",
      "1719/1719 [==============================] - 6s 3ms/step - loss: 0.6939 - accuracy: 0.5189 - val_loss: 0.6823 - val_accuracy: 0.5734\n",
      "Epoch 3/20\n",
      "1719/1719 [==============================] - 6s 3ms/step - loss: 0.6898 - accuracy: 0.5736 - val_loss: 0.6827 - val_accuracy: 0.5734\n",
      "Epoch 4/20\n",
      "1719/1719 [==============================] - 6s 3ms/step - loss: 0.6898 - accuracy: 0.5736 - val_loss: 0.6826 - val_accuracy: 0.5734\n",
      "Epoch 5/20\n",
      "1719/1719 [==============================] - 6s 3ms/step - loss: 0.6864 - accuracy: 0.5736 - val_loss: 0.6825 - val_accuracy: 0.5734\n",
      "Epoch 6/20\n",
      "1719/1719 [==============================] - 6s 3ms/step - loss: 0.6913 - accuracy: 0.5736 - val_loss: 0.6829 - val_accuracy: 0.5734\n",
      "Epoch 7/20\n",
      "1719/1719 [==============================] - 6s 3ms/step - loss: 0.6888 - accuracy: 0.5736 - val_loss: 0.6826 - val_accuracy: 0.5734\n",
      "Epoch 8/20\n",
      "1719/1719 [==============================] - 6s 3ms/step - loss: 0.6866 - accuracy: 0.5736 - val_loss: 0.6824 - val_accuracy: 0.5734\n",
      "Epoch 9/20\n",
      "1719/1719 [==============================] - 6s 3ms/step - loss: 0.6847 - accuracy: 0.5736 - val_loss: 0.6824 - val_accuracy: 0.5734\n",
      "Epoch 10/20\n",
      "1719/1719 [==============================] - 6s 3ms/step - loss: 0.6829 - accuracy: 0.5736 - val_loss: 0.6823 - val_accuracy: 0.5734\n",
      "Epoch 11/20\n",
      "1719/1719 [==============================] - 6s 3ms/step - loss: 0.6857 - accuracy: 0.5736 - val_loss: 0.6824 - val_accuracy: 0.5734\n",
      "Epoch 12/20\n",
      "1719/1719 [==============================] - 6s 3ms/step - loss: 0.6837 - accuracy: 0.5736 - val_loss: 0.6825 - val_accuracy: 0.5734\n",
      "Epoch 13/20\n",
      "1719/1719 [==============================] - 6s 3ms/step - loss: 0.6819 - accuracy: 0.5736 - val_loss: 0.6823 - val_accuracy: 0.5734\n",
      "Epoch 14/20\n",
      "1719/1719 [==============================] - 6s 3ms/step - loss: 0.6844 - accuracy: 0.5736 - val_loss: 0.6828 - val_accuracy: 0.5734\n",
      "Epoch 15/20\n",
      "1719/1719 [==============================] - 6s 3ms/step - loss: 0.6830 - accuracy: 0.5736 - val_loss: 0.6824 - val_accuracy: 0.5734\n",
      "Epoch 16/20\n",
      "1719/1719 [==============================] - 6s 3ms/step - loss: 0.6842 - accuracy: 0.5736 - val_loss: 0.6824 - val_accuracy: 0.5734\n",
      "Epoch 17/20\n",
      "1719/1719 [==============================] - 6s 3ms/step - loss: 0.6821 - accuracy: 0.5736 - val_loss: 0.6823 - val_accuracy: 0.5734\n",
      "Epoch 18/20\n",
      "1719/1719 [==============================] - 6s 3ms/step - loss: 0.6829 - accuracy: 0.5736 - val_loss: 0.6823 - val_accuracy: 0.5734\n",
      "Epoch 19/20\n",
      "1719/1719 [==============================] - 6s 3ms/step - loss: 0.6841 - accuracy: 0.5736 - val_loss: 0.6824 - val_accuracy: 0.5734\n",
      "Epoch 20/20\n",
      "1719/1719 [==============================] - 6s 3ms/step - loss: 0.6829 - accuracy: 0.5736 - val_loss: 0.6823 - val_accuracy: 0.5734\n",
      "Model: \"sequential_35\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " flatten_35 (Flatten)        (1, 50176)                0         \n",
      "                                                                 \n",
      " dense_72 (Dense)            (1, 100)                  5017700   \n",
      "                                                                 \n",
      " dense_73 (Dense)            (1, 1)                    101       \n",
      "                                                                 \n",
      " dropout_2 (Dropout)         (1, 1)                    0         \n",
      "                                                                 \n",
      " dense_74 (Dense)            (1, 1)                    2         \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 5,017,803\n",
      "Trainable params: 5,017,803\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "54/54 [==============================] - 0s 3ms/step\n",
      "Accuracy on train set: 57.36%\n",
      "14/14 [==============================] - 0s 3ms/step\n",
      "Accuracy on test set: 57.34%\n",
      "[[  0 183]\n",
      " [  0 246]]\n"
     ]
    }
   ],
   "source": [
    "history = run_binary_mlp_act(X_train_reshaped, y_train[:,0], X_test_reshaped, y_test[:,0],100, 20, 'linear')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "1719/1719 [==============================] - 7s 4ms/step - loss: 0.6878 - accuracy: 0.5730 - val_loss: 0.6844 - val_accuracy: 0.5734\n",
      "Epoch 2/20\n",
      "1719/1719 [==============================] - 6s 3ms/step - loss: 0.6835 - accuracy: 0.5736 - val_loss: 0.6828 - val_accuracy: 0.5734\n",
      "Epoch 3/20\n",
      "1719/1719 [==============================] - 6s 3ms/step - loss: 0.6828 - accuracy: 0.5736 - val_loss: 0.6824 - val_accuracy: 0.5734\n",
      "Epoch 4/20\n",
      "1719/1719 [==============================] - 6s 3ms/step - loss: 0.6825 - accuracy: 0.5736 - val_loss: 0.6823 - val_accuracy: 0.5734\n",
      "Epoch 5/20\n",
      "1719/1719 [==============================] - 6s 3ms/step - loss: 0.6825 - accuracy: 0.5736 - val_loss: 0.6823 - val_accuracy: 0.5734\n",
      "Epoch 6/20\n",
      "1719/1719 [==============================] - 6s 3ms/step - loss: 0.6825 - accuracy: 0.5736 - val_loss: 0.6823 - val_accuracy: 0.5734\n",
      "Epoch 7/20\n",
      "1719/1719 [==============================] - 6s 3ms/step - loss: 0.6825 - accuracy: 0.5736 - val_loss: 0.6823 - val_accuracy: 0.5734\n",
      "Epoch 8/20\n",
      "1719/1719 [==============================] - 6s 3ms/step - loss: 0.6825 - accuracy: 0.5736 - val_loss: 0.6823 - val_accuracy: 0.5734\n",
      "Epoch 9/20\n",
      "1719/1719 [==============================] - 6s 3ms/step - loss: 0.6825 - accuracy: 0.5736 - val_loss: 0.6823 - val_accuracy: 0.5734\n",
      "Epoch 10/20\n",
      "1719/1719 [==============================] - 6s 3ms/step - loss: 0.6825 - accuracy: 0.5736 - val_loss: 0.6823 - val_accuracy: 0.5734\n",
      "Epoch 11/20\n",
      "1719/1719 [==============================] - 6s 3ms/step - loss: 0.6826 - accuracy: 0.5736 - val_loss: 0.6823 - val_accuracy: 0.5734\n",
      "Epoch 12/20\n",
      "1719/1719 [==============================] - 6s 3ms/step - loss: 0.6825 - accuracy: 0.5736 - val_loss: 0.6823 - val_accuracy: 0.5734\n",
      "Epoch 13/20\n",
      "1719/1719 [==============================] - 6s 3ms/step - loss: 0.6825 - accuracy: 0.5736 - val_loss: 0.6823 - val_accuracy: 0.5734\n",
      "Epoch 14/20\n",
      "1719/1719 [==============================] - 6s 3ms/step - loss: 0.6825 - accuracy: 0.5736 - val_loss: 0.6823 - val_accuracy: 0.5734\n",
      "Epoch 15/20\n",
      "1719/1719 [==============================] - 6s 3ms/step - loss: 0.6825 - accuracy: 0.5736 - val_loss: 0.6823 - val_accuracy: 0.5734\n",
      "Epoch 16/20\n",
      "1719/1719 [==============================] - 6s 3ms/step - loss: 0.6825 - accuracy: 0.5736 - val_loss: 0.6823 - val_accuracy: 0.5734\n",
      "Epoch 17/20\n",
      "1719/1719 [==============================] - 6s 3ms/step - loss: 0.6825 - accuracy: 0.5736 - val_loss: 0.6823 - val_accuracy: 0.5734\n",
      "Epoch 18/20\n",
      "1719/1719 [==============================] - 6s 3ms/step - loss: 0.6825 - accuracy: 0.5736 - val_loss: 0.6823 - val_accuracy: 0.5734\n",
      "Epoch 19/20\n",
      "1719/1719 [==============================] - 6s 3ms/step - loss: 0.6825 - accuracy: 0.5736 - val_loss: 0.6823 - val_accuracy: 0.5734\n",
      "Epoch 20/20\n",
      "1719/1719 [==============================] - 6s 3ms/step - loss: 0.6825 - accuracy: 0.5736 - val_loss: 0.6823 - val_accuracy: 0.5734\n",
      "Model: \"sequential_36\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " flatten_36 (Flatten)        (1, 50176)                0         \n",
      "                                                                 \n",
      " dense_75 (Dense)            (1, 100)                  5017700   \n",
      "                                                                 \n",
      " dense_76 (Dense)            (1, 1)                    101       \n",
      "                                                                 \n",
      " dropout_3 (Dropout)         (1, 1)                    0         \n",
      "                                                                 \n",
      " dense_77 (Dense)            (1, 1)                    2         \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 5,017,803\n",
      "Trainable params: 5,017,803\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "54/54 [==============================] - 0s 3ms/step\n",
      "Accuracy on train set: 57.36%\n",
      "14/14 [==============================] - 0s 3ms/step\n",
      "Accuracy on test set: 57.34%\n",
      "[[  0 183]\n",
      " [  0 246]]\n"
     ]
    }
   ],
   "source": [
    "history = run_binary_mlp_act(X_train_reshaped, y_train[:,0], X_test_reshaped, y_test[:,0],100, 20, 'sigmoid')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "1719/1719 [==============================] - 6s 4ms/step - loss: 0.6877 - accuracy: 0.5730 - val_loss: 0.6841 - val_accuracy: 0.5734\n",
      "Epoch 2/20\n",
      "1719/1719 [==============================] - 6s 3ms/step - loss: 0.6834 - accuracy: 0.5736 - val_loss: 0.6828 - val_accuracy: 0.5734\n",
      "Epoch 3/20\n",
      "1719/1719 [==============================] - 6s 3ms/step - loss: 0.6826 - accuracy: 0.5736 - val_loss: 0.6823 - val_accuracy: 0.5734\n",
      "Epoch 4/20\n",
      "1719/1719 [==============================] - 6s 3ms/step - loss: 0.6826 - accuracy: 0.5736 - val_loss: 0.6824 - val_accuracy: 0.5734\n",
      "Epoch 5/20\n",
      "1719/1719 [==============================] - 6s 3ms/step - loss: 0.6825 - accuracy: 0.5736 - val_loss: 0.6823 - val_accuracy: 0.5734\n",
      "Epoch 6/20\n",
      "1719/1719 [==============================] - 6s 3ms/step - loss: 0.6825 - accuracy: 0.5736 - val_loss: 0.6823 - val_accuracy: 0.5734\n",
      "Epoch 7/20\n",
      "1719/1719 [==============================] - 6s 3ms/step - loss: 0.6825 - accuracy: 0.5736 - val_loss: 0.6823 - val_accuracy: 0.5734\n",
      "Epoch 8/20\n",
      "1719/1719 [==============================] - 6s 3ms/step - loss: 0.6825 - accuracy: 0.5736 - val_loss: 0.6823 - val_accuracy: 0.5734\n",
      "Epoch 9/20\n",
      "1719/1719 [==============================] - 6s 3ms/step - loss: 0.6825 - accuracy: 0.5736 - val_loss: 0.6823 - val_accuracy: 0.5734\n",
      "Epoch 10/20\n",
      "1719/1719 [==============================] - 6s 3ms/step - loss: 0.6825 - accuracy: 0.5736 - val_loss: 0.6823 - val_accuracy: 0.5734\n",
      "Epoch 11/20\n",
      "1719/1719 [==============================] - 6s 3ms/step - loss: 0.6824 - accuracy: 0.5736 - val_loss: 0.6823 - val_accuracy: 0.5734\n",
      "Epoch 12/20\n",
      "1719/1719 [==============================] - 6s 3ms/step - loss: 0.6825 - accuracy: 0.5736 - val_loss: 0.6823 - val_accuracy: 0.5734\n",
      "Epoch 13/20\n",
      "1719/1719 [==============================] - 6s 3ms/step - loss: 0.6825 - accuracy: 0.5736 - val_loss: 0.6823 - val_accuracy: 0.5734\n",
      "Epoch 14/20\n",
      "1719/1719 [==============================] - 6s 3ms/step - loss: 0.6825 - accuracy: 0.5736 - val_loss: 0.6823 - val_accuracy: 0.5734\n",
      "Epoch 15/20\n",
      "1719/1719 [==============================] - 6s 3ms/step - loss: 0.6825 - accuracy: 0.5736 - val_loss: 0.6823 - val_accuracy: 0.5734\n",
      "Epoch 16/20\n",
      "1719/1719 [==============================] - 6s 3ms/step - loss: 0.6825 - accuracy: 0.5736 - val_loss: 0.6823 - val_accuracy: 0.5734\n",
      "Epoch 17/20\n",
      "1719/1719 [==============================] - 6s 3ms/step - loss: 0.6824 - accuracy: 0.5736 - val_loss: 0.6823 - val_accuracy: 0.5734\n",
      "Epoch 18/20\n",
      "1719/1719 [==============================] - 6s 3ms/step - loss: 0.6824 - accuracy: 0.5736 - val_loss: 0.6823 - val_accuracy: 0.5734\n",
      "Epoch 19/20\n",
      "1719/1719 [==============================] - 6s 3ms/step - loss: 0.6825 - accuracy: 0.5736 - val_loss: 0.6823 - val_accuracy: 0.5734\n",
      "Epoch 20/20\n",
      "1719/1719 [==============================] - 6s 3ms/step - loss: 0.6825 - accuracy: 0.5736 - val_loss: 0.6823 - val_accuracy: 0.5734\n",
      "Model: \"sequential_37\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " flatten_37 (Flatten)        (1, 50176)                0         \n",
      "                                                                 \n",
      " dense_78 (Dense)            (1, 100)                  5017700   \n",
      "                                                                 \n",
      " dense_79 (Dense)            (1, 1)                    101       \n",
      "                                                                 \n",
      " dropout_4 (Dropout)         (1, 1)                    0         \n",
      "                                                                 \n",
      " dense_80 (Dense)            (1, 1)                    2         \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 5,017,803\n",
      "Trainable params: 5,017,803\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "54/54 [==============================] - 0s 3ms/step\n",
      "Accuracy on train set: 57.36%\n",
      "14/14 [==============================] - 0s 3ms/step\n",
      "Accuracy on test set: 57.34%\n",
      "[[  0 183]\n",
      " [  0 246]]\n"
     ]
    }
   ],
   "source": [
    "history = run_binary_mlp_act(X_train_reshaped, y_train[:,0], X_test_reshaped, y_test[:,0],100, 20, 'tanh')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reflection\n",
    "Testing the original model with linear, sigmoid, and tanh activations have no differnece according to these results above. However, note we are visited by the reoccurring accuracies (57.36, 57.34)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CNNs\n",
    "One of the limitations of dense networks is that they don't inherently take advange of spatially-associated information.  Using convolutional layers can enable the model to be able to learn more complicated features more efficiently since not all the layers are fully connected.  \n",
    "\n",
    "We start by creating a new generator for our CNN data to isolate it from previous experiments.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 1719 images belonging to 2 classes.\n",
      "Found 429 images belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "datagen_cnn = ImageDataGenerator(\n",
    "            validation_split=0.2,\n",
    "            rescale=1./255, # to bring the image range from 0..255 to 0..1\n",
    "            rotation_range=0.01,  # randomly rotate images in the range (degrees, 0 to 180)\n",
    "            zoom_range = 0.01, # randomly zoom image \n",
    "            width_shift_range=0.01,  # randomly shift images horizontally (fraction of total width)\n",
    "            height_shift_range=0.01,  # randomly shift images vertically (fraction of total height)\n",
    "            horizontal_flip=False,  # randomly flip images\n",
    "            vertical_flip=False) # randomly flip images\n",
    "\n",
    "# Do not modify the generator parameters unless you are making significant model changes that necessitate it\n",
    "train_it_cnn = datagen_cnn.flow_from_directory( '/data/cs2300/pistachio/', \n",
    "                                           target_size=(224,224), \n",
    "                                           color_mode='grayscale', \n",
    "                                           batch_size=32,\n",
    "                                           class_mode=\"binary\",\n",
    "                                           shuffle=True,\n",
    "                                           subset='training')\n",
    "valid_it_cnn = datagen.flow_from_directory( '/data/cs2300/pistachio/', \n",
    "                                           target_size=(224,224), \n",
    "                                           color_mode='grayscale', \n",
    "                                           shuffle=True,\n",
    "                                           batch_size=1,\n",
    "                                           class_mode=\"binary\",\n",
    "                                           subset='validation')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we create a function to define and run our CNN model.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_binary_cnn(train_it, valid_it, cnn_epochs=10, deepness=1):\n",
    "    '''This function takes a training and validation generator, builds and trains a model\n",
    "    number of epochs is passed in as well as the deepness.\n",
    "    deepness changes the structure of the model by adding layers for larger numbers\n",
    "    '''\n",
    "    cnn_model = Sequential()\n",
    "    cnn_model.add(Conv2D(32, kernel_size=(3, 3),\n",
    "                     activation='relu',\n",
    "                     input_shape=(224,224,1)))\n",
    "    cnn_model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "    cnn_model.add(Conv2D(64, (3, 3), activation='relu'))\n",
    "    cnn_model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "    \n",
    "    cnn_model.add(Dropout(0.6))\n",
    "    \n",
    "    if(deepness > 1):\n",
    "        cnn_model.add(Conv2D(128, (3, 3), activation='relu'))\n",
    "        cnn_model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "    if(deepness > 2):\n",
    "        cnn_model.add(Conv2D(256, (3, 3), activation='relu'))\n",
    "        cnn_model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "    cnn_model.add(Flatten())\n",
    "    if(deepness > 2):\n",
    "        cnn_model.add(Dense(512, activation='relu'))\n",
    "        cnn_model.add(Dropout(0.2))\n",
    "    if(deepness > 1):\n",
    "        cnn_model.add(Dense(256, activation='relu'))\n",
    "    cnn_model.add(Dense(128, activation='relu'))\n",
    "    cnn_model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "    cnn_model.compile(loss=\"binary_crossentropy\",\n",
    "                  optimizer='adam',\n",
    "                  metrics=['accuracy'])\n",
    "    history_cnn = cnn_model.fit(train_it_cnn,\n",
    "                  validation_data=valid_it_cnn,\n",
    "                  steps_per_epoch=train_it_cnn.samples/train_it_cnn.batch_size,\n",
    "                  validation_steps=valid_it_cnn.samples/valid_it_cnn.batch_size,\n",
    "                  epochs=cnn_epochs)\n",
    "    return history_cnn\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's run the model and see how it does..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/35\n",
      "53/53 [==============================] - 10s 184ms/step - loss: 0.5448 - accuracy: 0.7539 - val_loss: 0.5222 - val_accuracy: 0.7529\n",
      "Epoch 2/35\n",
      "53/53 [==============================] - 10s 184ms/step - loss: 0.3907 - accuracy: 0.8237 - val_loss: 0.4404 - val_accuracy: 0.7995\n",
      "Epoch 3/35\n",
      "53/53 [==============================] - 10s 181ms/step - loss: 0.3714 - accuracy: 0.8383 - val_loss: 0.4159 - val_accuracy: 0.8135\n",
      "Epoch 4/35\n",
      "53/53 [==============================] - 10s 181ms/step - loss: 0.3534 - accuracy: 0.8418 - val_loss: 0.4125 - val_accuracy: 0.8345\n",
      "Epoch 5/35\n",
      "53/53 [==============================] - 10s 184ms/step - loss: 0.3324 - accuracy: 0.8522 - val_loss: 0.3762 - val_accuracy: 0.8159\n",
      "Epoch 6/35\n",
      "53/53 [==============================] - 10s 183ms/step - loss: 0.3032 - accuracy: 0.8691 - val_loss: 0.3860 - val_accuracy: 0.8205\n",
      "Epoch 7/35\n",
      "53/53 [==============================] - 10s 183ms/step - loss: 0.2926 - accuracy: 0.8767 - val_loss: 0.3535 - val_accuracy: 0.8531\n",
      "Epoch 8/35\n",
      "53/53 [==============================] - 10s 181ms/step - loss: 0.2625 - accuracy: 0.8848 - val_loss: 0.3728 - val_accuracy: 0.8205\n",
      "Epoch 9/35\n",
      "53/53 [==============================] - 10s 181ms/step - loss: 0.2535 - accuracy: 0.8930 - val_loss: 0.3293 - val_accuracy: 0.8765\n",
      "Epoch 10/35\n",
      "53/53 [==============================] - 10s 180ms/step - loss: 0.2474 - accuracy: 0.8918 - val_loss: 0.3090 - val_accuracy: 0.8765\n",
      "Epoch 11/35\n",
      "53/53 [==============================] - 10s 187ms/step - loss: 0.2330 - accuracy: 0.9005 - val_loss: 0.3020 - val_accuracy: 0.8811\n",
      "Epoch 12/35\n",
      "53/53 [==============================] - 10s 186ms/step - loss: 0.2139 - accuracy: 0.9081 - val_loss: 0.3287 - val_accuracy: 0.8765\n",
      "Epoch 13/35\n",
      "53/53 [==============================] - 10s 185ms/step - loss: 0.1987 - accuracy: 0.9209 - val_loss: 0.3330 - val_accuracy: 0.8765\n",
      "Epoch 14/35\n",
      "53/53 [==============================] - 10s 183ms/step - loss: 0.1653 - accuracy: 0.9314 - val_loss: 0.3669 - val_accuracy: 0.8741\n",
      "Epoch 15/35\n",
      "53/53 [==============================] - 10s 182ms/step - loss: 0.1824 - accuracy: 0.9267 - val_loss: 0.3036 - val_accuracy: 0.8718\n",
      "Epoch 16/35\n",
      "53/53 [==============================] - 10s 181ms/step - loss: 0.1514 - accuracy: 0.9442 - val_loss: 0.3327 - val_accuracy: 0.8951\n",
      "Epoch 17/35\n",
      "53/53 [==============================] - 10s 181ms/step - loss: 0.1366 - accuracy: 0.9488 - val_loss: 0.2801 - val_accuracy: 0.8881\n",
      "Epoch 18/35\n",
      "53/53 [==============================] - 10s 182ms/step - loss: 0.1406 - accuracy: 0.9459 - val_loss: 0.3340 - val_accuracy: 0.8811\n",
      "Epoch 19/35\n",
      "53/53 [==============================] - 10s 184ms/step - loss: 0.1191 - accuracy: 0.9593 - val_loss: 0.3424 - val_accuracy: 0.8718\n",
      "Epoch 20/35\n",
      "53/53 [==============================] - 10s 185ms/step - loss: 0.0942 - accuracy: 0.9634 - val_loss: 0.3066 - val_accuracy: 0.8951\n",
      "Epoch 21/35\n",
      "53/53 [==============================] - 10s 183ms/step - loss: 0.1054 - accuracy: 0.9616 - val_loss: 0.2678 - val_accuracy: 0.9044\n",
      "Epoch 22/35\n",
      "53/53 [==============================] - 10s 186ms/step - loss: 0.0824 - accuracy: 0.9697 - val_loss: 0.2283 - val_accuracy: 0.9324\n",
      "Epoch 23/35\n",
      "53/53 [==============================] - 10s 184ms/step - loss: 0.0859 - accuracy: 0.9639 - val_loss: 0.3492 - val_accuracy: 0.8765\n",
      "Epoch 24/35\n",
      "53/53 [==============================] - 10s 185ms/step - loss: 0.0783 - accuracy: 0.9732 - val_loss: 0.3499 - val_accuracy: 0.9068\n",
      "Epoch 25/35\n",
      "53/53 [==============================] - 10s 184ms/step - loss: 0.0659 - accuracy: 0.9738 - val_loss: 0.2350 - val_accuracy: 0.9184\n",
      "Epoch 26/35\n",
      "53/53 [==============================] - 10s 184ms/step - loss: 0.0627 - accuracy: 0.9767 - val_loss: 0.2316 - val_accuracy: 0.9347\n",
      "Epoch 27/35\n",
      "53/53 [==============================] - 10s 186ms/step - loss: 0.0629 - accuracy: 0.9767 - val_loss: 0.2367 - val_accuracy: 0.9417\n",
      "Epoch 28/35\n",
      "53/53 [==============================] - 10s 184ms/step - loss: 0.0484 - accuracy: 0.9831 - val_loss: 0.2862 - val_accuracy: 0.9301\n",
      "Epoch 29/35\n",
      "53/53 [==============================] - 10s 182ms/step - loss: 0.0463 - accuracy: 0.9884 - val_loss: 0.2932 - val_accuracy: 0.9301\n",
      "Epoch 30/35\n",
      "53/53 [==============================] - 10s 185ms/step - loss: 0.0397 - accuracy: 0.9855 - val_loss: 0.4051 - val_accuracy: 0.8998\n",
      "Epoch 31/35\n",
      "53/53 [==============================] - 10s 181ms/step - loss: 0.0377 - accuracy: 0.9866 - val_loss: 0.3462 - val_accuracy: 0.9254\n",
      "Epoch 32/35\n",
      "53/53 [==============================] - 10s 185ms/step - loss: 0.0427 - accuracy: 0.9866 - val_loss: 0.3522 - val_accuracy: 0.9184\n",
      "Epoch 33/35\n",
      "53/53 [==============================] - 10s 184ms/step - loss: 0.0529 - accuracy: 0.9802 - val_loss: 0.3104 - val_accuracy: 0.9347\n",
      "Epoch 34/35\n",
      "53/53 [==============================] - 10s 176ms/step - loss: 0.0576 - accuracy: 0.9808 - val_loss: 0.3009 - val_accuracy: 0.9184\n",
      "Epoch 35/35\n",
      "53/53 [==============================] - 10s 187ms/step - loss: 0.0541 - accuracy: 0.9796 - val_loss: 0.2551 - val_accuracy: 0.9254\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYgAAAEWCAYAAAB8LwAVAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjUuMCwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8/fFQqAAAACXBIWXMAAAsTAAALEwEAmpwYAABHQ0lEQVR4nO3dd3iUZfbw8e9JISEkhJJA6B0hdAhFioIoIkqxgm3Furr21f0t67p2V99ddW1Y166AgAVk7QoCUiT03kILEEgoaaTnfv+4n+AQJsmEZDKT5HyuK1dmnjLPyShz5rnLucUYg1JKKVVcgK8DUEop5Z80QSillHJLE4RSSim3NEEopZRySxOEUkoptzRBKKWUcksThFKAiLwvIk95eOxuETnf2zEp5WuaIJRSSrmlCUKpGkREgnwdg6o5NEGoasNp2vmLiKwTkUwReUdEmorINyKSLiI/ikhDl+PHichGETkuIgtEpKvLvj4isso571MgtNi1LhGRNc65S0Skp4cxXiwiq0UkTUT2ichjxfYPdV7vuLN/srO9rog8LyJ7RCRVRBY724aLSKKb9+F85/FjIjJbRD4WkTRgsogMEJGlzjUOisirIlLH5fxuIvKDiBwVkUMi8pCIxIjICRFp7HJcXxFJFpFgT/52VfNoglDVzeXABUBnYCzwDfAQEI39//keABHpDEwH7nP2fQ18JSJ1nA/LL4GPgEbALOd1cc7tA7wL/BFoDLwJzBWREA/iywT+ADQALgbuEJEJzuu2ceJ9xYmpN7DGOe85oB8w2Inp/4BCD9+T8cBs55qfAAXA/UAUcDYwEviTE0ME8CPwLdAc6Aj8ZIxJAhYAV7m87vXADGNMnodxqBpGE4Sqbl4xxhwyxuwHFgHLjTGrjTHZwBdAH+e4icD/jDE/OB9wzwF1sR/Ag4Bg4EVjTJ4xZjawwuUatwFvGmOWG2MKjDEfADnOeaUyxiwwxqw3xhQaY9Zhk9S5zu5rgB+NMdOd6x4xxqwRkQDgJuBeY8x+55pLjDE5Hr4nS40xXzrXzDLGrDTGLDPG5BtjdmMTXFEMlwBJxpjnjTHZxph0Y8xyZ98HwHUAIhIIXI1NoqqW0gShqptDLo+z3DwPdx43B/YU7TDGFAL7gBbOvv3m1EqVe1wetwEecJpojovIcaCVc16pRGSgiMx3mmZSgdux3+RxXmOnm9OisE1c7vZ5Yl+xGDqLyDwRSXKanf7pQQwAc4BYEWmHvUtLNcb8doYxqRpAE4SqqQ5gP+gBEBHBfjjuBw4CLZxtRVq7PN4HPG2MaeDyE2aMme7BdacBc4FWxphI4A2g6Dr7gA5uzkkBskvYlwmEufwdgdjmKVfFSzK/DmwBOhlj6mOb4FxjaO8ucOcubCb2LuJ69O6h1tMEoWqqmcDFIjLS6WR9ANtMtARYCuQD94hIsIhcBgxwOfdt4HbnbkBEpJ7T+RzhwXUjgKPGmGwRGYBtViryCXC+iFwlIkEi0lhEejt3N+8CL4hIcxEJFJGznT6PbUCoc/1g4GGgrL6QCCANyBCRLsAdLvvmAc1E5D4RCRGRCBEZ6LL/Q2AyMA5NELWeJghVIxljtmK/Cb+C/YY+FhhrjMk1xuQCl2E/CI9i+ys+dzk3HrgVeBU4BuxwjvXEn4AnRCQdeASbqIpedy8wBpusjmI7qHs5ux8E1mP7Qo4C/w8IMMakOq/5X+zdTyZwyqgmNx7EJqZ0bLL71CWGdGzz0VggCdgOjHDZ/yu2c3yVMca12U3VQqILBimlXInIz8A0Y8x/fR2L8i1NEEqpk0SkP/ADtg8l3dfxKN/yWhOTiLwrIodFZEMJ+0VEXhaRHWInPvV12XeDiGx3fm7wVoxKqd+JyAfYORL3aXJQ4MU7CBE5B8gAPjTGdHezfwxwN7ZNdiDwkjFmoIg0AuKBOOzojJVAP2PMMa8EqpRSyi2v3UEYYxZiO9tKMh6bPIwxZhnQQESaARcCPxhjjjpJ4QdgtLfiVEop5Z4vC3u14NQJPonOtpK2n0ZEbsPOeqVevXr9unTp4p1IlVKqhlq5cmWKMab43BrAtwmiwowxbwFvAcTFxZn4+HgfR6SUUtWLiJQ4nNmX8yD2Y2e2FmnpbCtpu1JKqSrkywQxF/iDM5ppELbuy0HgO2CUiDQUW7p5lLNNKaVUFfJaE5OITAeGA1FOPftHsRU0Mca8gS2/PAY7S/UEcKOz76iIPMnv1TWfMMaU1tmtlFLKC7yWIIwxV5ex3wB3lrDvXWxtmgrJy8sjMTGR7Ozsir6UcoSGhtKyZUuCg3UNGaVqumrdSV2WxMREIiIiaNu2LacW7lRnwhjDkSNHSExMpF27dr4ORynlZTW6WF92djaNGzfW5FBJRITGjRvrHZlStUSNThCAJodKpu+nUrVHjU8QSikFsGF/Ku8s3sXqvccoKNQipZ6o0X0Q/uD48eNMmzaNP/3pT+U6b8yYMUybNo0GDRp4JzClaok1+47zyk/b+WnL4ZPbIusGM6RjY4Z1imZoxyhaNQor5RVOZYwhK6+gzONCggIJDKjed9yaILzs+PHjvPbaa6cliPz8fIKCSn77v/76a2+HplSNFr/7KC//vIOF25JpEBbMg6M6M65XC9YkHmfx9mQWbkvh6/VJALSLqsewTlEM6xRNbPP6HMnI4WBqNkmp2c7vLPs7zW7LyS8s8/pR4SH8dfRZXN63JQHVNFFogvCyKVOmsHPnTnr37k1wcDChoaE0bNiQLVu2sG3bNiZMmMC+ffvIzs7m3nvv5bbbbgOgbdu2xMfHk5GRwUUXXcTQoUNZsmQJLVq0YM6cOdStW9fHf5lS/mlZwhFe/mk7S3YeoXG9Oky5qAvXDWpDeIj9uGvdOIxxvZpjjGFncgYLt6WwaHsys+IT+XDp6VUn6gQG0DQyhGb169KrZQNGdwulYb06lPaRb4DvNybxl9nrmLFiH0+M70a35pHe+YO9qMYsGOSuFtPmzZvp2rUrAI9/tZFNB9Iq9Zqxzevz6NhupR6ze/duLrnkEjZs2MCCBQu4+OKL2bBhw8lhokePHqVRo0ZkZWXRv39/fvnlFxo3bnxKgujYsSPx8fH07t2bq666inHjxnHddddV6t9SHq7vq1L+wBjDkp1HeOmn7fy26yjRESH88Zz2XDOwNWF1PPsenJNfwMo9x9iVkkmTiFCaRYYSExlKo7A6Z3QHUFho+GxVIs9+s4VjJ3L5w9ltuf+CzkTW9a85RCKy0hgT526f3kFUsQEDBpwyh+Dll1/miy++AGDfvn1s376dxo0bn3JOu3bt6N27NwD9+vVj9+7dVRWuUn5vV0omj83dyC/bkompH8pjY2OZNKA1ocGB5XqdkKBABneIYnCHqEqJKyBAuDKuFaNiY3jhh618uHQ389YdYMpFXbmsT4tq0exUaxJEWd/0q0q9evVOPl6wYAE//vgjS5cuJSwsjOHDh7udYxASEnLycWBgIFlZWVUSq1L+LCu3gKnzd/DWwgRCggL4xyWxXDeoNSFB5UsM3hYZFszj47tzZVwrHpmzgQdnrWXGb3t5Ynx3YpvXd3tOXkEhGdn5ZOTkExAgNIkIITiw6ged1poE4SsRERGkp7tfvTE1NZWGDRsSFhbGli1bWLZsWRVHp1T1Y4zhu42HeHLeJvYfz+KyPi2YMqYLTSJCfR1aqbq3iGT27YNPNjtd8soihnSMsskgJ/9kQkjPzj+tE1wEosNDTjZ7NYus6/wOpWn9UFo0qFuukVie0gThZY0bN2bIkCF0796dunXr0rRp05P7Ro8ezRtvvEHXrl0566yzGDRokA8jVcr/uTYndYmJ4NPbBjGwfeOyT/QTrs1O//lxG7/tOkp4aBBNI0LpEB1EeEgQ4aFBRIQUPQ4mr6CQpKIRVWnZ7ErJZMnOI6Rn55983Z4tI5l719BKj7fWdFKryqPvq6pqxZuT7r+gM384uw1BPmh28ReZOfknh90KMLjjmfWdaCe1Usqv5BUUsnbfcRZuT/FoZnNCciZJadnVpjmpKtQLCaJDdDgdosO9dg1NEEoprzPGsOfICRZtT2bh9hSW7jxiO2AFusTUp15I6R3LXZpF8NKk3tWqOakm0AShlKoUOfkFp3S0ZuTkczg9h6U7j7BoezKJx+zou5YN6zK2V3PO6WSHlEaG+de8APU7TRBKqXLbeCCVx+Zu5FBazskROLkF7stPhIcEcXaHxvzxnPYM6xRNm8ZhWhW4mtAEoZQqlwVbD3PnJ6sIDw3i7PaNCQ8NIjwkmIhQZ+SNy0ic+nWDOSsmwidj+FXFaYJQSnlsxm97+fuXG+jcNIL3JvcnJlI7i2syTet+Jjzcjkg4cOAAV1xxhdtjhg8fTvEhvcW9+OKLnDhx4uTzMWPGcPz48UqLU1VPq/ceY8LUX/nXt1tIy87z+DxjDM99t5Upn69nSMcoZt1+tiaHWkAThJ9q3rw5s2fPPuPziyeIr7/+WteWqMWMMbz/6y6uenMpe45k8tqCnZz7r/m8u3gXuWWUrs7NL+TPM9fy6vwdTOrfinduiDtZGVXVbJogvGzKlClMnTr15PPHHnuMp556ipEjR9K3b1969OjBnDlzTjtv9+7ddO/eHYCsrCwmTZpE165dufTSS0+pxXTHHXcQFxdHt27dePTRRwFbAPDAgQOMGDGCESNGALZ8eEpKCgAvvPAC3bt3p3v37rz44osnr9e1a1duvfVWunXrxqhRo7TmUw2Rnp3HXdNW89hXmzinUzQLHhzBvLuHEtu8Pk/M28T5L/zCV2sPUOhmLkJqVh43vPsbX6zez4OjOvPMZT20P6EWqT1fA76ZAknrK/c1Y3rARc+WesjEiRO57777uPPOOwGYOXMm3333Hffccw/169cnJSWFQYMGMW7cuBJHdrz++uuEhYWxefNm1q1bR9++fU/ue/rpp2nUqBEFBQWMHDmSdevWcc899/DCCy8wf/58oqJOnV25cuVK3nvvPZYvX44xhoEDB3LuuefSsGFDtm/fzvTp03n77be56qqr+Oyzz3xaVlxV3JakNP708Sr2HD3BX0d34Y/ntCcgQIgMi+TjmweycHsKz3y9mbunr+btRQlMuajLyWqmicdOcON7K9h9JJMXJ/ZmQp8WPv5rVFWrPQnCR/r06cPhw4c5cOAAycnJNGzYkJiYGO6//34WLlxIQEAA+/fv59ChQ8TExLh9jYULF3LPPfcA0LNnT3r27Hly38yZM3nrrbfIz8/n4MGDbNq06ZT9xS1evJhLL730ZFXZyy67jEWLFjFu3DgtK17DzF6ZyMNfriciNJhptww8bZKZiHBuZ7vk5her9/PC91u55u3ljDgrmiv6teKxrzaSnVfABzcNqLQS2Kp6qT0Jooxv+t505ZVXMnv2bJKSkpg4cSKffPIJycnJrFy5kuDgYNq2beu2zHdZdu3axXPPPceKFSto2LAhkydPPqPXKaJlxf3XobRsvll/kJDgQDo1Cadjk3AahNVxe2x2XgGPztnIp/H7OLt9Y166uneppSkCA4Qr+rXkkp7NeH/JbqbO38H8rcm0aFCXabcMpFPTCG/9WcrP1Z4E4UMTJ07k1ltvJSUlhV9++YWZM2fSpEkTgoODmT9/Pnv2nL7MoatzzjmHadOmcd5557FhwwbWrVsHQFpaGvXq1SMyMpJDhw7xzTffMHz4cOD3MuPFm5iGDRvG5MmTmTJlCsYYvvjiCz766COv/N2qYnLzC/l5yyFmxieyYOthincRREeE0DE6nE5Nw52kEUF4SBD/99k6Nh9M464RHbn/gs4EergwTWhwILef24GJca2Yu/YAF/WI0ZpHtZwmiCrQrVs30tPTadGiBc2aNePaa69l7Nix9OjRg7i4OLp06VLq+XfccQc33ngjXbt2pWvXrvTr1w+AXr160adPH7p06UKrVq0YMmTIyXNuu+02Ro8eTfPmzZk/f/7J7X379mXy5MkMGDAAgFtuuYU+ffpoc5If2ZKUxqz4RL5YvZ+jmbk0rR/C7ed24PJ+LakTGMCO5Ax2HMpg++F0th/O4ItV+0nP+b30c4OwYN6b3J8RXZqc0fUb1qvDDYPbVtJfo6ozLfetyk3f18qXmpXH3LUHmBW/j3WJqQQHChfENuXKuFYM6xhVallrYwyH0nLYfjidvUdPMOKsJjRvULcKo1fVmZb7VsqP7T+exfhXfyUlI4cuMRE8OjaW8b1b0Kie+z6G4kSEGGelMaUqkyYIpXwoO6+A2z9aSU5eAZ/dcTZ9WzfUQnbKb9T4BGGM0X9wlaimNEn6A2MMD32xnvX7U/nvH+Lo16aRr0NS6hQ1ekpkaGgoR44c0Q+1SmKM4ciRI4SGalNGZfhw6R4+X7Wfe0d24vzYpmWfoFQVq9F3EC1btiQxMZHk5GRfh1JjhIaG0rJlS1+HUe0tTzjCk/M2MbJLE+4d2cnX4SjlllcThIiMBl4CAoH/GmOeLba/DfAuEA0cBa4zxiQ6+wqAotoYe40x48p7/eDgYNq1a1eBv0CpyncwNYs7p62idaMw/jOpNwEezlNQqqp5LUGISCAwFbgASARWiMhcY8wml8OeAz40xnwgIucBzwDXO/uyjDG9vRWfUr6QnVfA7R+vIiu3gOm3DqJ+qC63qfyXN/sgBgA7jDEJxphcYAYwvtgxscDPzuP5bvYrVWMYY3hkzgbW7jvO81f10hIWyu95M0G0APa5PE90trlaC1zmPL4UiBCRoopioSISLyLLRGSCuwuIyG3OMfHaz6B8YfuhdHYcTvdoIMQny/cyMz6Ru0Z0ZHT3ZlUQnVIV4+tO6geBV0VkMrAQ2A8UOPvaGGP2i0h74GcRWW+M2el6sjHmLeAtsDOpqy5sVZsVFBp+2JTEu4t389vuowA0iwxlWKcohnay1VGLT3KL332Ux7/ayPCzorn/gs6+CFupcvNmgtgPtHJ53tLZdpIx5gDOHYSIhAOXG2OOO/v2O78TRGQB0Ac4JUEoVZVSs/KYuWIfHyzdTeKxLFo2rMvDF3elXkgQi7en8N1GW1hPBLo3j2RYpyiGdYqmZcO63PHJKpo3qMtLE/t4XDxPKV/zWi0mEQkCtgEjsYlhBXCNMWajyzFRwFFjTKGIPA0UGGMeEZGGwAljTI5zzFJgfLEO7lO4q8WkVGXYlZLJ+7/uYtbKRE7kFjCgXSNuGtKOC2KbnvJhX1BoWL8/lUXbklm0PYVVe4+R75RgDasTyBd/GsJZMdrvoPyLT2oxGWPyReQu4DvsMNd3jTEbReQJIN4YMxcYDjwjIgbbxHSnc3pX4E0RKcT2kzxbWnJQyhvW7jvOyz9t5+ethwkOCGBsr+bcOKQt3VtEuj0+MEDo3aoBvVs14O6RnUjPzmNZwlGW7Exh+FlNNDmoaqdGV3NV6kwlp+dw3vMLqBMYwHWD2nDtoNa6NoKqkbSaq1Ll9MzXm8nOK+CLPw2hY5NwX4dTu+WegP3x0HYYaF21KlWjazEpdSaW7jzC56v3c9s57TU5+NquRfD6YPhgLPz0hK+jqXU0QSjlIje/kH/M2UDLhnW5a4TWSPKZnHSY92f44BL7vOs4WPwCLJ3q27hqGW1iUsrFO4t3seNwBu/cEEfdOoG+Dqd22vEjfHUfpCbC2XfBiL9DUAjMugG+ewjCoqDXRF9HWStoglDKkXjsBC//tJ1RsU0Z2VXLb1e5rGPw3d9hzScQdRbc/D20GvD7/sv+C59cAXP+BHUbQudRvou1ltAmJqUcj39lR1I/MjbWx5HUQlv+B1MHwtoZMOwB+OPCU5MDQHAoTJoGTbvBzD/A3uW+ibUW0QShFPDjpkP8sOkQ94zsRMuGYb4Op/bIyYDZN8GMa6BeNNz6M4x8xCYDd0Lrw7WfQf1mMO1KOFSNp0el7ICEBb6OolSaIFStl5VbwGNfbaRTk3BuHqrrh1SpHx6BDZ/D8Ifg1vnQvHfZ54RHw/VfQlBd+PgyOL7X21FWvpwM+OhS+OgyOLDG19GUSBOEqvVenb+dxGNZPDmhO3WCavg/CWNg1UeQecTXkcDuxRD/Dgy6A4b/FYLqlH1OkYZt4PrPIe+E/aDNTPFenN7w0+OQus/eEc25CwryfB2RWzX8X4OqjT5flciLP27jUFp2mcfuOJzOWwsTuKxvCwa1b1zm8dXe7sUw9y5Y9b5v48g9YT8YG7aF8x4+s9do2g2umWlHO318uR0aWx3sWQK/vQUD/wjjXoVD62Hxi76Oyi0dxaRqlHcW7+LJebZd+tWfd3Bxz2bcNKQdvVo1OO1YYwz/+HIjdYMDeWhM1yqO1EdWf2x/H1zn2zjmPw3HdsENX0Gdemf+Oq0HwZUf2D6MD8dDy/6lHx8RA0Pu892M7LwsmxgbtIbz/gEh4dDtUlj4L+g6Fpp08U1cJdAEoWqMN3/ZyTPfbOGi7jE8MOospi3fy8z4fcxZc4B+bRpy05B2XNitKUGB9sZ57toDLE04wlMTuhMVHuLj6KtAdipsmmMfJ60v/VhvSlwJy16DfpOh3TkVf72zRsOlb9ghskd2lHxcYQHkZkCbIaePkKoqC56BozttH0qIM0v/on9Dwi8w5047tDfAf+bfaIJQNcLU+Tv493dbubhnM16c2JvgwAAeGRvL/Rd0YlZ8Iu8v2c2d01bRPDKUPwxuy8U9mvHkvM30ahnJ1QNa+zr8qrHxC8jPgs4XwbZvbJNMSBVXmM3PsR+E4TFwQSWWzuh5lf0pTXYaPNcJ1s/yTYLYvxKWvAJ9/wAdRvy+PTwaLvoXfH4LLH8Dzr6z5NeoYtoHoaq9l37czr+/28r43s15yUkORSJCg7lpaDvmPzict/8QR5vG9Xj2my0M+9d8jmbm8NSEHrVnAZ/VH0N0F4i70T5P2lD1MSx8DpI3w9gXIdR92XSvCa0PZ11kR00V5FfttfNzYc7dEN4URj11+v4eV0Dn0fDTk3DEf9ZF0wShqi1jDC98v5X//LiNy/q24IWrep9sPiouMEC4ILYp028bxDf3DuPqAa15aExXerSs4g8pX0neCokroPe1ENPTbkuq4n6IpPW2nlLPidD5wqq9dpEeV8KJlIrNPziw2n6QZx3z/JzFL8DhjXDJf9wnRhG7LzAYvroXCgs9e93CAvj1ZZj/jOexlIMmCFUtGWP493dbefnnHVwV15J/X9HL4zuBrs3q88xlPbhlWHsvR+lHVn8MEgi9JtmO2rCoqu2oLsi3nbN1G8LoZ6vuusV1PN9+QK+fdeav8e1DsOg5mDrIzgAvy6FN9s6px5X2DqYk9Zvbu4vdizwbZXZ4M7xzAfzwD5t8PE0q5aAJQlU7xhie/WYLry3YydUDWvPsZT1rTzPRmSjIsyUsOo+G8Cb222qznlV7B7H0FTi4BsY8B2GNqu66xQWFQOx42DLPDrUtr0MbYe8S6HsD1Iuyo6dm31TyPIyCfNvnEhoJo/9f2a/f9w/Q7lz4/hE7fNfta+bBL/+GN4bBsd1w+Ttw1UcQUPkf55ogVLVijOHJeZt5c2EC1w9qw9MTuhOgyaF0O36EzMPQ59rft8X0tN9A83O9f/2U7bYJpOtY6DbB+9crS48r7Wimbd+W/9wV/4WgUDj/MTvze8TfYdNcmDoANnxmJyK6WjYVDqyCMf+Ceh7MsxGBcS+DKbAVbYu/3sG18NYImP+UfT/v/M32X3hp2K4mCFWtPPPNFt79dRc3DmnLE+O7aXLwxOqPbZ2jTi7VT5v1hMI8SN7i3WsXFtqmpeC6MOZ5717LU22GQEQzWD+7fOdlp8HaT6H75fYuKKgOnPt/trBggzb2TuLT6yA9yR6fsgPm/xO6XALdLvP8Og3bwshHYccPsO5Tuy0/x/Z7vDXCJvuJH8OV79m7GC/SBKGqjRm/7eUt587hkUtiEV1+smwZyfabcs+JtgO0SFV1VK94G/Yts/0OEX5SQj0g0H7Ib/++fB3Na2dAXib0v/nU7U1j4eYf4IIn7d3a1AGw+hOYe7dt0rr4+fJ/wx9wG7QaCN9OgS1f2+akRc/ZPqQ7l9u7hyqgCUJVC8sSjvDwlxs4p3M0j47V5OCxdZ9CYT70ue7U7Y06QHA973ZUJ2+FHx+zHcO9JnnvOmeixxX2DmrTXM+ON8Y2LzXvCy36nb4/MAiG3AO3/wpNYu2aFXuXwIX/tIMCyisgwJbhyD0BM66G3ExbxXbCa7ajv4roRDnl9/YcyeSOj1fSpnEYr17Tp8ShrKoYY2zzUos4aFKslEhAAMR0996M6tREW6m0TjiMfcl3pS1K0qw3NO5oRzP1u6Hs43cvgpStMP610o+L6giTv4aV70Lqfjus+ExFd4bxr8KhDTDsQTuPo4ppglB+LS07j5s/iMcA79zQn/qhwWWeoxwHVtlJaZf8x/3+mJ622aSwsHJHwJw4apNDThpMngeRLSvvtSuLiO2sXvAspB2wQ0xLs+K/9pt7dw/6EgICoP8tlRNnz6uAMmaIe5F+FVN+q6DQcM/01exOyeS1a/vSNqoCRd1qo9Uf2xE33S93v79ZT8hNt0XzKktuJnxypR1+OWkaNOtVea9d2XpcCRg7+qg0aQdh8zzbTBdct0pC8xeaIJTf+ufXm1mwNZknxndncAfvjtaocfKyYP1ndsx/SSUtYnrY35XVUZ2fC59eb+9crngX2g2rnNf1lsYdbJ9CWZPmVr4PphDibqqSsPyJJgjll2b8tpd3Fu9i8uC2XDOwlhTTq0yb50FOault4E1iISCocvohCgttx+zOn2yfQ9dLKv6aVaHHlXZuQfI29/sL8myC6Hg+NKpFM+8dmiCU33EdsfTwxbVknYbKtvoju+ZA21K+xQeF2OJ9FR3JZAx89zf7TXzko3Y2cHXR/TJAYEMJcyK2zIOMpMrrU6hmNEEov6IjlirBsT2wa6G9eyir8zmmEkpuLHrOlqkedCcMvb9ir1XVImLsmhTrZ50+axlgxTs20Xa6oOpj8wP6r0/5DR2xVEnWTre/e19T9rExPSDjEKQfOrNrxb8HPz9lJ+KNesr/hrN6oseVcDTB9p24OrzZDm+Nu8mvFvGpSpoglF/YcTidSW8u0xFLFVVYCGs+gfbn2m++ZWlWgRnVm+bA//5sS3iMn+qVYnFVoutYCKxzeumNFe9AYAj0qUZNZpWsmv4XVTWFMYaPlu3hklcWk5SWzZvX99MRSxWxexEc3wu9ryv7WDjzkUxHd8Fnt9g1oK/84NQyHtVN3QY2yW34zK6vAHa1vbUz7HrRnhTZq6F0opzymZSMHP46ex0/bTnMOZ2jee6KnjSpH+rrsKq31R9DSKTno4hCI21xuPJ2VK/71I7wueJdqBNW7jD9To8rbYf07kXQfrj9+3LTYcCtvo7MpzRBKJ+Yv/Uwf5m1lrTsfB4dG8sNZ7fVyqwVlfCLbfbpc235JnSVt6PaGNup23aof86SPhOdL4Q6EbBull2P4bf/2kl+7uou1SJebWISkdEislVEdojIFDf724jITyKyTkQWiEhLl303iMh258eDYimqOsjOK+DRORu48b0VRIWHMPeuIdw4pJ0mh4paOwM+vtyO1T/n/8p3bkxP20mbnebZ8QfXwJEdzkzkGiK4ru2L2DwXdv5sS5T0v7V6drpXIq8lCBEJBKYCFwGxwNUiElvssOeAD40xPYEngGeccxsBjwIDgQHAoyJSdSUMlVdsOpDG2FcW88HSPdw8tB1f3jmELjFVX4CsRjHGri72xR+h9SC46Vuo36x8r1HUUX1og2fHr58NAcEQO6581/F3Pa6w9aPm3AmhDUouUVKLePMOYgCwwxiTYIzJBWYA44sdEwv87Dye77L/QuAHY8xRY8wx4AdgtBdjVV5kjOHDpbuZMPVXUrPy+PCmAfzjklhCg2vn0MFKU5Bn1xyY/xT0nATXfW47XMvr5NoQHsyoLiywnbmdRlVp2ekq0e5cqNcE0g/auks1oW+lgryZIFoA+1yeJzrbXK0FisojXgpEiEhjD89FRG4TkXgRiU9OTq60wFXlyc0v5G+fr+eRORsZ2imKb+87h3M6R/s6rOovOw2mTbQzps/5P7j0DbvC2ZmIiLErznnSUb17sf0A7VmDmpeKBAY5dw1SK+suuePrTuoHgVdFZDKwENgPFHh6sjHmLeAtgLi4ODfTIJUvpWTkcMfHK1mx+xh3jejIny/orH0NlSHtoK2YengTjHul4qUtROxw16S1ZR+7fpZd46FzDb2hH/E3O7S1cQdfR+IXvJkg9gOtXJ63dLadZIw5gHMHISLhwOXGmOMish8YXuzcBV6MVVWyjQdSue3DlRzJzOGVq/swtlcZ9faVZw5tsskh+zhcO9MWkasMMT1h6VRbkbWkO5H8HLsCW9exNbfsdWgktB7o6yj8hjebmFYAnUSknYjUASYBp6zvJyJRIlIUw9+Ad53H3wGjRKSh0zk9ytmmqoH/rTvI5a8vodAYZt8+uPomh6zjkJPh6yh+l/ALvHshmAK48ZvKSw5gO6oL8yB5S8nHbP/BVojtcUXlXVf5Na8lCGNMPnAX9oN9MzDTGLNRRJ4QkaLhD8OBrSKyDWgKPO2cexR4EptkVgBPONuUHyssNLzw/VbunLaKbs0jmXvXULq3KGEtAn9XWADvjIKXetpOWXeF3KpSxmHb5xDZEm758feRR5UlxlnYp7T5EOtnQVgUtBteuddWfsujJiYR+Rx4B/jGGFPo6YsbY74Gvi627RGXx7MBt3V2jTHv8vsdhfKRw+nZLNyWQuPwOjSLDKVZ/brUrxuEFBsfnpGTz58/XcP3mw5xVVxLnpzQnZCgajxKacv/7BrEka1g9k2w4XO4+PkzW4C+Mix/E/Kz4aqPvDM5rVF7CK5nO6r7uNmfnQbbvrX9HYG+7rpUVcXT/9KvATcCL4vILOA9Y8xW74Wl/MGvO1K4d8ZqUjJyT9leNziQZpGhxDg/zSJD+XHTYXYkZ/Do2FgmD257WgKpVoyBJS/bEhR3rrClrOc/DVMHwOhnodfVVTuBKifDronc5WKI6uidawQEQEz3ku8gtvzPJqiaNDlOlcmjBGGM+RH4UUQigaudx/uAt4GPjTF5XoxRVbHCQsPU+Tv4z4/baB8dzpvX9wOEpNRsDqZm2d9p2SSlZrM84ShJadnUDw3igxsHMLRTDSi0t3cZJK6AMc/ZDtsh98BZY2DuXfDlHbbJ6ZIXoUGrMl+qUqz+2HZKD7nXu9eJ6WlLhRcWnl6Zdf0sWx22ZX/vxqD8isf3is78hOuA64HVwCfAUOAGTh1xpKqxY5m53D9zDQu2JjO+d3P+eWkP6oWU/r9JQaHBGFNzFvdZ8jLUbXTqcp1RHWHy1/ab/I+PwWtnw6gnoO9k75a5LsiHZVOh1SBoNcB71wHbr7HibTi269RhnhmHIWEBDL2v1peeqG08+j9bRL4AFgFhwFhjzDhjzKfGmLuBcG8GqKrO6r3HuPjlRSzZcYSnJnTnxYm9y0wOAIEBUnOSQ/JW2Po1DLjt9Jm0AQEw8Db40xJo0Rfm3Q8fjrOlr71l05e2fPeQe7x3jSInZ1QXa2ba+IUdOaXNS7WOp/+qXzbGxBpjnjHGHHTdYYyJ80JcqgoZY3j/111c9eZSAgKEz+4YzHWD2lTvfoQzteQVCAotvcxzw7bwhzkw9mW74P3rg2HZ67+vJVBZivpCGneCzhdV7mu706QrBASdPqN6/Sxo2t3uV7WKpwkiVkQaFD1x5if8yTshqaqUnp3HXdNW89hXmzi3czT/u3sYPVpW06GpFZWeZNcB6H0t1CujL0UE+t0Af1oGbYfBt1PgvYsgeVvlxbNroU1Ag++qmtXagkIgusupdxBHd9n+GL17qJU8/b/uVmPM8aInTgG92r2SRg2wJSmN8a/+yrcbk5hyURfeuj6OyLBqvDJYRS1/0xbAO/tOz8+JbAHXfAqXvQ0p2+CNobDoBdt3UFFLXrbF43pOqvhreSqm56l3EBucUeha2bRW8jRBBIpLe4NTyvsMK4MpfzB7ZSITpv5Kek4+024ZyO3ndqjddZJy0iH+HVtGorx1eESg51Vw529w1mj46XH470hI8rB8tjtJG2DHj7bPI7gKV9lr1hMyD0P6IdvEtW4WtB5cdSO2lF/xNEF8C3wqIiNFZCQw3dmmqpnsvAL+OnsdD85aS59WDfnfPUMZ2L72rrl70qqPIDu1YkNJw5vAVR/aNZrT9sNb58L8Z2x9o/Ja8oqduBZ385nHcyZc16g+tMFOFtTSGrWWp8Nc/wr8EbjDef4D8F+vRKS8ZndKJnd8sorNB9O4a0RH7r+gM4FVcddgDOxdCqn77WQvf6uzX5AHy16z35RbVsKYi24ToN05tl/il2dh81dw2Zu/f/iWJTXRNu30vwXCGlU8nvIoivHgWrs+c0AQxE6o2hiU3/B0olwh8Lrzo6qhbzcc5C+z1hEYKLw3uT8jujTx/kXzc2yJiuWv2w8csIvM9Jtsl3OMPG2JD9/Y+CWk7oMx/6681wxrBJe9Zdvuv7oX3rkQrnwfOo8q+9xlr9ukOsgH40BCI+0orYNrYf8q6DAS6ukdZm3l6TyITiIyW0Q2iUhC0Y+3g1MVl1dQyJPzNnH7x6to3ySceXcP9X5yyEiGBf8P/tMdvrwd8rLtzOMbvrIjfn59CV7sYWscJcZ7N5ayGGPjieoMnS6s/NfvfCHctsBOtJs+CeLfK/347FRY+YFdk6Bhm8qPxxMxPW3dpbREHb1Uy3naxPQedo3o/wAjsHWZasjMqJrrYGoWd01bzco9x5g8uC0PjelKnSAv/mdLWg/L3rDj5gtyoOMFMOgO6HDe7zNw250Dx/bAb2/Bqg9t2YqW/WHg7RA7HgKreBRVwnw4tB7Gveq9oaQRMXYW9uwbYd59duLbef9wf7349yA3vWomxpWkWU/YPBeCw+CsKph/ofyWpwmirjHmJxERY8we4DERWQk8UtaJyjfmbznMA7PWkpNXULEFe47ttuUlCsso4pu0zrZZB4fZ9XwH3g7Rnd0f27ANXPg0DJ8Ca6bbJqjPbobv/2H7KALLGCAX1tAmleZ9IbT+Gf1ZJ/36MoTH2FFI3hQSDpOmw9cPwOIXbJPW+Kl27kGR/FxbGLDdudCsl3fjKU3RjOqzxti4Va3laYLIcRb22S4id2FXhtP/c/xQdl4B//x6Mx8u3UOXmAimXtuXDtEV+E+14h07oqZOROnH1YuC8x+35aA97VgNibDDOPvfAtu/tx3Fa2eUfV5uuvNA7OzeFv1swmjZH6LPggAPy4wfXGfvIEY+euoHtbcEBjlF/trYobBpB2HSx7ZfBuydV/pBGP+q92MpTasB0CS29NnkqlYQ48FCKCLSH7voTwPsQj71gX8bY5Z5NbpyiIuLM/HxPm7P9rGNB1K5b8Yath/O4Oah7fjLhWcRGlzBNRk+HG9XVvvjL5USY6XIOg4HVtn+i8QV9ifrmN1XJ8LWSSpKGC3jSp4V/dmttu7S/RuhboOqit5aN8tWhm3UHq51KqW+drZNbrcv1qJ4qsqIyMqSSiaVeQfhTIqbaIx5EMjA9j8oP1JYaHhn8S7+/d1WGoQF89HNAxjWKbriL2yM/Zbd9ZKKv1ZlqtvA9mt0OM8+NwaOJvyeLPb9Bov/YwvMATRsd2rCaNodMg7Z/o+Bt1d9cgDoeaXtm/j0Wvjv+TDwj5C8GS59U5OD8htlJghjTIGIDK2KYFT5JaVm88CsNfy64wijYpvy7OU9aVSvkia5p+2HrKO/t0n7KxE7+7lxB+jllKXIPQEH1/yeNHYthPUz7b6gULt0JthOdF9pNwxu+h4+uRJ+fhLqt9CSFsqveNoHsVpE5gKzgMyijcaYz70SlfLItxsOMuXz9eTkFfLsZT2Y2L9V5VZgLarJ48sO0zNVJwzaDLY/YO8y0vY7CSPe/vSa5PsSEk262DWmv7rX3lVU9SgupUrhaYIIBY4A57lsM4AmCB/Izivg0Tkb+TR+Hz1bRvLixN60r0hHdEmS1mE7gmMr/7WrmohdyzmypZ1j4E8imsI1HnTOK1XFPJ1Jrf0OfuT577fyafw+7hzRgfvO70ywtxbrSVoPjTvqUEelaimPEoSIvIe9YziFMeamSo9IlWrD/lTeWbyLawa25i8XdvHuxQ6ug1a6BrFStZWnTUzzXB6HApcCByo/HFWa/IJCpny+jsbhIfx1tJeTw4mjkLoX+ldxNVGllN/wtInpM9fnIjIdWOyViFSJ3l+ymw3705h6TV8i63q5MzNpvf3dzM9HMCmlvOZMG687AVVQDlQV2Xf0BM9/v42RXZowpkeM9y9YlCD8fYirUsprPO2DSOfUPogk7BoRqgoYY/jHnA2IwBMTulfuUNaSJK2DiOZlr82slKqxPG1iKqMQj/KmeesOsmBrMo9cEkuLBnWr5qIH12nzklK1nKfrQVwqIpEuzxuIyASvRaVOSj2Rx+NfbaRny0huGNy2ai6alwUp27R5SalaztM+iEeNMalFT4wxx7HrQygve+abzRw7kcczl/WomuVBAQ5tsnWMPF0iUylVI3maINwd5+kQWXWGliccYcaKfdwytB3dmkeWfUJlSSoqsaF3EErVZp4miHgReUFEOjg/LwArvRlYbZeTX8DfvlhPq0Z1uff8TlV78aR1dm3iBj5a8lIp5Rc8TRB3A7nAp8AMIBu401tBKXht/k4SkjN5akIPwupU8c3awXW2/0HLTitVq3k6iikTmOLlWJRjx+F0Xluwg/G9m3Nu50pY16E8Cgvg0EaI0yoqStV2no5i+kFEGrg8bygi33lw3mgR2SoiO0TktAQjIq1FZL6IrBaRdSIyxtneVkSyRGSN8/NGOf6maq2w0PC3z9cTVieIf1xSgSqquxbCS70gI7l856Vsh/ws7aBWSnncxBTljFwCwBhzjDJmUjsr0U0FLgJigatFpPgn3sPATGNMH2AS8JrLvp3GmN7Oz+0exlmtGWN4Yt4mVuw+xt/HdCUqvALrJK98H47ths1zyneelthQSjk8TRCFItK66ImItMVNdddiBgA7jDEJxphcbN/F+GLHGOz61gCR1OICgIWFdrb0+0t2c/PQdlwZ1/LMXywvG7Y5N3ib5pbv3KS1EBgCUZ3P/PpKqRrB097PvwOLReQXQIBhwG1lnNMC2OfyPBEYWOyYx4DvReRuoB5wvsu+diKyGkgDHjbGLCp+ARG5rSiO1q1bF99dbRQWGv7+5Xqm/7aPP57bnimju1SsnMbOnyE3A1rEwe7FkJniecmMg+ugaayubKaU8uwOwhjzLRAHbAWmAw8AWZVw/auB940xLYExwEciEgAcBFo7TU9/BqaJSP3iJxtj3jLGxBlj4qKjq7gzt5IUFBr++tk6pv+2j7tGdKx4cgDYNAdCG8BF/7IT3rb8z7PzjLFDXHUGtVIKzzupbwF+wiaGB4GPsN/+S7MfcF3wt6WzzdXNwEwAY8xS7FoTUcaYHGPMEWf7SmAnUOPaPAoKDX+ZtZZZKxO5d2QnHhjVueLJIT8Xtn4DXS6GFn3tXIbNHjYzpSZC1jHtoFZKAZ73QdwL9Af2GGNGAH2A42WcswLoJCLtRKQOthO6+CfVXmAkgIh0xSaIZBGJdjq5EZH22PLiCR7GWi3kFxRy/6dr+Hz1fh64oDP3X1AJyQFg1y+Qkwqx4+08htjxkLDAfvCX5WQHda+Kx6GUqvY8TRDZxphsABEJMcZsAc4q7QRjTD5wF/AdsBk7WmmjiDwhIuOcwx4AbhWRtdimq8nGGAOcA6wTkTXAbOB2Y8zRcv5tfiuvoJB7Z6xh7toD/HV0F+4eWYkzpTd9CSH1of1w+zx2AhTmw9Zvyz43aR0g0LRb5cWjlKq2PO2kTnTmQXwJ/CAix4A9ZZ1kjPka+LrYtkdcHm8Chrg57zPgs+Lba4Lc/ELunr6K7zYe4u9junLrOe0r78UL8mx/Q+fREOQMkW3RF+q3tP0Sva8u/fyD6yCqE9SpV3kxKaWqLU9nUl/qPHxMROZjh6R68JVUudqdkslT/9vEj5sP88glsdw0tF0lX2CxbUqKdRlNLAKx42DFfyE7DUJP6+v/XdI6aFV8oJlSqrYqd5EfY8wv3gikptpxOINv1h/k6w1JbD6YZleFG9+NP5zdtvIvtmkOBNeDjiNP3d51HCx7DbZ/Dz2ucH/uiaOQug/631L5cSmlqiUt2V3JjDFsPZTO1+uT+HbDQbYdygCgX5uGPHxxV0Z3j6Flw7DKv3BhAWyZB51HQXCxVedaDYTwGNs/UVKC0BnUSqliNEFUktz8Ql5bsIO5aw6QkJKJCAxo24jHx3Xjwm4xxESGejeAvUshM/nU5qUiAQHQdSys/hhyM933MRStARGjI5iUUpYmiEry3PdbeWthAoM7NOamoe0Y1a0pTSK8nBRcbZoLQaHQ8QL3+2PHwYq3YfsP0G3C6fsProP6LaBeY6+GqZSqPjRBVIL5Ww/z1sIErh/UhicndK/6AAoL7WS4judDSLj7Y1oPhrAo20/hLkHoDGqlVDGezoNQJTicls2DM9fSJSaCv1/ctfSDjYGkDfZ3ZUpcAekH7ZyHkgQGQddLbEd1XrEqKXlZkLJNZ1ArpU6hCaICCgoN9326hhO5Bbx6TR9CgwNLP2H9LHhjCCx/s3ID2TwXAutA5wtLP67rOFvEb+fPp24/tAlMoXZQK6VOoQmiAt74ZSdLdh7h8XHd6NgkovSDC/JhwTP28YJn7LDSymCMbTbqcF7pcxwA2p1ji/htKrZGRNJa+1ubmJRSLjRBnKGVe47ywg/bGNuruWdrN6z7FI4mwHn/gJy035NFRR1YZecvuBu9VFxgMHS5xJbdyM/5ffvBdRAaCQ2qb8l0pVTl0wRxBlJP5HHP9DW0aFCXpy/tXnaRvYI8+OX/QbPeMOwBu97zinfg8OaKB7NpLgQEwVkXeXZ87DhbzC/BZb5jUQd1ZRQLVErVGJogyskYu37DobRsXrm6D/VDPVhYZ80ncHwPjPi7/RAe/pAdbfTdQxXrsC5qXmp3LtRt6Nk57YfbYn5FzUwF+XBoozYvKaVOowminD5evpdvNybx19Fd6NWqQdkn5OfAwufs6m6dnDkK9RrDuVNsZ/H2H848mKT1cGyXvSvwVFCILea39X/2zubIDsjP1g5qpdRpNEGUw+aDaTw5bxPndo7mZk8L7a3+yPYRjHjo1Cac/rdA4472LqIg7wwDmgsSYPsVyiN2vC3qt3uRywxqTRBKqVNpgvDQidx87p6+msi6wTx/VS8CAjxor8/LhoXPQ6tBdpSRq6A6MOppOLLdVlotL2Ng45fQdqjn600X6TjSFvXbNAcOroXAEIiqcQv2KaUqSBOEhx6fu4mdyRm8OLE3UeEhnp206gNIP3D63UORzhfaxHEmw16Tt9jk0rUczUtFguvaon6b59kE0TTWTqRTSikXmiA8sPfICT6N38dt57RnSEcPv63nZcGi56HNUDv/wB0RuPCfkJMO8/9ZvqA2zQXEFuE7E7Hj4USKbWbS5iWllBuaIDyw7VA6ABd2i/H8pPh3IeMQjPhb6cNHm3S1w17j3y3fsNdNc6D12RBRjphcdbwAgpyy4NpBrZRyQxOEBxJS7JoOHaJKKIRXXG4mLP6PHX7admjZx5dn2KsxdqLb4Y3lG71UXEj47wsLaYlvpZQbmiA8kJCcSeN6dYgM82DOA8Bvb9u1GUY85Nnxpwx7/d79MXnZdj2HN4bC9IkQ0Qy6XebZ65dkwG12MaEYH1SgVUr5Pe2Z9EBCcibto90ssuNOTjr8+hJ0GAmtB3l+kQG32mam7x6yHdeBTjJKPwTx79iZ1ydSoEksjHsFelx5+spx5dX+XPujlFJuaILwwM7kDC6IberZwcvfhKyjnt89FAkMhgufhmlX2TuQNoNh+RuwfjYU5tvJbYPusB3eWhJDKVUFNEGUIfVEHkcycz27g8hOhSWvQKcLoWVc+S/WaZS9e/jhHzYpBNeDuBth4O3QuEP5X08ppSpAE0QZdjod1O096aBe9gZkHy//3UMREbjoXzDvfjtHos/1ULfBmb2WUkpVkCaIMiQkZwKUfQeRmQJLp9qyF817n/kFozrB5Hlnfr5SSlUSHcVUhoTkDIIChFaNwko+qLAAPr/VFr077+GqC04ppbxI7yDKkJCcSevGYQQHlpJLFz5nh6he8qKd+KaUUjWA3kGUISElo/T+h53zbS2lnpOg3+Qqi0sppbxNE0QpCgoNu4+coENJ/Q9pB+CzWyC6C1zygg4/VUrVKJogSrH/WBa5+YXuO6gL8mDWjbYo31UfQB0PJ9IppVQ1oX0QpdiZ7AxxjXbTxPTT47BvGVz+DkSfVcWRKaWU9+kdRCmKEkSH4gliy//shLi4m6HHFT6ITCmlvE8TRCkSUjJpEBZMo3p1ft94dBd8cQc06w2jn/FZbEop5W1eTRAiMlpEtorIDhGZ4mZ/axGZLyKrRWSdiIxx2fc357ytInKhN+MsSUJyBu2jXPoW8rJh1g0g2H6HIA9XllNKqWrIawlCRAKBqcBFQCxwtYjEFjvsYWCmMaYPMAl4zTk31nneDRgNvOa8XpWyVVxdmpe+nWKX6JzwBjRsW9XhKKVUlfLmHcQAYIcxJsEYkwvMAMYXO8YA9Z3HkcAB5/F4YIYxJscYswvY4bxelUnPzuNwes7vI5jWzYSV78GQe6HLmNJPVkqpGsCbCaIFsM/leaKzzdVjwHUikgh8DdxdjnMRkdtEJF5E4pOTkysrbgB2pTg1mKLC4cRR+Oo+aD0YznukUq+jlFL+yted1FcD7xtjWgJjgI9ExOOYjDFvGWPijDFx0dHRlRpYUZG+DtH1YM+vkJcJ5z8KgToyWClVO3gzQewHWrk8b+lsc3UzMBPAGLMUCAWiPDzXqxKSMwgQaN04DPYug8AQaN6nKkNQSimf8maCWAF0EpF2IlIH2+k8t9gxe4GRACLSFZsgkp3jJolIiIi0AzoBv3kx1tPsTMmkVaMwQoICYd9vNjnoqCWlVC3itQRhjMkH7gK+AzZjRyttFJEnRGScc9gDwK0ishaYDkw21kbsncUm4FvgTmNMgbdidWfnYWeIa142HFwDraq0j1wppXzOqw3qxpivsZ3PrtsecXm8CRhSwrlPA097M76SFBYadh/JZEjHKJscCnKh9SBfhKKUUj7j605qv3QgNYvsvEJbYmPvMruxpd5BKKVqF00QbpyyzOi+36BRBwiv3FFSSinl7zRBuJFQVMU1Kgz2LdfmJaVUraQJwo2ElEwiQoKIzt0PJ1K0g1opVStpgnDD1mCqh+xbbje00jsIpVTtownCjYTkDFukb98yCI2EqM6+DkkppaqcJohiTuTmcyA1286B2PcbtBoIAfo2KaVqH/3kK6ZoBNNZkfmQvEX7H5RStZYmiGISnCquXQq22g3a/6CUqqU0QRSTkJyBCDRPXQsSCC36+jokpZTyCU0QxSQkZ9I8si5BB1ZAs55Qp17ZJymlVA2kCaKYhJQMOkeHQmK8Ni8ppWo1TRAujDHsSs7k7LD9kJ+lHdRKqVpNE4SLQ2k5ZOYW0Ee22Q2tBvo2IKWU8iFNEC6KajC1O7EBIltB5GnLYCulVK2hCcLFzpRMwNDwyCq9e1BK1XqaIFzsPJxBxzrHCMxM0gShlKr1NEG4SEjJZFTEHvuktSYIpVTtpgnCRUJyBoOCt0OdcGjSzdfhKKWUT2mCcGTnFbD/eBZn5W2GFv0g0KvLdSullN/TBOHYfSSTMJNFk8ztuoKcUkqhCeKkhORMegfsQCjUCXJKKYUmiJMSkjPoJ9sxCLTs7+twlFLK5zRBOBKSMxlcZwfSJNauIqeUUrWcJgjHruQ0erJNh7cqpZRDh+pgi/QFpGwhjBNawVUppRx6BwEkZ+TQJW+zfaId1EopBWiCAGz/Q7+AbeSERkPDtr4ORyml/IImCGyCiJOtFLToDyK+DkcppfyC9kEAh/fvpnVAMoXtB/s6FKWU8ht6BwEEH1wBQIDOoFZKqZM0QQBRx9aQK3WgWS9fh6KUUn6j1ieI3PxCOudu4lB4LATV8XU4SinlN7yaIERktIhsFZEdIjLFzf7/iMga52ebiBx32Vfgsm+ut2I8nppK94Dd5DTT8hpKKeXKa53UIhIITAUuABKBFSIy1xizqegYY8z9LsffDfRxeYksY0xvb8VXpEmdXOg+gY59x3r7UkopVa14cxTTAGCHMSYBQERmAOOBTSUcfzXwqBfjcS+iKVzxTpVfViml/J03m5haAPtcnic6204jIm2AdsDPLptDRSReRJaJyIQSzrvNOSY+OTm5ksJWSikF/tNJPQmYbYwpcNnWxhgTB1wDvCgiHYqfZIx5yxgTZ4yJi46OrqpYlVKqVvBmgtgPtHJ53tLZ5s4kYLrrBmPMfud3ArCAU/snlFJKeZk3E8QKoJOItBOROtgkcNpoJBHpAjQElrpsaygiIc7jKGAIJfddKKWU8gKvdVIbY/JF5C7gOyAQeNcYs1FEngDijTFFyWISMMMYY1xO7wq8KSKF2CT2rOvoJ6WUUt4np34uV19xcXEmPj7e12EopVS1IiIrnf7e0/hLJ7VSSik/owlCKaWUWzWmiUlEkoE9FXiJKCClksKpCtUtXtCYq0p1i7m6xQs1K+Y2xhi38wRqTIKoKBGJL6kdzh9Vt3hBY64q1S3m6hYv1J6YtYlJKaWUW5oglFJKuaUJ4ndv+TqAcqpu8YLGXFWqW8zVLV6oJTFrH4RSSim39A5CKaWUW5oglFJKuVXrE0RZy6L6IxHZLSLrneVY/bK+iIi8KyKHRWSDy7ZGIvKDiGx3fjf0ZYzFlRDzYyKy32X52zG+jNGViLQSkfkisklENorIvc52v32fS4nZn9/nUBH5TUTWOjE/7mxvJyLLnc+OT52ipD5XSrzvi8gul/e4d5mvVZv7IJxlUbfhsiwqcLW/FwYUkd1AnDHGbyfqiMg5QAbwoTGmu7PtX8BRY8yzTjJuaIz5qy/jdFVCzI8BGcaY53wZmzsi0gxoZoxZJSIRwEpgAjAZP32fS4n5Kvz3fRagnjEmQ0SCgcXAvcCfgc+NMTNE5A1grTHmdV/GCqXGezswzxgz29PXqu13ECeXRTXG5AJFy6KqCjLGLASOFts8HvjAefwB9oPBb5QQs98yxhw0xqxyHqcDm7GrNvrt+1xKzH7LWBnO02DnxwDnAUUftn7zPpcSb7nV9gTh8bKofsYA34vIShG5zdfBlENTY8xB53ES0NSXwZTDXSKyzmmC8pvmGlci0ha7qNZyqsn7XCxm8OP3WUQCRWQNcBj4AdgJHDfG5DuH+NVnR/F4jTFF7/HTznv8n6I1d0pT2xNEdTXUGNMXuAi402kaqVac9T+qQ/vm60AHoDdwEHjep9G4ISLhwGfAfcaYNNd9/vo+u4nZr99nY0yBMaY3dmXMAUAX30ZUuuLxikh34G/YuPsDjYAymx1re4Ioz7KofsNlOdbDwBfY/2Grg0NOG3RRW/RhH8dTJmPMIecfWyHwNn72XjttzJ8BnxhjPnc2+/X77C5mf3+fixhjjgPzgbOBBiJStOiaX352uMQ72mneM8aYHOA9PHiPa3uC8GhZVH8iIvWczj1EpB4wCthQ+ll+Yy5wg/P4BmCOD2PxSNEHreNS/Oi9djoj3wE2G2NecNnlt+9zSTH7+fscLSINnMd1sYNaNmM/eK9wDvOb97mEeLe4fGkQbH9Jme9xrR7FBOAMp3uR35dFfdq3EZVORNpj7xrALhk7zR9jFpHpwHBsieFDwKPAl8BMoDW2NPtVxhi/6RQuIebh2GYPA+wG/ujSvu9TIjIUWASsBwqdzQ9h2/T98n0uJear8d/3uSe2EzoQ+6V6pjHmCeff4gxsc81q4Drn27lPlRLvz0A0IMAa4HaXzmz3r1XbE4RSSin3ansTk1JKqRJoglBKKeWWJgillFJuaYJQSinlliYIpZRSbmmCUMoPiMhwEZnn6ziUcqUJQimllFuaIJQqBxG5zqm1v0ZE3nSKomU4xc82ishPIhLtHNtbRJY5xdG+KCpAJyIdReRHp17/KhHp4Lx8uIjMFpEtIvKJM+NVKZ/RBKGUh0SkKzARGOIUQisArgXqAfHGmG7AL9gZ2AAfAn81xvTEzhwu2v4JMNUY0wsYjC1OB7ay6X1ALNAeGOLlP0mpUgWVfYhSyjES6AescL7c18UWwisEPnWO+Rj4XEQigQbGmF+c7R8As5w6Wi2MMV8AGGOyAZzX+80Yk+g8XwO0xS72opRPaIJQynMCfGCM+dspG0X+Uey4M61f41rHpwD996l8TJuYlPLcT8AVItIETq793Ab776ioquc1wGJjTCpwTESGOduvB35xVlFLFJEJzmuEiEhYVf4RSnlKv6Eo5SFjzCYReRi7ml8AkAfcCWRiF2V5GNvkNNE55QbgDScBJAA3OtuvB94UkSec17iyCv8MpTym1VyVqiARyTDGhPs6DqUqmzYxKaWUckvvIJRSSrmldxBKKaXc0gShlFLKLU0QSiml3NIEoZRSyi1NEEoppdz6/8aIgBwqvj2RAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "history = run_binary_cnn(train_it_cnn, valid_it, 35, 2)\n",
    "plt.plot(history.history['accuracy'])\n",
    "plt.plot(history.history['val_accuracy'])\n",
    "plt.title('model accuracy')\n",
    "plt.ylabel('accuracy')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'validation'], loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 8\n",
    "Run several experiments trying to improve the model accuracy by tuning hyperparameters changing the model structure (using the deepness parameter or tuning further if you like), and using data augmentation.  Your goal should be to beat your best MLP model by as much as possible.  You should be reading the training results to identify overfitting and tune your model and training accordingly.  Ask the instructor for help if you need guidance.  \n",
    "\n",
    "Capture at least 5 of your best experiments in the table in the Results section below.  You should capture enough information about each experiment to make it possible to re-create your results.  Write a few statements about what you learned through this exercise.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*All tests are run, but to avoid a longggg pdf I will only include the accuracy results for the test*\n",
    "\n",
    "**Starting accuracy:**\n",
    "\n",
    "Training: 93.78\n",
    "\n",
    "Testing: 92.54"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Image Augmentation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Rotation ( .01 -> .1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training: 93.78\n",
    "\n",
    "Testing: 92.54"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Horizontal flip (False->True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training: 91.51\n",
    "\n",
    "Testing: 90.91"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Vertical flip (False -> True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training: 91.04\n",
    "\n",
    "Testing: 88.11"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Perameter Augmentation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Epochs(10 -> 35)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training: 99.88\n",
    "\n",
    "Testing: 93.94"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Deepness (2 -> 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training: 95.29\n",
    "\n",
    "Testing: 87.88"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Activation Augmentation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Relu -> Sigmoid"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training: 57.36\n",
    "\n",
    "Testing: 57.34"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Relu -> tanh"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training: 87.09 \n",
    "\n",
    "Testing: 86.01"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Augmentation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Add Dropout layer (.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training: 91.39\n",
    "\n",
    "Testing: 88.11"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Add Dense Layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training: 93.78\n",
    "\n",
    "Testing: 90.44\n",
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we know how individual changes effect the accuracy, lets combine a couple that improved some aspect of accuracy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 35 Epochs & Add dropout layer (.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training: 98.84\n",
    "\n",
    "Testing: 92.07"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Very close... Lets try to increase dropout rate to .4:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training: 99.71\n",
    "\n",
    "Testing: 93.47"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Increase dropout rate to .6:\n",
    "\n",
    "Training: 97.96\n",
    "\n",
    "Testing: 92.54"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Deepness 1 & Dropout layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training: 93.95 \n",
    "\n",
    "Testing: 89.51"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Deepness 1 & Dropout layer & 35 Epochs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training: 98.95\n",
    "\n",
    "Testing: 90.21"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Results and Reflection\n",
    "Looking at our last 3 experiemnts with combined changes, the only one to actually increase valdation accuracy was to increase the number of epochs to 35 and add a dropout layer with a rate of .4. This increased the accuracy by ~1%. By just increasing the epochs, we can increase the training accuracy but overfitting occurs. So, to counteract the overfitting by adding a dropout layer. A rate of .2 was not enough to fix the overfitting, so increasing the rate to .4 results in a 1% increase in validation accuracy. Thus, the most accurate model for this data set has a dropout rate of .4 and 35 epochs. However, increasing the number of epochs may increase the accuracy, but for the sake of time we will max out at 35."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
